"""
OpenAlex API í´ë¼ì´ì–¸íŠ¸
"""
import logging
from typing import List, Dict, Optional
import httpx

from ..config.openalex_config import OpenAlexConfig
from ..utils.parser import parse_abstract_inverted_index

logger = logging.getLogger(__name__)


class OpenAlexAPIClient:
    """OpenAlex API í´ë¼ì´ì–¸íŠ¸"""
    
    BASE_URL = "https://api.openalex.org"
    
    def __init__(self):
        self.http_client = httpx.AsyncClient(
            timeout=OpenAlexConfig.TIMEOUT,
            limits=httpx.Limits(
                max_connections=20,  # 10â†’20 (ë³‘ë ¬ ì²˜ë¦¬ ê°œì„ )
                max_keepalive_connections=10
            ),
            http2=True  # HTTP/2 í™œì„±í™” (ë©€í‹°í”Œë ‰ì‹±)
        )
    
    async def search_papers(
        self, 
        query: Dict, 
        exclude_ids: Optional[List[str]] = None,
        sort_by: str = "relevance"
    ) -> List[Dict]:
        """
        OpenAlex API ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (tokens, year_from í¬í•¨)
            exclude_ids: ì œì™¸í•  ë…¼ë¬¸ ID ë¦¬ìŠ¤íŠ¸
            sort_by: ì •ë ¬ ê¸°ì¤€ ("relevance", "cited_by_count", "hybrid")
                - "relevance": í‚¤ì›Œë“œ ì—°ê´€ì„± ìš°ì„  (ê¸°ë³¸ê°’)
                - "cited_by_count": ì¸ìš©ìˆ˜ ìš°ì„ 
                - "hybrid": ì—°ê´€ì„± ë†’ì€ ë…¼ë¬¸ ì¤‘ ì¸ìš©ìˆ˜ ìƒìœ„ ì„ íƒ
            
        Returns:
            List[Dict]: íŒŒì‹±ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ê²€ìƒ‰ ë¬¸ìì—´ ìƒì„± (tokens ê¸°ë°˜)
            tokens = query.get("tokens", [])
            search_str = " ".join(tokens)
            
            # year_from ì²˜ë¦¬
            year_from = query.get("year_from", OpenAlexConfig.DEFAULT_YEAR_FROM)
            
            # í•„í„° êµ¬ì„±
            filters = [
                f"from_publication_date:{year_from}-01-01",
                "language:en",
                "is_paratext:false",
                "type:article",
            ]
            
            # ì •ë ¬ ì˜µì…˜ ê²°ì •
            if sort_by == "cited_by_count":
                sort_param = "cited_by_count:desc"
            elif sort_by == "hybrid":
                # Hybrid ëª¨ë“œ: ë” ë§ì€ ë…¼ë¬¸ ê°€ì ¸ì˜¨ í›„ ì¬ì •ë ¬
                sort_param = "relevance_score:desc"
                per_page = OpenAlexConfig.PER_PAGE * 2  # 50ê°œ ê°€ì ¸ì˜¤ê¸°
            else:  # relevance (ê¸°ë³¸ê°’)
                sort_param = "relevance_score:desc"
                per_page = OpenAlexConfig.PER_PAGE
            
            # OpenAlex API í˜¸ì¶œ
            params = {
                "search": search_str,
                "filter": ",".join(filters),
                "sort": sort_param,
                "per_page": per_page if sort_by == "hybrid" else OpenAlexConfig.PER_PAGE
            }
            
            logger.info(f"ğŸ” OpenAlex API ìš”ì²­:")
            logger.info(f"   â”œâ”€ URL: {self.BASE_URL}/works")
            logger.info(f"   â”œâ”€ search: \"{search_str}\"")
            logger.info(f"   â”œâ”€ filters: {filters}")
            logger.info(f"   â”œâ”€ sort: {sort_param}")
            logger.info(f"   â””â”€ per_page: {params['per_page']}")
            
            response = await self.http_client.get(
                f"{self.BASE_URL}/works",
                params=params
            )
            response.raise_for_status()
            data = response.json()
            
            works = data.get("results", [])
            logger.info(f"ğŸ“„ OpenAlex ì›ë³¸ ê²€ìƒ‰: {len(works)}ê°œ")
            
            if len(works) == 0:
                logger.warning(f"âš ï¸  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ. ê°€ëŠ¥í•œ ì›ì¸:")
                logger.warning(f"   1. ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ êµ¬ì²´ì : \"{search_str}\"")
                logger.warning(f"   2. TOKEN ìˆ˜ê°€ ë§ìŒ: {len(tokens)}ê°œ")
                logger.warning(f"   3. year_from í•„í„°: {year_from}")
                logger.warning(f"   í•´ê²°: TOKEN ì¤„ì´ê¸° (2-3ê°œ), year_from ì¡°ì • (2015)")
                return []
            
            # íŒŒì‹± ë° í•„í„°ë§
            papers = []
            exclude_set = set(exclude_ids or [])
            
            for work in works:
                paper = self._parse_paper(work)
                
                if paper is None:
                    continue
                
                # ì œì™¸ ID ì²´í¬
                if paper.get("id") in exclude_set:
                    logger.debug(f"â­ï¸  ì œì™¸ë¨ (exclude_ids): {paper.get('title', '')[:50]}")
                    continue
                
                papers.append(paper)
            
            # Hybrid ëª¨ë“œ: ì—°ê´€ì„± ì ìˆ˜ 0.5 ì´ìƒì¸ ë…¼ë¬¸ ì¤‘ ì¸ìš©ìˆ˜ë¡œ ì¬ì •ë ¬
            if sort_by == "hybrid" and papers:
                # ì—°ê´€ì„± ì„ê³„ê°’ (ìƒìœ„ 60%)
                threshold = max(p.get("relevance_score", 0) for p in papers) * 0.6
                relevant_papers = [p for p in papers if p.get("relevance_score", 0) >= threshold]
                
                # ì¸ìš©ìˆ˜ë¡œ ì •ë ¬
                relevant_papers.sort(key=lambda x: x.get("cited_by_count", 0), reverse=True)
                papers = relevant_papers[:OpenAlexConfig.PER_PAGE]
                
                logger.info(f"ğŸ”„ Hybrid ì¬ì •ë ¬: ì—°ê´€ì„± {threshold:.2f} ì´ìƒ ë…¼ë¬¸ {len(relevant_papers)}ê°œ â†’ ì¸ìš©ìˆ˜ ìƒìœ„ {len(papers)}ê°œ ì„ íƒ")
            
            logger.info(f"âœ… OpenAlex íŒŒì‹± ì™„ë£Œ: {len(papers)}ê°œ")
            return papers
            
        except httpx.TimeoutException:
            logger.error("âŒ OpenAlex API íƒ€ì„ì•„ì›ƒ")
            return []
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ OpenAlex API HTTP ì˜¤ë¥˜: {e.response.status_code}")
            return []
        except Exception as e:
            logger.error(f"âŒ OpenAlex API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_paper(self, work: Dict) -> Optional[Dict]:
        """
        OpenAlex work ê°ì²´ íŒŒì‹±
        
        Args:
            work: OpenAlex work ê°ì²´
            
        Returns:
            Dict or None (í•„í„°ë§ë¨)
        """
        try:
            # ê¸°ë³¸ ì •ë³´
            paper_id = work.get("id", "")
            title = work.get("title", "")
            year = work.get("publication_year")
            cited_by_count = work.get("cited_by_count", 0)
            doi = work.get("doi", "")
            url = doi if doi else paper_id
            
            # ì´ˆë¡ íŒŒì‹±
            abstract_inverted = work.get("abstract_inverted_index")
            abstract = parse_abstract_inverted_index(abstract_inverted)
            
            # ì €ì íŒŒì‹±
            authors = []
            authorships = work.get("authorships", [])
            for authorship in authorships[:5]:  # ìƒìœ„ 5ëª…ë§Œ
                author = authorship.get("author", {})
                name = author.get("display_name")
                if name:
                    authors.append(name)
            
            # ì´ˆë¡ ì—†ëŠ” ë…¼ë¬¸ í•„í„°ë§
            no_abstract = not abstract or len(abstract.strip()) < 50
            
            if no_abstract:
                # ì´ˆë¡ ì—†ê³  ì¸ìš© ìˆ˜ ë‚®ìŒ â†’ ì œì™¸
                if cited_by_count < 100:
                    logger.debug(
                        f"â­ï¸  ì œì™¸ë¨ (ì´ˆë¡ ì—†ìŒ + ì¸ìš© ìˆ˜ {cited_by_count}): {title[:50]}"
                    )
                    return None
            
            return {
                "id": paper_id,
                "title": title,
                "abstract": abstract,
                "year": year,
                "cited_by_count": cited_by_count,
                "url": url,
                "authors": authors,
                "no_abstract": no_abstract,
                "relevance_score": work.get("relevance_score", 0.0)
            }
            
        except Exception as e:
            logger.error(f"âŒ ë…¼ë¬¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    async def close(self):
        """HTTP í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ"""
        await self.http_client.aclose()
