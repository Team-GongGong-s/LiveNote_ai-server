"""
GoogleKit - Google Custom Search ê¸°ë°˜ ê²€ìƒ‰ ì¶”ì²œ ëª¨ë“ˆ
"""
from .models import (
    RAGChunk,
    PreviousSummary,
    GoogleSearchResult,
    GoogleRequest,
    GoogleResponse,
)
from .service import GoogleService

__version__ = "0.1.0"

__all__ = [
    "RAGChunk",
    "PreviousSummary",
    "GoogleSearchResult",
    "GoogleRequest",
    "GoogleResponse",
    "GoogleService",
]
"""
API íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
"""
from .google_client import GoogleSearchClient

__all__ = ["GoogleSearchClient"]
"""
Google Custom Search API í´ë¼ì´ì–¸íŠ¸
"""
import aiohttp
import logging
from typing import List, Dict, Any, Optional

from ..config.google_config import GoogleConfig

logger = logging.getLogger(__name__)


class GoogleSearchClient:
    """Google Custom Search API í´ë¼ì´ì–¸íŠ¸"""
    
    BASE_URL = "https://www.googleapis.com/customsearch/v1"
    
    def __init__(self, api_key: Optional[str] = None, engine_id: Optional[str] = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            api_key: Google Search API í‚¤ (Noneì´ë©´ í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)
            engine_id: Search Engine ID (Noneì´ë©´ í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)
        """
        self.api_key = api_key or GoogleConfig.GOOGLE_SEARCH_API_KEY
        self.engine_id = engine_id or GoogleConfig.GOOGLE_SEARCH_ENGINE_ID
        
        if not self.api_key:
            raise ValueError("GOOGLE_SEARCH_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if not self.engine_id:
            raise ValueError("GOOGLE_SEARCH_ENGINE_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    async def search(
        self,
        query: str,
        lang: str = "ko",
        num: int = 10,
    ) -> List[Dict[str, Any]]:
        """
        Google Custom Search í˜¸ì¶œ
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            lang: ê²€ìƒ‰ ì–¸ì–´ (ko/en)
            num: ê²°ê³¼ ê°œìˆ˜ (ìµœëŒ€ 10)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        params = {
            "key": self.api_key,
            "cx": self.engine_id,
            "q": query,
            "lr": f"lang_{lang}",  # Language restrict
            "num": min(num, 10),  # ìµœëŒ€ 10ê°œ
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(self.BASE_URL, params=params) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"âŒ Google API ì˜¤ë¥˜ ({response.status}): {error_text}")
                        return []
                    
                    data = await response.json()
                    items = data.get("items", [])
                    
                    logger.info(f"ğŸ” Google API ì‘ë‹µ: {len(items)}ê°œ ê²°ê³¼")
                    
                    # ê²°ê³¼ ì •ê·œí™”
                    results = []
                    for item in items:
                        results.append({
                            "title": item.get("title", ""),
                            "link": item.get("link", ""),
                            "snippet": item.get("snippet", ""),
                            "displayLink": item.get("displayLink", ""),
                        })
                    
                    return results
        
        except aiohttp.ClientError as e:
            logger.error(f"âŒ Google API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ Google API ì˜ˆì™¸ ë°œìƒ: {e}")
            return []
"""
Config íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
"""
from .flags import (
    NO_SCORING,
    VERIFY_GOOGLE_DEFAULT,
    KEYWORD_MIN,
    KEYWORD_MAX,
    WEIGHT_TITLE_MATCH,
    WEIGHT_SNIPPET_MATCH,
    WEIGHT_DOMAIN_TRUST,
    TRUSTED_DOMAINS,
)
from .google_config import GoogleConfig
from . import prompts

__all__ = [
    "NO_SCORING",
    "VERIFY_GOOGLE_DEFAULT",
    "KEYWORD_MIN",
    "KEYWORD_MAX",
    "WEIGHT_TITLE_MATCH",
    "WEIGHT_SNIPPET_MATCH",
    "WEIGHT_DOMAIN_TRUST",
    "TRUSTED_DOMAINS",
    "GoogleConfig",
    "prompts",
]
"""
Google Provider ì „ìš© í”Œë˜ê·¸
"""

# â”â”â” ê²€ì¦ ìŠ¤ìœ„ì¹˜ â”â”â”
NO_SCORING = False  # Trueì´ë©´ ê²€ì¦ ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
VERIFY_GOOGLE_DEFAULT = True  # ê¸°ë³¸ê°’: LLM ê²€ì¦

# â”â”â” í‚¤ì›Œë“œ ìƒì„± ì„¤ì • â”â”â”
KEYWORD_MIN = 2  # ìµœì†Œ í‚¤ì›Œë“œ ê°œìˆ˜
KEYWORD_MAX = 4  # ìµœëŒ€ í‚¤ì›Œë“œ ê°œìˆ˜

# â”â”â” ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ â”â”â”
WEIGHT_TITLE_MATCH = 0.4
WEIGHT_SNIPPET_MATCH = 0.3
WEIGHT_DOMAIN_TRUST = 0.3

# â”â”â” ì‹ ë¢° ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸ â”â”â”
TRUSTED_DOMAINS = [
    ".edu",
    ".gov",
    "arxiv.org",
    "scholar.google.com",
    "stackoverflow.com",
    "github.com",
    "microsoft.com",
    "mozilla.org",
]
"""
Google ëª¨ë“ˆ ì„¤ì •
"""
import os
from typing import Optional


class GoogleConfig:
    """Google ëª¨ë“ˆ ì„¤ì •"""
    
    # API í‚¤
    GOOGLE_SEARCH_API_KEY: str = os.getenv("GOOGLE_SEARCH_API_KEY", "")
    GOOGLE_SEARCH_ENGINE_ID: str = os.getenv("GOOGLE_SEARCH_ENGINE_ID", "")
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
    
    # ê²€ìƒ‰ ì„¤ì •
    DEFAULT_TOP_K: int = 5
    DEFAULT_LANGUAGE: str = "ko"
    DEFAULT_SEARCH_LANG: str = "en"
    
    # ì œí•œ
    MAX_TOP_K: int = 10
    CARD_LIMIT: int = 15  # ê²€ì¦ ëŒ€ìƒ ìµœëŒ€ ìˆ˜
    SEARCH_LIMIT: int = 10  # API í•œ ë²ˆ í˜¸ì¶œ ì‹œ ìµœëŒ€ ê²°ê³¼
    FANOUT: int = 3  # ë™ì‹œ ê²€ìƒ‰ í‚¤ì›Œë“œ ê°œìˆ˜
    
    # LLM ì„¤ì •
    LLM_MODEL: str = "gpt-4o"
    #LLM_MODEL: str = "gpt-4o"
    LLM_TEMPERATURE: float = 0.2
    MAX_TOKENS_QUERY: int = 150
    MAX_TOKENS_SCORE: int = 120
    
    # ë³‘ë ¬ ì²˜ë¦¬
    VERIFY_CONCURRENCY: int = 15
    
    @classmethod
    def validate(cls):
        """í™˜ê²½ ë³€ìˆ˜ ê²€ì¦"""
        if not cls.GOOGLE_SEARCH_API_KEY:
            raise ValueError("GOOGLE_SEARCH_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if not cls.GOOGLE_SEARCH_ENGINE_ID:
            raise ValueError("GOOGLE_SEARCH_ENGINE_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if not cls.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
"""
Google ê²€ìƒ‰ì„ ìœ„í•œ LLM í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
"""

# â”â”â” í‚¤ì›Œë“œ ìƒì„± í”„ë¡¬í”„íŠ¸ â”â”â”
KEYWORD_GENERATION_PROMPT = """You are an expert search query generator for academic content.

Generate {keyword_min}-{keyword_max} specific search queries in {language} for the given lecture topic.

**Guidelines:**
- Use detailed technical phrases (3-7 words)
- Include specific concepts, technologies, or methodologies
- Create queries optimized for Google search to find high-quality results
- Return ONLY the queries, one per line

**Lecture Summary:**
{lecture_summary}

{context}

**Search Queries:**"""


KEYWORD_CONTEXT_TEMPLATE = """**Previous Context:**
{previous_summaries}

**Related Materials:**
{rag_context}
"""


# â”â”â” ê²€ì¦ í”„ë¡¬í”„íŠ¸ â”â”â”
SCORING_PROMPT = """You are evaluating the relevance of a search result to a lecture topic.

**Lecture Summary:**
{lecture_summary}

**Search Result:**
Title: {title}
URL: {url}
Snippet: {snippet}

**Task:**
Rate the relevance on a scale of 0-10 and provide a brief reason in {language}.

**Scoring Guidelines:**
- 10: Original authoritative source (official documentation, seminal papers, standard specifications)
- 9: Highly relevant with comprehensive technical depth and accurate explanations
- 7-8: Very relevant, covers key concepts with good technical details
- 5-6: Moderately relevant, provides useful background or related information
- 3-4: Somewhat relevant, mentions the topic but lacks depth
- 0-2: Not relevant, off-topic or too general

**Bonus Points (+0.5 each, max +1.0):**
- Technical documentation from official sources (.edu, .gov, official project sites)
- In-depth tutorials with practical examples
- Educational content from reputable institutions

**Response Format (JSON only):**
{{"score": <float>, "reason": "<1-2 sentences in {language}>"}}

**JSON Response:**"""
"""
LLM íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
"""
from .openai_client import GoogleLLMClient

__all__ = ["GoogleLLMClient"]
"""
Google ê²€ìƒ‰ì„ ìœ„í•œ OpenAI LLM í´ë¼ì´ì–¸íŠ¸
"""
import asyncio
import json
import logging
from typing import List, Dict, Any

from openai import AsyncOpenAI

from ..config.google_config import GoogleConfig
from ..config import prompts
from ..config import flags

logger = logging.getLogger(__name__)


class GoogleLLMClient:
    """Google ê²€ìƒ‰ì„ ìœ„í•œ LLM í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, api_key: str = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            api_key: OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)
        """
        api_key = api_key or GoogleConfig.OPENAI_API_KEY
        if not api_key:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = GoogleConfig.LLM_MODEL
        self.temperature = GoogleConfig.LLM_TEMPERATURE
    
    async def generate_keywords(
        self,
        lecture_summary: str,
        language: str,
        previous_summaries: List[Dict] = None,
        rag_context: List[Dict] = None
    ) -> List[str]:
        """
        ê°•ì˜ ìš”ì•½ â†’ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
        
        Args:
            lecture_summary: ê°•ì˜ ìš”ì•½
            language: í‚¤ì›Œë“œ ìƒì„± ì–¸ì–´ (search_lang)
            previous_summaries: ì´ì „ ì„¹ì…˜ ìš”ì•½ (ì„ íƒ)
            rag_context: RAG ì»¨í…ìŠ¤íŠ¸ (ì„ íƒ)
            
        Returns:
            ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = ""
        if previous_summaries or rag_context:
            prev_text = ""
            if previous_summaries:
                prev_text = "\n".join([
                    f"Section {s['section_id']}: {s['summary']}"
                    for s in previous_summaries[:3]  # ìµœê·¼ 3ê°œë§Œ
                ])
            
            rag_text = ""
            if rag_context:
                rag_text = "\n".join([
                    chunk['text'][:200]
                    for chunk in rag_context[:3]  # ìµœëŒ€ 3ê°œ
                ])
            
            context = prompts.KEYWORD_CONTEXT_TEMPLATE.format(
                previous_summaries=prev_text or "None",
                rag_context=rag_text or "None"
            )
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = prompts.KEYWORD_GENERATION_PROMPT.format(
            keyword_min=flags.KEYWORD_MIN,
            keyword_max=flags.KEYWORD_MAX,
            language=language,
            lecture_summary=lecture_summary,
            context=context
        )
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                max_tokens=GoogleConfig.MAX_TOKENS_QUERY,
            )
            
            content = response.choices[0].message.content.strip()
            
            # í‚¤ì›Œë“œ íŒŒì‹± (ì¤„ë°”ê¿ˆ ê¸°ì¤€)
            keywords = [
                line.strip()
                for line in content.split('\n')
                if line.strip() and not line.strip().startswith('#')
            ]
            
            logger.info(f"ğŸ¤– LLM í‚¤ì›Œë“œ ìƒì„±: {keywords}")
            
            return keywords[:GoogleConfig.FANOUT]  # ìµœëŒ€ FANOUT ê°œìˆ˜ë§Œ ë°˜í™˜
        
        except Exception as e:
            logger.error(f"âŒ LLM í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°•ì˜ ìš”ì•½ì˜ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ
            fallback = lecture_summary.split()[:3]
            logger.warning(f"âš ï¸  í´ë°± í‚¤ì›Œë“œ ì‚¬ìš©: {fallback}")
            return fallback
    
    async def score_result(
        self,
        lecture_summary: str,
        title: str,
        snippet: str,
        url: str,
        language: str
    ) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ê²°ê³¼ LLM ê²€ì¦
        
        Args:
            lecture_summary: ê°•ì˜ ìš”ì•½
            title: ê²€ìƒ‰ ê²°ê³¼ ì œëª©
            snippet: ê²€ìƒ‰ ê²°ê³¼ ìŠ¤ë‹ˆí«
            url: ê²€ìƒ‰ ê²°ê³¼ URL
            language: ì‘ë‹µ ì–¸ì–´
            
        Returns:
            {"score": 8.5, "reason": "..."}
        """
        prompt = prompts.SCORING_PROMPT.format(
            lecture_summary=lecture_summary,
            title=title,
            snippet=snippet,
            url=url,
            language=language
        )
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                max_tokens=GoogleConfig.MAX_TOKENS_SCORE,
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                result = json.loads(content)
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ë‚´ìš©: {content[:200]}")
                # ê°„ë‹¨í•œ ì •ê·œì‹ìœ¼ë¡œ scoreì™€ reason ì¶”ì¶œ ì‹œë„
                import re
                score_match = re.search(r'"score"\s*:\s*([0-9.]+)', content)
                reason_match = re.search(r'"reason"\s*:\s*"([^"]*)"', content)
                
                if score_match and reason_match:
                    return {
                        "score": float(score_match.group(1)),
                        "reason": reason_match.group(1)
                    }
                raise e
            
            return {
                "score": float(result.get("score", 0.0)),
                "reason": result.get("reason", "")
            }
        
        except Exception as e:
            logger.error(f"âŒ LLM ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {"score": 0.0, "reason": "ê²€ì¦ ì‹¤íŒ¨"}
"""
Google ê²€ìƒ‰ ì¶”ì²œì„ ìœ„í•œ ë°ì´í„° ëª¨ë¸
"""
from pydantic import BaseModel, Field, field_validator
from typing import List, Optional, Dict, Any
from datetime import datetime

from .config import flags


# â”â”â” ê³µí†µ ëª¨ë¸ â”â”â”
class RAGChunk(BaseModel):
    """RAG ê²€ìƒ‰ ê²°ê³¼ ì²­í¬"""
    text: str = Field(..., description="ì²­í¬ í…ìŠ¤íŠ¸")
    score: float = Field(..., ge=0.0, description="ê´€ë ¨ë„ ì ìˆ˜")
    metadata: Optional[Dict[str, Any]] = Field(default=None, description="ë©”íƒ€ë°ì´í„°")


class PreviousSummary(BaseModel):
    """ì´ì „ ì„¹ì…˜ ìš”ì•½"""
    section_id: int = Field(..., ge=1, description="ì„¹ì…˜ ë²ˆí˜¸")
    summary: str = Field(..., min_length=1, description="ì„¹ì…˜ ìš”ì•½")
    timestamp: Optional[str] = Field(default=None, description="íƒ€ì„ìŠ¤íƒ¬í”„")


# â”â”â” Google íŠ¹í™” ëª¨ë¸ â”â”â”
class GoogleSearchResult(BaseModel):
    """Google ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ì •ë³´"""
    url: str = Field(..., description="ì›¹í˜ì´ì§€ URL")
    title: str = Field(..., description="í˜ì´ì§€ ì œëª©")
    snippet: str = Field(..., description="í˜ì´ì§€ ìš”ì•½ (3-4ì¤„)")
    display_link: str = Field(..., description="í‘œì‹œ ë„ë©”ì¸ (ì˜ˆ: naver.com)")
    lang: str = Field(..., description="í˜ì´ì§€ ì–¸ì–´ (ko/en)")
    
    @field_validator('title', 'snippet')
    @classmethod
    def normalize_newlines(cls, v: str) -> str:
        """ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜"""
        return v.replace('\n', ' ').replace('\r', ' ')


class GoogleRequest(BaseModel):
    """Google ê²€ìƒ‰ ê²°ê³¼ ì¶”ì²œ ìš”ì²­"""
    
    # â”â”â” í•„ìˆ˜ í•„ë“œ â”â”â”
    lecture_id: str = Field(..., description="ê°•ì˜ ì„¸ì…˜ ID (ì¶”ì ìš©)")
    section_id: int = Field(..., ge=1, description="í˜„ì¬ ì„¹ì…˜ ë²ˆí˜¸")
    lecture_summary: str = Field(..., min_length=10, description="í˜„ì¬ ê°•ì˜ ì„¹ì…˜ ìš”ì•½")
    
    # â”â”â” ì„ íƒ í•„ë“œ â”â”â”
    language: str = Field(default="ko", description="ì‘ë‹µ ì–¸ì–´ (ko/en)")
    top_k: int = Field(default=5, ge=1, le=10, description="ì¶”ì²œ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜")
    verify_google: bool = Field(
        default=flags.VERIFY_GOOGLE_DEFAULT,
        description="LLM ê²€ì¦ ì—¬ë¶€ (True: LLM, False: Heuristic)"
    )
    
    # â”â”â” ì»¨í…ìŠ¤íŠ¸ í•„ë“œ â”â”â”
    previous_summaries: List[PreviousSummary] = Field(
        default_factory=list,
        description="ì´ì „ Nê°œ ì„¹ì…˜ ìš”ì•½ (ì»¨í…ìŠ¤íŠ¸ í™•ì¥ìš©)"
    )
    rag_context: List[RAGChunk] = Field(
        default_factory=list,
        description="RAG ê²€ìƒ‰ ê²°ê³¼ (ê°•ì˜ë…¸íŠ¸/ì´ì „ ì„¹ì…˜)"
    )
    
    # â”â”â” ê²€ìƒ‰ ì œì–´ í•„ë“œ â”â”â”
    search_lang: str = Field(default="ko", description="Google ê²€ìƒ‰ ì–¸ì–´ (ko/en/auto)")
    exclude_urls: List[str] = Field(
        default_factory=list,
        description="ì œì™¸í•  URL ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ë°©ì§€)"
    )
    min_score: float = Field(
        default=5.0,
        ge=0.0,
        le=10.0,
        description="ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’ (ì´ ì ìˆ˜ ë¯¸ë§Œ ê²°ê³¼ ì œì™¸)"
    )


class GoogleResponse(BaseModel):
    """Google ê²€ìƒ‰ ê²°ê³¼ ì¶”ì²œ ì‘ë‹µ"""
    lecture_id: str = Field(..., description="ê°•ì˜ ì„¸ì…˜ ID")
    section_id: int = Field(..., description="ì„¹ì…˜ ë²ˆí˜¸")
    search_result: GoogleSearchResult = Field(..., description="ê²€ìƒ‰ ê²°ê³¼ ì •ë³´")
    reason: str = Field(..., description="ì¶”ì²œ ì´ìœ  (1-2ë¬¸ì¥)")
    score: float = Field(..., ge=0.0, le=15.0, description="ê´€ë ¨ë„ ì ìˆ˜ (0-10, LLMì´ ì´ˆê³¼ ê°€ëŠ¥)")
    
    @field_validator('reason')
    @classmethod
    def normalize_newlines(cls, v: str) -> str:
        """ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜"""
        return v.replace('\n', ' ').replace('\r', ' ')
"""
Google ê²€ìƒ‰ ì¶”ì²œ ì„œë¹„ìŠ¤
"""
import asyncio
import logging
from typing import List

from .models import GoogleRequest, GoogleResponse, GoogleSearchResult
from .api.google_client import GoogleSearchClient
from .llm.openai_client import GoogleLLMClient
from .utils import (
    deduplicate_results,
    rerank_results,
    filter_excluded_urls,
    heuristic_score,
    calculate_reason,
)
from .config import flags
from .config.google_config import GoogleConfig

logger = logging.getLogger(__name__)


class GoogleService:
    """Google ê²€ìƒ‰ ì¶”ì²œ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        GoogleConfig.validate()
        
        self.api_client = GoogleSearchClient()
        self.llm_client = GoogleLLMClient()
        self.config = GoogleConfig
    
    async def recommend_results(
        self,
        request: GoogleRequest
    ) -> List[GoogleResponse]:
        """
        ê²€ìƒ‰ ê²°ê³¼ ì¶”ì²œ íŒŒì´í”„ë¼ì¸
        
        íë¦„:
        1. í‚¤ì›Œë“œ ìƒì„± (LLM)
        2. íŒ¬ì•„ì›ƒ ë³‘ë ¬ ê²€ìƒ‰ (Google API)
        3. ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        4. ì œì™¸ URL í•„í„°ë§
        5. ì¬ì •ë ¬ (í‚¤ì›Œë“œ ë§¤ì¹­)
        6. ìƒìœ„ Nê°œ ì„ íƒ (CARD_LIMIT)
        7. NO_SCORING ëª¨ë“œ ì²´í¬
           - True: ê²€ì¦ ìŠ¤í‚µ, reason="search", score=10
           - False: ê²€ì¦ ë‹¨ê³„ ì§„í–‰
        8. ì¡°ê±´ë¶€ ê²€ì¦ (LLM or Heuristic)
        9. min_score í•„í„°ë§
        10. ì ìˆ˜ ìˆœ ì •ë ¬ + top_k ë°˜í™˜
        
        Args:
            request: Google ê²€ìƒ‰ ìš”ì²­
            
        Returns:
            ì¶”ì²œ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” Google ê²€ìƒ‰ ì‹œì‘ (lecture_id={request.lecture_id}, section_id={request.section_id})")
        
        # 1. í‚¤ì›Œë“œ ìƒì„±
        logger.info(f"ğŸ¤– LLM í‚¤ì›Œë“œ ìƒì„± ì‹œì‘ (language={request.search_lang})")
        
        prev_summaries = [
            {"section_id": ps.section_id, "summary": ps.summary}
            for ps in request.previous_summaries
        ]
        rag_chunks = [
            {"text": chunk.text, "score": chunk.score}
            for chunk in request.rag_context
        ]
        
        keywords = await self.llm_client.generate_keywords(
            lecture_summary=request.lecture_summary,
            language=request.search_lang,
            previous_summaries=prev_summaries,
            rag_context=rag_chunks
        )
        
        if not keywords:
            logger.warning("âš ï¸  í‚¤ì›Œë“œê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        logger.info(f"âœ… í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: {keywords}")
        
        # 2. íŒ¬ì•„ì›ƒ ë³‘ë ¬ ê²€ìƒ‰
        logger.info(f"ğŸŒ Google API íŒ¬ì•„ì›ƒ ê²€ìƒ‰ ì‹œì‘ (keywords={len(keywords)}ê°œ)")
        
        search_tasks = [
            self.api_client.search(
                query=keyword,
                lang=request.search_lang,
                num=self.config.SEARCH_LIMIT
            )
            for keyword in keywords[:self.config.FANOUT]
        ]
        
        search_results_list = await asyncio.gather(*search_tasks)
        
        # ê²°ê³¼ ë³‘í•©
        all_results = []
        for results in search_results_list:
            all_results.extend(results)
        
        logger.info(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘: {len(all_results)}ê°œ")
        
        if not all_results:
            logger.warning("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 3. ì¤‘ë³µ ì œê±°
        unique_results = deduplicate_results(all_results)
        
        # 4. ì œì™¸ URL í•„í„°ë§
        filtered_results = filter_excluded_urls(unique_results, request.exclude_urls)
        
        if not filtered_results:
            logger.warning("âš ï¸  í•„í„°ë§ í›„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 5. ì¬ì •ë ¬
        reranked_results = rerank_results(filtered_results, keywords)
        
        # 6. ìƒìœ„ Nê°œ ì„ íƒ
        top_results = reranked_results[:self.config.CARD_LIMIT]
        
        logger.info(f"ğŸ“¥ ê²€ì¦ ëŒ€ìƒ: {len(top_results)}ê°œ")
        
        # 7. NO_SCORING ëª¨ë“œ ì²´í¬
        if flags.NO_SCORING:
            logger.info("âš¡ NO_SCORING ëª¨ë“œ: ê²€ì¦ ìŠ¤í‚µ")
            
            responses = []
            for item in top_results[:request.top_k]:
                result_info = GoogleSearchResult(
                    url=item.get("link", ""),
                    title=item.get("title", ""),
                    snippet=item.get("snippet", "")[:300],
                    display_link=item.get("displayLink", ""),
                    lang=request.search_lang
                )
                
                responses.append(GoogleResponse(
                    lecture_id=request.lecture_id,
                    section_id=request.section_id,
                    search_result=result_info,
                    reason="search",
                    score=10.0
                ))
            
            logger.info(f"âœ… NO_SCORING ê²°ê³¼: {len(responses)}ê°œ")
            return responses
        
        # 8. ê²€ì¦ (LLM or Heuristic)
        if request.verify_google:
            logger.info("ğŸ¤– LLM ê²€ì¦ ì‹œì‘")
            verified_results = await self._verify_with_llm(
                top_results,
                request.lecture_summary,
                request.language,
                keywords,
                request.lecture_id,
                request.section_id
            )
        else:
            logger.info("ğŸ“Š Heuristic ê²€ì¦ ì‹œì‘")
            verified_results = self._verify_with_heuristic(
                top_results,
                keywords,
                request.language,
                request.lecture_id,
                request.section_id
            )
        
        logger.info(f"âœ… ê²€ì¦ ì™„ë£Œ: {len(verified_results)}ê°œ")
        
        # 9. min_score í•„í„°ë§
        filtered_by_score = [
            result for result in verified_results
            if result.score >= request.min_score
        ]
        
        logger.info(f"ğŸ¯ min_score í•„í„°ë§: {len(filtered_by_score)}ê°œ (>= {request.min_score})")
        
        # 10. ì ìˆ˜ ìˆœ ì •ë ¬ + top_k ë°˜í™˜
        filtered_by_score.sort(key=lambda x: x.score, reverse=True)
        final_results = filtered_by_score[:request.top_k]
        
        logger.info(f"âœ… Google ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ë°˜í™˜")
        
        return final_results
    
    async def _verify_with_llm(
        self,
        results: List[dict],
        lecture_summary: str,
        language: str,
        keywords: List[str],
        lecture_id: str,
        section_id: int
    ) -> List[GoogleResponse]:
        """
        LLMì„ ì‚¬ìš©í•œ ê²€ì¦
        
        Args:
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            lecture_summary: ê°•ì˜ ìš”ì•½
            language: ì‘ë‹µ ì–¸ì–´
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ
            lecture_id: ê°•ì˜ ID
            section_id: ì„¹ì…˜ ID
            
        Returns:
            ê²€ì¦ëœ GoogleResponse ë¦¬ìŠ¤íŠ¸
        """
        semaphore = asyncio.Semaphore(self.config.VERIFY_CONCURRENCY)
        
        async def verify_one(item: dict):
            async with semaphore:
                title = item.get("title", "")
                snippet = item.get("snippet", "")
                url = item.get("link", "")
                
                # LLM ì ìˆ˜ ê³„ì‚°
                llm_result = await self.llm_client.score_result(
                    lecture_summary=lecture_summary,
                    title=title,
                    snippet=snippet,
                    url=url,
                    language=language
                )
                
                result_info = GoogleSearchResult(
                    url=item.get("link", ""),
                    title=title,
                    snippet=snippet[:300],
                    display_link=item.get("displayLink", ""),
                    lang=language
                )
                
                return GoogleResponse(
                    lecture_id=lecture_id,
                    section_id=section_id,
                    search_result=result_info,
                    reason=llm_result["reason"],
                    score=llm_result["score"]
                )
        
        # ë³‘ë ¬ ê²€ì¦
        tasks = [verify_one(item) for item in results]
        
        verified = await asyncio.gather(*tasks)
        
        return verified
    
    def _verify_with_heuristic(
        self,
        results: List[dict],
        keywords: List[str],
        language: str,
        lecture_id: str,
        section_id: int
    ) -> List[GoogleResponse]:
        """
        Heuristicì„ ì‚¬ìš©í•œ ê²€ì¦
        
        Args:
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ
            language: ì‘ë‹µ ì–¸ì–´
            lecture_id: ê°•ì˜ ID
            section_id: ì„¹ì…˜ ID
            
        Returns:
            ê²€ì¦ëœ GoogleResponse ë¦¬ìŠ¤íŠ¸
        """
        verified = []
        
        for item in results:
            title = item.get("title", "")
            snippet = item.get("snippet", "")
            display_link = item.get("displayLink", "")
            
            # Heuristic ì ìˆ˜ ê³„ì‚°
            score = heuristic_score(title, snippet, keywords, display_link)
            reason = calculate_reason(title, snippet, keywords, score, language)
            
            result_info = GoogleSearchResult(
                url=item.get("link", ""),
                title=title,
                snippet=snippet[:300],
                display_link=display_link,
                lang=language
            )
            
            verified.append(GoogleResponse(
                lecture_id=lecture_id,
                section_id=section_id,
                search_result=result_info,
                reason=reason,
                score=score
            ))
        
        return verified
"""
Utils íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
"""
from .filters import deduplicate_results, rerank_results, filter_excluded_urls
from .scoring import heuristic_score, calculate_reason

__all__ = [
    "deduplicate_results",
    "rerank_results",
    "filter_excluded_urls",
    "heuristic_score",
    "calculate_reason",
]
"""
Google ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ ìœ í‹¸ë¦¬í‹°
"""
import logging
from typing import List, Dict, Any
from urllib.parse import urlparse

logger = logging.getLogger(__name__)


def deduplicate_results(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì¤‘ë³µ ì œê±°ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    seen_urls = set()
    unique_results = []
    
    for result in results:
        url = result.get("link", "")
        if not url:
            continue
        
        # URL ì •ê·œí™” (í”„ë¡œí† ì½œ, www ì œê±° í›„ ë¹„êµ)
        parsed = urlparse(url)
        normalized = f"{parsed.netloc.replace('www.', '')}{parsed.path}"
        
        if normalized not in seen_urls:
            seen_urls.add(normalized)
            unique_results.append(result)
    
    logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {len(results)}ê°œ â†’ {len(unique_results)}ê°œ")
    
    return unique_results


def rerank_results(
    results: List[Dict[str, Any]],
    keywords: List[str]
) -> List[Dict[str, Any]]:
    """
    í‚¤ì›Œë“œ ë§¤ì¹­ë„ ê¸°ì¤€ ì¬ì •ë ¬
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì¬ì •ë ¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    def keyword_match_score(result: Dict[str, Any]) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        title = result.get("title", "").lower()
        snippet = result.get("snippet", "").lower()
        
        score = 0.0
        for keyword in keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in title:
                score += 2.0  # ì œëª© ë§¤ì¹­ ê°€ì¤‘ì¹˜ ë†’ìŒ
            if keyword_lower in snippet:
                score += 1.0
        
        return score
    
    # ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
    scored_results = [
        (result, keyword_match_score(result))
        for result in results
    ]
    scored_results.sort(key=lambda x: x[1], reverse=True)
    
    # ê²°ê³¼ë§Œ ë°˜í™˜
    reranked = [result for result, _ in scored_results]
    
    logger.info(f"ğŸ“Š ì¬ì •ë ¬ ì™„ë£Œ: {len(reranked)}ê°œ")
    
    return reranked


def filter_excluded_urls(
    results: List[Dict[str, Any]],
    exclude_urls: List[str]
) -> List[Dict[str, Any]]:
    """
    ì œì™¸ URL í•„í„°ë§
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        exclude_urls: ì œì™¸í•  URL ë¦¬ìŠ¤íŠ¸
        
    Returns:
        í•„í„°ë§ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    if not exclude_urls:
        return results
    
    # URL ì •ê·œí™”
    exclude_normalized = set()
    for url in exclude_urls:
        parsed = urlparse(url)
        normalized = f"{parsed.netloc.replace('www.', '')}{parsed.path}"
        exclude_normalized.add(normalized)
    
    filtered = []
    for result in results:
        url = result.get("link", "")
        parsed = urlparse(url)
        normalized = f"{parsed.netloc.replace('www.', '')}{parsed.path}"
        
        if normalized not in exclude_normalized:
            filtered.append(result)
    
    logger.info(f"ğŸš« ì œì™¸ URL í•„í„°ë§: {len(results)}ê°œ â†’ {len(filtered)}ê°œ")
    
    return filtered
"""
Google ê²€ìƒ‰ ê²°ê³¼ Heuristic ì ìˆ˜ ê³„ì‚°
"""
import logging
from typing import List
from ..config import flags

logger = logging.getLogger(__name__)


def heuristic_score(
    title: str,
    snippet: str,
    keywords: List[str],
    display_link: str
) -> float:
    """
    Heuristic ì ìˆ˜ ê³„ì‚°
    
    ê°€ì¤‘ì¹˜:
    - ì œëª© ë§¤ì¹­: 40%
    - ìŠ¤ë‹ˆí« ë§¤ì¹­: 30%
    - ë„ë©”ì¸ ì‹ ë¢°ë„: 30%
    
    Args:
        title: ê²€ìƒ‰ ê²°ê³¼ ì œëª©
        snippet: ê²€ìƒ‰ ê²°ê³¼ ìŠ¤ë‹ˆí«
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        display_link: ë„ë©”ì¸ (ì˜ˆ: naver.com)
        
    Returns:
        ì ìˆ˜ (0.0-10.0)
    """
    title_lower = title.lower()
    snippet_lower = snippet.lower()
    
    # 1. ì œëª© ë§¤ì¹­ ì ìˆ˜ (0-10)
    title_score = 0.0
    for keyword in keywords:
        keyword_lower = keyword.lower()
        if keyword_lower in title_lower:
            title_score += 10.0 / len(keywords)
    title_score = min(title_score, 10.0)
    
    # 2. ìŠ¤ë‹ˆí« ë§¤ì¹­ ì ìˆ˜ (0-10)
    snippet_score = 0.0
    for keyword in keywords:
        keyword_lower = keyword.lower()
        if keyword_lower in snippet_lower:
            snippet_score += 10.0 / len(keywords)
    snippet_score = min(snippet_score, 10.0)
    
    # 3. ë„ë©”ì¸ ì‹ ë¢°ë„ ì ìˆ˜ (0-10)
    domain_score = 5.0  # ê¸°ë³¸ê°’
    display_link_lower = display_link.lower()
    
    for trusted_domain in flags.TRUSTED_DOMAINS:
        if trusted_domain.lower() in display_link_lower:
            domain_score = 10.0
            break
    
    # ê°€ì¤‘ í‰ê· 
    final_score = (
        title_score * flags.WEIGHT_TITLE_MATCH +
        snippet_score * flags.WEIGHT_SNIPPET_MATCH +
        domain_score * flags.WEIGHT_DOMAIN_TRUST
    )
    
    return round(final_score, 2)


def calculate_reason(
    title: str,
    snippet: str,
    keywords: List[str],
    score: float,
    language: str = "ko"
) -> str:
    """
    Heuristic ì ìˆ˜ì— ëŒ€í•œ ì´ìœ  ìƒì„±
    
    Args:
        title: ê²€ìƒ‰ ê²°ê³¼ ì œëª©
        snippet: ê²€ìƒ‰ ê²°ê³¼ ìŠ¤ë‹ˆí«
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        score: ê³„ì‚°ëœ ì ìˆ˜
        language: ì‘ë‹µ ì–¸ì–´
        
    Returns:
        ì¶”ì²œ ì´ìœ  (í•­ìƒ "Heuristic" ë°˜í™˜)
    """
    return "Heuristic"
from setuptools import setup, find_packages

setup(
    name="googlekit",
    version="0.1.0",
    packages=find_packages(),
    install_requires=[
        "requests>=2.31.0",
        "aiohttp>=3.9.0",
        "pydantic>=2.0.0",
        "python-dotenv>=1.0.0",
        "openai>=1.0.0",
        "python-dateutil>=2.8.0",
    ],
    python_requires=">=3.11",
)
#!/bin/bash

echo "ğŸš€ GoogleKit ì„¤ì¹˜ ì‹œì‘..."

# ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
if [[ -z "$VIRTUAL_ENV" ]]; then
    echo "âš ï¸  ê²½ê³ : ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "   ê°€ìƒí™˜ê²½ì„ ë¨¼ì € í™œì„±í™”í•´ì£¼ì„¸ìš”: source .venv/bin/activate"
    exit 1
fi

# ì˜ì¡´ì„± ì„¤ì¹˜
echo "ğŸ“¦ ì˜ì¡´ì„± ì„¤ì¹˜ ì¤‘..."
pip install -e .

if [ $? -eq 0 ]; then
    echo "âœ… GoogleKit ì„¤ì¹˜ ì™„ë£Œ!"
else
    echo "âŒ ì„¤ì¹˜ ì‹¤íŒ¨"
    exit 1
fi
"""
Google ê²€ìƒ‰ ì¶”ì²œ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
"""
import asyncio
import time
import os
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from googlekit.service import GoogleService
from googlekit.models import GoogleRequest


async def test_basic_search():
    """ê¸°ë³¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 1: ê¸°ë³¸ ê²€ìƒ‰")
    print("=" * 60)
    
    service = GoogleService()
    request = GoogleRequest(
        lecture_id="test101",
        section_id=1,
        lecture_summary="í•˜ì´í¼ ìŠ¤ë ˆë”©(Hyper-Threading)ì€ ì¸í…”ì˜ ë™ì‹œ ë©€í‹°ìŠ¤ë ˆë”© ê¸°ìˆ ì…ë‹ˆë‹¤.",
        top_k=3,
        search_lang="en",
        language="ko"
    )
    
    start = time.perf_counter()
    results = await service.recommend_results(request)
    elapsed = int((time.perf_counter() - start) * 1000)
    
    print(f"\nâœ… ê²°ê³¼: {len(results)}ê°œ")
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed}ms")
    
    for i, result in enumerate(results, 1):
        print(f"\n--- ê²°ê³¼ {i} ---")
        print(f"ì œëª©: {result.search_result.title}")
        print(f"URL: {result.search_result.url}")
        print(f"ì ìˆ˜: {result.score}")
        print(f"ì´ìœ : {result.reason}")
        print(f"ë„ë©”ì¸: {result.search_result.display_link}")
    
    assert len(results) > 0, "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    print("\nâœ… í…ŒìŠ¤íŠ¸ 1 í†µê³¼!")


async def test_no_scoring_mode():
    """NO_SCORING ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 2: NO_SCORING ëª¨ë“œ")
    print("=" * 60)
    
    # NO_SCORING í”Œë˜ê·¸ í™œì„±í™”
    from googlekit.config import flags
    original_flag = flags.NO_SCORING
    flags.NO_SCORING = True
    
    try:
        service = GoogleService()
        request = GoogleRequest(
            lecture_id="test102",
            section_id=1,
            lecture_summary="CPU ë©€í‹°í”„ë¡œì„¸ì‹±ê³¼ ë©€í‹°ìŠ¤ë ˆë”©ì˜ ì°¨ì´ì ì„ ë¹„êµí•©ë‹ˆë‹¤.",
            top_k=5,
            search_lang="en",
            language="ko"
        )
        
        start = time.perf_counter()
        results = await service.recommend_results(request)
        elapsed = int((time.perf_counter() - start) * 1000)
        
        print(f"\nâœ… ê²°ê³¼: {len(results)}ê°œ")
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed}ms")
        
        # NO_SCORING ëª¨ë“œ ê²€ì¦
        for i, result in enumerate(results, 1):
            print(f"\n--- ê²°ê³¼ {i} ---")
            print(f"ì œëª©: {result.search_result.title}")
            print(f"ì ìˆ˜: {result.score} (should be 10.0)")
            print(f"ì´ìœ : {result.reason} (should be 'search')")
            
            assert result.score == 10.0, f"NO_SCORING ëª¨ë“œì—ì„œ ì ìˆ˜ê°€ 10.0ì´ ì•„ë‹™ë‹ˆë‹¤: {result.score}"
            assert result.reason == "search", f"NO_SCORING ëª¨ë“œì—ì„œ ì´ìœ ê°€ 'search'ê°€ ì•„ë‹™ë‹ˆë‹¤: {result.reason}"
        
        print("\nâœ… í…ŒìŠ¤íŠ¸ 2 í†µê³¼!")
    
    finally:
        # í”Œë˜ê·¸ ì›ìƒë³µêµ¬
        flags.NO_SCORING = original_flag


async def test_heuristic_mode():
    """Heuristic ê²€ì¦ ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 3: Heuristic ê²€ì¦ ëª¨ë“œ")
    print("=" * 60)
    
    service = GoogleService()
    request = GoogleRequest(
        lecture_id="test103",
        section_id=1,
        lecture_summary="ìš´ì˜ì²´ì œì˜ ë©”ëª¨ë¦¬ ê´€ë¦¬ ê¸°ë²•ì— ëŒ€í•´ í•™ìŠµí•©ë‹ˆë‹¤.",
        top_k=3,
        verify_google=False,  # Heuristic ëª¨ë“œ
        search_lang="en",
        language="ko"
    )
    
    start = time.perf_counter()
    results = await service.recommend_results(request)
    elapsed = int((time.perf_counter() - start) * 1000)
    
    print(f"\nâœ… ê²°ê³¼: {len(results)}ê°œ")
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed}ms")
    
    for i, result in enumerate(results, 1):
        print(f"\n--- ê²°ê³¼ {i} ---")
        print(f"ì œëª©: {result.search_result.title}")
        print(f"ì ìˆ˜: {result.score}")
        print(f"ì´ìœ : {result.reason}")
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ 3 í†µê³¼!")


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ GoogleKit í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    try:
        await test_basic_search()
        await test_no_scoring_mode()
        await test_heuristic_mode()
        
        print("\n" + "=" * 60)
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("=" * 60)
    
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
"""
OpenAlexKit - í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰ ë° ì¶”ì²œ ëª¨ë“ˆ

LiveNote í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ OpenAlex API ê¸°ë°˜ ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œ
"""

from .service import OpenAlexService
from .models import (
    OpenAlexRequest,
    OpenAlexResponse,
    PaperInfo,
    RAGChunk,
    PreviousSectionSummary,
)

__version__ = "0.1.0"

__all__ = [
    "OpenAlexService",
    "OpenAlexRequest",
    "OpenAlexResponse",
    "PaperInfo",
    "RAGChunk",
    "PreviousSectionSummary",
]
"""
OpenAlexKit API í´ë¼ì´ì–¸íŠ¸
"""
"""
OpenAlex API í´ë¼ì´ì–¸íŠ¸
"""
import logging
from typing import List, Dict, Optional
import httpx

from ..config.openalex_config import OpenAlexConfig
from ..utils.parser import parse_abstract_inverted_index

logger = logging.getLogger(__name__)


class OpenAlexAPIClient:
    """OpenAlex API í´ë¼ì´ì–¸íŠ¸"""
    
    BASE_URL = "https://api.openalex.org"
    
    def __init__(self):
        self.http_client = httpx.AsyncClient(
            timeout=OpenAlexConfig.TIMEOUT,
            limits=httpx.Limits(
                max_connections=20,  # 10â†’20 (ë³‘ë ¬ ì²˜ë¦¬ ê°œì„ )
                max_keepalive_connections=10
            ),
            http2=True  # HTTP/2 í™œì„±í™” (ë©€í‹°í”Œë ‰ì‹±)
        )
    
    async def search_papers(
        self, 
        query: Dict, 
        exclude_ids: Optional[List[str]] = None,
        sort_by: str = "relevance"
    ) -> List[Dict]:
        """
        OpenAlex API ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (tokens, year_from í¬í•¨)
            exclude_ids: ì œì™¸í•  ë…¼ë¬¸ ID ë¦¬ìŠ¤íŠ¸
            sort_by: ì •ë ¬ ê¸°ì¤€ ("relevance", "cited_by_count", "hybrid")
                - "relevance": í‚¤ì›Œë“œ ì—°ê´€ì„± ìš°ì„  (ê¸°ë³¸ê°’)
                - "cited_by_count": ì¸ìš©ìˆ˜ ìš°ì„ 
                - "hybrid": ì—°ê´€ì„± ë†’ì€ ë…¼ë¬¸ ì¤‘ ì¸ìš©ìˆ˜ ìƒìœ„ ì„ íƒ
            
        Returns:
            List[Dict]: íŒŒì‹±ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ê²€ìƒ‰ ë¬¸ìì—´ ìƒì„± (tokens ê¸°ë°˜)
            tokens = query.get("tokens", [])
            search_str = " ".join(tokens)
            
            # year_from ì²˜ë¦¬
            year_from = query.get("year_from", OpenAlexConfig.DEFAULT_YEAR_FROM)
            
            # í•„í„° êµ¬ì„±
            filters = [
                f"from_publication_date:{year_from}-01-01",
                "language:en",
                "is_paratext:false",
                "type:article",
            ]
            
            # ì •ë ¬ ì˜µì…˜ ê²°ì •
            if sort_by == "cited_by_count":
                sort_param = "cited_by_count:desc"
            elif sort_by == "hybrid":
                # Hybrid ëª¨ë“œ: ë” ë§ì€ ë…¼ë¬¸ ê°€ì ¸ì˜¨ í›„ ì¬ì •ë ¬
                sort_param = "relevance_score:desc"
                per_page = OpenAlexConfig.PER_PAGE * 2  # 50ê°œ ê°€ì ¸ì˜¤ê¸°
            else:  # relevance (ê¸°ë³¸ê°’)
                sort_param = "relevance_score:desc"
                per_page = OpenAlexConfig.PER_PAGE
            
            # OpenAlex API í˜¸ì¶œ
            params = {
                "search": search_str,
                "filter": ",".join(filters),
                "sort": sort_param,
                "per_page": per_page if sort_by == "hybrid" else OpenAlexConfig.PER_PAGE
            }
            
            logger.info(f"ğŸ” OpenAlex API ìš”ì²­:")
            logger.info(f"   â”œâ”€ URL: {self.BASE_URL}/works")
            logger.info(f"   â”œâ”€ search: \"{search_str}\"")
            logger.info(f"   â”œâ”€ filters: {filters}")
            logger.info(f"   â”œâ”€ sort: {sort_param}")
            logger.info(f"   â””â”€ per_page: {params['per_page']}")
            
            response = await self.http_client.get(
                f"{self.BASE_URL}/works",
                params=params
            )
            response.raise_for_status()
            data = response.json()
            
            works = data.get("results", [])
            logger.info(f"ğŸ“„ OpenAlex ì›ë³¸ ê²€ìƒ‰: {len(works)}ê°œ")
            
            if len(works) == 0:
                logger.warning(f"âš ï¸  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ. ê°€ëŠ¥í•œ ì›ì¸:")
                logger.warning(f"   1. ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ êµ¬ì²´ì : \"{search_str}\"")
                logger.warning(f"   2. TOKEN ìˆ˜ê°€ ë§ìŒ: {len(tokens)}ê°œ")
                logger.warning(f"   3. year_from í•„í„°: {year_from}")
                logger.warning(f"   í•´ê²°: TOKEN ì¤„ì´ê¸° (2-3ê°œ), year_from ì¡°ì • (2015)")
                return []
            
            # íŒŒì‹± ë° í•„í„°ë§
            papers = []
            exclude_set = set(exclude_ids or [])
            
            for work in works:
                paper = self._parse_paper(work)
                
                if paper is None:
                    continue
                
                # ì œì™¸ ID ì²´í¬
                if paper.get("id") in exclude_set:
                    logger.debug(f"â­ï¸  ì œì™¸ë¨ (exclude_ids): {paper.get('title', '')[:50]}")
                    continue
                
                papers.append(paper)
            
            # Hybrid ëª¨ë“œ: ì—°ê´€ì„± ì ìˆ˜ 0.5 ì´ìƒì¸ ë…¼ë¬¸ ì¤‘ ì¸ìš©ìˆ˜ë¡œ ì¬ì •ë ¬
            if sort_by == "hybrid" and papers:
                # ì—°ê´€ì„± ì„ê³„ê°’ (ìƒìœ„ 60%)
                threshold = max(p.get("relevance_score", 0) for p in papers) * 0.6
                relevant_papers = [p for p in papers if p.get("relevance_score", 0) >= threshold]
                
                # ì¸ìš©ìˆ˜ë¡œ ì •ë ¬
                relevant_papers.sort(key=lambda x: x.get("cited_by_count", 0), reverse=True)
                papers = relevant_papers[:OpenAlexConfig.PER_PAGE]
                
                logger.info(f"ğŸ”„ Hybrid ì¬ì •ë ¬: ì—°ê´€ì„± {threshold:.2f} ì´ìƒ ë…¼ë¬¸ {len(relevant_papers)}ê°œ â†’ ì¸ìš©ìˆ˜ ìƒìœ„ {len(papers)}ê°œ ì„ íƒ")
            
            logger.info(f"âœ… OpenAlex íŒŒì‹± ì™„ë£Œ: {len(papers)}ê°œ")
            return papers
            
        except httpx.TimeoutException:
            logger.error("âŒ OpenAlex API íƒ€ì„ì•„ì›ƒ")
            return []
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ OpenAlex API HTTP ì˜¤ë¥˜: {e.response.status_code}")
            return []
        except Exception as e:
            logger.error(f"âŒ OpenAlex API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_paper(self, work: Dict) -> Optional[Dict]:
        """
        OpenAlex work ê°ì²´ íŒŒì‹±
        
        Args:
            work: OpenAlex work ê°ì²´
            
        Returns:
            Dict or None (í•„í„°ë§ë¨)
        """
        try:
            # ê¸°ë³¸ ì •ë³´
            paper_id = work.get("id", "")
            title = work.get("title", "")
            year = work.get("publication_year")
            cited_by_count = work.get("cited_by_count", 0)
            doi = work.get("doi", "")
            url = doi if doi else paper_id
            
            # ì´ˆë¡ íŒŒì‹±
            abstract_inverted = work.get("abstract_inverted_index")
            abstract = parse_abstract_inverted_index(abstract_inverted)
            
            # ì €ì íŒŒì‹±
            authors = []
            authorships = work.get("authorships", [])
            for authorship in authorships[:5]:  # ìƒìœ„ 5ëª…ë§Œ
                author = authorship.get("author", {})
                name = author.get("display_name")
                if name:
                    authors.append(name)
            
            # ì´ˆë¡ ì—†ëŠ” ë…¼ë¬¸ í•„í„°ë§
            no_abstract = not abstract or len(abstract.strip()) < 50
            
            if no_abstract:
                # ì´ˆë¡ ì—†ê³  ì¸ìš© ìˆ˜ ë‚®ìŒ â†’ ì œì™¸
                if cited_by_count < 100:
                    logger.debug(
                        f"â­ï¸  ì œì™¸ë¨ (ì´ˆë¡ ì—†ìŒ + ì¸ìš© ìˆ˜ {cited_by_count}): {title[:50]}"
                    )
                    return None
            
            return {
                "id": paper_id,
                "title": title,
                "abstract": abstract,
                "year": year,
                "cited_by_count": cited_by_count,
                "url": url,
                "authors": authors,
                "no_abstract": no_abstract,
                "relevance_score": work.get("relevance_score", 0.0)
            }
            
        except Exception as e:
            logger.error(f"âŒ ë…¼ë¬¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    async def close(self):
        """HTTP í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ"""
        await self.http_client.aclose()
"""
OpenAlexKit ì„¤ì • ê´€ë¦¬
"""
"""
OpenAlex Provider ì „ìš© í”Œë˜ê·¸
"""

# â”â”â” ê²€ì¦ ìŠ¤ìœ„ì¹˜ â”â”â”
#NO_SCORING = True  # Trueì´ë©´ ê²€ì¦ ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
NO_SCORING = False  # Trueì´ë©´ ê²€ì¦ ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
VERIFY_OPENALEX_DEFAULT = True  # ê¸°ë³¸ê°’: LLM ê²€ì¦

# â”â”â” í† í° ìƒì„± ì„¤ì • â”â”â”
TOKEN_MIN = 2  # ìµœì†Œ ê²€ìƒ‰ í† í° ê°œìˆ˜
TOKEN_MAX = 3  # ìµœëŒ€ ê²€ìƒ‰ í† í° ê°œìˆ˜ (4â†’3, ê²€ìƒ‰ ê²°ê³¼ ì¦ê°€)
"""
OpenAlex ëª¨ë“ˆ ì„¤ì •
"""
import os
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class OpenAlexConfig:
    """OpenAlex ëª¨ë“ˆ ì„¤ì •"""
    
    # â”â”â” OpenAI API â”â”â”
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
    
    # â”â”â” OpenAlex API â”â”â”
    TIMEOUT: int = 15  # HTTP íƒ€ì„ì•„ì›ƒ (ì´ˆ) - 20â†’15 (25% ê°ì†Œ)
    PER_PAGE: int = 40  # í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜ - 50â†’40 (API ì‘ë‹µ ì†ë„ ê°œì„ ) ì‘ë‹µ ì†ë„ ê°œì„ ì˜ í•µì‹¬
    
    # â”â”â” ê¸°ë³¸ê°’ â”â”â”
    DEFAULT_LANGUAGE: str = "ko"
    DEFAULT_TOP_K: int = 5
    DEFAULT_YEAR_FROM: int = 1930
    
    # â”â”â” ì œí•œ â”â”â”
    CARD_LIMIT: int = 13  # ê²€ì¦ ëŒ€ìƒ ìµœëŒ€ ìˆ˜ (13<-10, 23% ê°ì†Œ)
    MAX_TOP_K: int = 10   # ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜
    
    # â”â”â” LLM ì„¤ì • â”â”â”
    #LLM_MODEL: str = "gpt-4o-mini"
    LLM_MODEL: str = "gpt-4o"
    LLM_TEMPERATURE: float = 0.2  # 0.3â†’0.2 (ë” ê²°ì •ì , ë¹ ë¥¸ ì‘ë‹µ)
    MAX_TOKENS_QUERY: int = 150   # 100<-80 (20% ê°ì†Œ)
    MAX_TOKENS_SCORE: int = 200   # 120â†’200 (reason ì˜ë¦¼ ë°©ì§€)
    
    # â”â”â” ì´ˆë¡ ê¸¸ì´ â”â”â”
    ABSTRACT_MAX_LENGTH: int = 400  # 500â†’400 (20% ê°ì†Œ)
    
    # â”â”â” ë³‘ë ¬ ì²˜ë¦¬ â”â”â”
    VERIFY_CONCURRENCY: int = 20  # 15â†’20 (33% ì¦ê°€, YouTubeì™€ ë™ì¼)
    
    @classmethod
    def validate(cls):
        """ì„¤ì • ê²€ì¦"""
        if not cls.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if cls.CARD_LIMIT < 1:
            raise ValueError(f"CARD_LIMITì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤: {cls.CARD_LIMIT}")
        
        if cls.VERIFY_CONCURRENCY < 1:
            raise ValueError(
                f"VERIFY_CONCURRENCYëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤: {cls.VERIFY_CONCURRENCY}"
            )
"""
OpenAlex Provider ì „ìš© í”„ë¡¬í”„íŠ¸
"""
from . import flags


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í”„ë¡¬í”„íŠ¸ 1: ì¿¼ë¦¬ ìƒì„±
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

QUERY_GENERATION_PROMPT = f"""You are to produce terse technical search tokens for OpenAlex.

Current section summary:
{{section_summary}}

Previous sections context:
{{previous_summaries}}

RAG context:
{{rag_context}}

Return a JSON with {flags.TOKEN_MIN}~{flags.TOKEN_MAX} technical English tokens suitable for academic paper search:
{{{{
  "tokens": ["term1", "term2", ...]
}}}}

Rules:
- Extract core concepts and technical terms from all provided context
- Use precise academic terminology
- Expand abbreviations where needed
- Include field-specific keywords
- Keep it concise ({flags.TOKEN_MIN}-{flags.TOKEN_MAX} tokens max)
"""


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í”„ë¡¬í”„íŠ¸ 2: ë…¼ë¬¸ ê²€ì¦
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

SCORE_PAPER_PROMPT = """Current section summary: {section_summary}

Keywords: {keywords}
Paper title: {title}
Abstract: {abstract}
Publication year: {year}
Citation count: {cited_by_count}

Does this paper directly cover the core concept discussed in the lecture?

Scoring (strict criteria):
- 10: Seminal/foundational paper that FIRST introduced the concept
   - Highly cited (typically >10,000) AND matches core concept perfectly
- 9: Directly addresses core concept with clear methodology/application
- 7-8: Covers core concept but partially or indirectly
- 4-6: Related background but slightly off-topic
- 1-3: Only keyword overlap, content unrelated

Return JSON (reason: one sentence, no line breaks, in {language}):
{{"score": <number>, "reason": "clear one-sentence evaluation"}}
"""

"""
OpenAlexKit LLM í´ë¼ì´ì–¸íŠ¸
"""
"""
OpenAI API í´ë¼ì´ì–¸íŠ¸ (LLM)
"""
import json
import logging
from typing import Dict, Any
import httpx
from openai import AsyncOpenAI

from ..config.openalex_config import OpenAlexConfig
from ..config import prompts

logger = logging.getLogger(__name__)


class OpenAIClient:
    """OpenAI API í´ë¼ì´ì–¸íŠ¸ (ì¿¼ë¦¬ ìƒì„± + ë…¼ë¬¸ ê²€ì¦)"""
    
    def __init__(self):
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        http_client = httpx.AsyncClient(
            http2=True,
            timeout=15.0,
            limits=httpx.Limits(max_connections=20, max_keepalive_connections=10)
        )
        
        self.client = AsyncOpenAI(
            api_key=OpenAlexConfig.OPENAI_API_KEY,
            http_client=http_client
        )
    
    async def generate_query(self, request_data: Dict) -> Dict[str, Any]:
        """
        ì„¹ì…˜ ìš”ì•½ â†’ OpenAlex ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (LLM)
        
        Args:
            request_data: {
                "section_summary": str,
                "previous_summaries": List[PreviousSectionSummary],
                "rag_context": List[RAGChunk]
            }
            
        Returns:
            {"tokens": ["term1", "term2", ...]}
        """
        try:
            # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            section_summary = request_data.get("section_summary", "")
            previous_summaries = request_data.get("previous_summaries", [])
            rag_context = request_data.get("rag_context", [])
            
            # Previous summaries í…ìŠ¤íŠ¸í™”
            prev_text = ""
            if previous_summaries:
                prev_items = [
                    f"ì„¹ì…˜ {ps.section_id}: {ps.summary}" 
                    for ps in previous_summaries
                ]
                prev_text = "\n".join(prev_items)
            else:
                prev_text = "(ì—†ìŒ)"
            
            # RAG context í…ìŠ¤íŠ¸í™”
            rag_text = ""
            if rag_context:
                rag_items = [
                    f"[{rc.score:.2f}] {rc.text[:100]}..." 
                    for rc in rag_context[:3]  # ìƒìœ„ 3ê°œë§Œ
                ]
                rag_text = "\n".join(rag_items)
            else:
                rag_text = "(ì—†ìŒ)"
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = prompts.QUERY_GENERATION_PROMPT.format(
                section_summary=section_summary,
                previous_summaries=prev_text,
                rag_context=rag_text
            )
            
            logger.info("ğŸ¤– LLM ì¿¼ë¦¬ ìƒì„± ì‹œì‘...")
            
            # OpenAI API í˜¸ì¶œ
            response = await self.client.chat.completions.create(
                model=OpenAlexConfig.LLM_MODEL,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=OpenAlexConfig.LLM_TEMPERATURE,
                max_tokens=OpenAlexConfig.MAX_TOKENS_QUERY
            )
            
            # ì‘ë‹µ íŒŒì‹±
            content = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± (ì½”ë“œ ë¸”ë¡ ì œê±°)
            if "```" in content:
                content = content.split("```")[1]
                if content.startswith("json"):
                    content = content[4:]
            
            result = json.loads(content)
            tokens = result.get("tokens", [])
            
            logger.info(f"âœ… LLM ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ: {tokens}")
            
            return {"tokens": tokens}
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}, content: {content}")
            # Fallback: ì„¹ì…˜ ìš”ì•½ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
            words = section_summary.split()[:3]
            return {"tokens": words}
        except Exception as e:
            logger.error(f"âŒ LLM ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"tokens": []}
    
    async def score_paper(
        self, 
        paper: Dict, 
        section_summary: str,
        keywords: str,
        language: str = "Korean"
    ) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ë…¼ë¬¸ ê²€ì¦ (LLM)
        
        Args:
            paper: ë…¼ë¬¸ ì •ë³´ (title, abstract, year, cited_by_count)
            section_summary: í˜„ì¬ ì„¹ì…˜ ìš”ì•½
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ
            language: ì‘ë‹µ ì–¸ì–´ (Korean, English, etc.)
            
        Returns:
            {"score": float, "reason": str}
        """
        try:
            # í…ìŠ¤íŠ¸ ì •ì œ (JSON ê¹¨ì§ ë°©ì§€)
            title = paper.get("title", "").replace("\n", " ").replace('"', "'").strip()
            abstract = paper.get("abstract", "").replace("\n", " ").replace('"', "'").strip()
            section_clean = section_summary.replace("\n", " ").replace('"', "'").strip()
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = prompts.SCORE_PAPER_PROMPT.format(
                section_summary=section_clean,
                keywords=keywords,
                title=title,
                abstract=abstract[:OpenAlexConfig.ABSTRACT_MAX_LENGTH],
                year=paper.get("year", "N/A"),
                cited_by_count=paper.get("cited_by_count", 0),
                language=language
            )
            
            # OpenAI API í˜¸ì¶œ
            response = await self.client.chat.completions.create(
                model=OpenAlexConfig.LLM_MODEL,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=OpenAlexConfig.LLM_TEMPERATURE,
                max_tokens=OpenAlexConfig.MAX_TOKENS_SCORE
            )
            
            # ì‘ë‹µ íŒŒì‹±
            content = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± (ì½”ë“œ ë¸”ë¡ ì œê±°)
            if "```" in content:
                content = content.split("```")[1]
                if content.startswith("json"):
                    content = content[4:]
            
            result = json.loads(content)
            score = float(result.get("score", 5.0))
            reason = result.get("reason", "ê²€ì¦ ì™„ë£Œ")
            
            return {"score": score, "reason": reason}
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}, content: {content}")
            return {"score": 5.0, "reason": "ê²€ì¦ ì‹¤íŒ¨ (JSON íŒŒì‹± ì˜¤ë¥˜)"}
        except Exception as e:
            logger.error(f"âŒ LLM ë…¼ë¬¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {"score": 5.0, "reason": "ê²€ì¦ ì‹¤íŒ¨ (API ì˜¤ë¥˜)"}
"""
OpenAlexKit ë°ì´í„° ëª¨ë¸ ì •ì˜
"""
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field


class RAGChunk(BaseModel):
    """RAG ê²€ìƒ‰ ê²°ê³¼ ì²­í¬"""
    text: str = Field(..., description="ì²­í¬ í…ìŠ¤íŠ¸ ë‚´ìš©")
    score: float = Field(..., description="ìœ ì‚¬ë„ ì ìˆ˜")
    metadata: Optional[Dict[str, Any]] = Field(default=None, description="ë©”íƒ€ë°ì´í„°")


class PreviousSectionSummary(BaseModel):
    """ì´ì „ ì„¹ì…˜ ìš”ì•½"""
    section_id: int = Field(..., description="ì„¹ì…˜ ë²ˆí˜¸")
    summary: str = Field(..., description="ì„¹ì…˜ ìš”ì•½ í…ìŠ¤íŠ¸")
    timestamp: Optional[int] = Field(default=None, description="íƒ€ì„ìŠ¤íƒ¬í”„ (ms)")


class OpenAlexRequest(BaseModel):
    """OpenAlex ë…¼ë¬¸ ì¶”ì²œ ìš”ì²­"""
    
    # â”â”â” í•„ìˆ˜ í•„ë“œ â”â”â”
    lecture_id: str = Field(..., description="ê°•ì˜ ì„¸ì…˜ ID (ì¶”ì ìš©)")
    section_id: int = Field(..., ge=1, description="í˜„ì¬ ì„¹ì…˜ ë²ˆí˜¸")
    section_summary: str = Field(..., min_length=10, description="í˜„ì¬ ì„¹ì…˜ ìš”ì•½")
    
    # â”â”â” ì„ íƒ í•„ë“œ â”â”â”
    language: str = Field(default="ko", description="ì‘ë‹µ ì–¸ì–´ (ko/en)")
    top_k: int = Field(default=5, ge=1, le=10, description="ì¶”ì²œ ë…¼ë¬¸ ê°œìˆ˜")
    verify_openalex: bool = Field(
        default=True, 
        description="LLM ê²€ì¦ ì—¬ë¶€ (True: LLM, False: Heuristic)"
    )
    
    # â”â”â” ì»¨í…ìŠ¤íŠ¸ í•„ë“œ â”â”â”
    previous_summaries: List[PreviousSectionSummary] = Field(
        default_factory=list,
        description="ì´ì „ Nê°œ ì„¹ì…˜ ìš”ì•½ (ì»¨í…ìŠ¤íŠ¸ í™•ì¥ìš©)"
    )
    rag_context: List[RAGChunk] = Field(
        default_factory=list,
        description="RAG ê²€ìƒ‰ ê²°ê³¼ (ê°•ì˜ë…¸íŠ¸/ì´ì „ ì„¹ì…˜)"
    )
    
    # â”â”â” ê²€ìƒ‰ ì œì–´ í•„ë“œ â”â”â”
    year_from: int = Field(default=2015, description="ë…¼ë¬¸ ì¶œíŒ ì—°ë„ í•„í„° (YYYY)")
    exclude_ids: List[str] = Field(
        default_factory=list,
        description="ì œì™¸í•  ë…¼ë¬¸ ID ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ë°©ì§€)"
    )
    sort_by: str = Field(
        default="hybrid",
        description="ì •ë ¬ ê¸°ì¤€ (relevance: í‚¤ì›Œë“œ ì—°ê´€ì„±, cited_by_count: ì¸ìš©ìˆ˜, hybrid: ê· í˜•)"
    )
    min_score: float = Field(
        default=5.0,
        ge=0.0,
        le=10.0,
        description="ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’ (ì´ ì ìˆ˜ ë¯¸ë§Œ ë…¼ë¬¸ ì œì™¸, ê¸°ë³¸: 5.0)"
    )


class PaperInfo(BaseModel):
    """ë…¼ë¬¸ ìƒì„¸ ì •ë³´"""
    url: str = Field(..., description="ë…¼ë¬¸ URL (DOI or OpenAlex ID)")
    title: str = Field(..., description="ë…¼ë¬¸ ì œëª©")
    abstract: str = Field(..., description="ë…¼ë¬¸ ì´ˆë¡ (ìµœëŒ€ 500ì)")
    year: Optional[int] = Field(None, description="ì¶œíŒ ì—°ë„")
    cited_by_count: int = Field(default=0, description="í”¼ì¸ìš© ìˆ˜")
    authors: List[str] = Field(default_factory=list, description="ì €ì ë¦¬ìŠ¤íŠ¸")


class OpenAlexResponse(BaseModel):
    """OpenAlex ë…¼ë¬¸ ì¶”ì²œ ì‘ë‹µ"""
    lecture_id: str = Field(..., description="ê°•ì˜ ì„¸ì…˜ ID")
    section_id: int = Field(..., description="ì„¹ì…˜ ë²ˆí˜¸")
    paper_info: PaperInfo = Field(..., description="ë…¼ë¬¸ ì •ë³´")
    reason: str = Field(..., description="ì¶”ì²œ ì´ìœ  (1-2ë¬¸ì¥, í•œêµ­ì–´/ì˜ì–´)")
    score: float = Field(..., ge=0.0, le=15.0, description="ê´€ë ¨ë„ ì ìˆ˜ (0-10, LLMì´ ì´ˆê³¼ ê°€ëŠ¥)")
"""
OpenAlexKit í•µì‹¬ ì„œë¹„ìŠ¤
"""
import asyncio
import logging
from typing import List

from .models import OpenAlexRequest, OpenAlexResponse, PaperInfo
from .config.openalex_config import OpenAlexConfig
from .config import flags
from .api.openalex_client import OpenAlexAPIClient
from .llm.openai_client import OpenAIClient
from .utils.filters import deduplicate_papers, rerank_papers

logger = logging.getLogger(__name__)


class OpenAlexService:
    """OpenAlex ë…¼ë¬¸ ì¶”ì²œ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        # ì„¤ì • ê²€ì¦
        OpenAlexConfig.validate()
        
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.api_client = OpenAlexAPIClient()
        self.llm_client = OpenAIClient()
    
    async def recommend_papers(
        self, 
        request: OpenAlexRequest
    ) -> List[OpenAlexResponse]:
        """
        ë…¼ë¬¸ ì¶”ì²œ (ë³‘ë ¬ ê²€ì¦)
        
        íë¦„:
        1. ì„¹ì…˜ ìš”ì•½ â†’ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (LLM)
        2. OpenAlex API í˜¸ì¶œ (í•„í„° ì ìš©)
        3. ë…¼ë¬¸ íŒŒì‹± + ì¤‘ë³µ ì œê±° + ì¬ë­í‚¹
        4. ìƒìœ„ Nê°œ ì„ íƒ (CARD_LIMIT)
        5. ì¡°ê±´ë¶€ ê²€ì¦:
           - verify_openalex=True: LLM ë³‘ë ¬ ê²€ì¦
           - verify_openalex=False: Heuristic ìŠ¤ì½”ì–´ë§
        6. ì ìˆ˜ ìˆœ ì •ë ¬ â†’ top_k ë°˜í™˜
        
        Args:
            request: OpenAlexRequest
        
        Returns:
            List[OpenAlexResponse]: ì¶”ì²œ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ (top_kê°œ)
        """
        logger.info(
            f"ğŸš€ ë…¼ë¬¸ ì¶”ì²œ ì‹œì‘: lecture={request.lecture_id}, "
            f"section={request.section_id}, verify={request.verify_openalex}"
        )
        
        try:
            # 1. ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (LLM)
            logger.info(f"ğŸ” OpenAlex ê²€ìƒ‰ ì‹œì‘ (lecture={request.lecture_id}, section={request.section_id})")
            logger.info(f"   â”œâ”€ section_summary: {request.section_summary[:100]}...")
            logger.info(f"   â”œâ”€ previous_summaries: {len(request.previous_summaries)}ê°œ")
            logger.info(f"   â””â”€ rag_context: {len(request.rag_context)}ê°œ")
            
            query = await self._generate_search_query(request)
            
            tokens = query.get('tokens', [])
            logger.info(f"ğŸ“ ìƒì„±ëœ ì¿¼ë¦¬:")
            logger.info(f"   â”œâ”€ tokens: {tokens} (ì´ {len(tokens)}ê°œ)")
            logger.info(f"   â””â”€ year_from: {query.get('year_from')}")
            
            if not tokens:
                logger.warning("âš ï¸  ê²€ìƒ‰ í† í°ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return []
            
            # TOKEN ìˆ˜ ê²½ê³ 
            if len(tokens) > 3:
                logger.warning(f"âš ï¸  TOKEN ìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤ ({len(tokens)}ê°œ). ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                logger.warning(f"   ê¶Œì¥: 2-3ê°œ TOKEN (í˜„ì¬: {tokens})")
            
            # 2. OpenAlex API í˜¸ì¶œ
            logger.info(f"ğŸŒ OpenAlex API í˜¸ì¶œ:")
            logger.info(f"   â”œâ”€ search_str: {' '.join(tokens)}")
            logger.info(f"   â”œâ”€ year_from: {query.get('year_from')}")
            logger.info(f"   â”œâ”€ sort_by: {request.sort_by}")
            logger.info(f"   â””â”€ exclude_ids: {len(request.exclude_ids)}ê°œ")
            
            papers = await self.api_client.search_papers(
                query=query,
                exclude_ids=request.exclude_ids,
                sort_by=request.sort_by
            )
            
            if not papers:
                logger.warning(f"âš ï¸  ê²€ìƒ‰ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤ (tokens={tokens})")
                logger.warning(f"   ê°€ëŠ¥í•œ ì›ì¸:")
                logger.warning(f"   1. TOKENì´ ë„ˆë¬´ ë§ìŒ ({len(tokens)}ê°œ) â†’ 2-3ê°œ ê¶Œì¥")
                logger.warning(f"   2. year_fromì´ ë¶€ì ì ˆ ({query.get('year_from')}) â†’ 2015 ê¶Œì¥")
                logger.warning(f"   3. ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ êµ¬ì²´ì  â†’ ì¼ë°˜ì ì¸ í•™ìˆ  ìš©ì–´ ì‚¬ìš©")
                return []
            
            logger.info(f"ğŸ“š ê²€ìƒ‰ëœ ë…¼ë¬¸: {len(papers)}ê°œ")
            logger.info(f"   ìƒìœ„ 3ê°œ ì œëª©:")
            for i, paper in enumerate(papers[:3], 1):
                logger.info(f"   {i}. {paper.get('title', 'Unknown')[:80]}...")
            
            # 3. ì¤‘ë³µ ì œê±° + ì¬ë­í‚¹
            before_dedup = len(papers)
            papers = deduplicate_papers(papers)
            papers = rerank_papers(papers, query)
            logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {before_dedup}ê°œ â†’ {len(papers)}ê°œ")
            
            # 4. ìƒìœ„ Nê°œ ì„ íƒ (CARD_LIMIT)
            papers = papers[:OpenAlexConfig.CARD_LIMIT]
            logger.info(f"ğŸ“„ ê²€ì¦ ëŒ€ìƒ: {len(papers)}ê°œ (ìƒí•œ: {OpenAlexConfig.CARD_LIMIT})")
            logger.info(f"   â”œâ”€ verify_openalex: {request.verify_openalex}")
            logger.info(f"   â”œâ”€ NO_SCORING: {flags.NO_SCORING}")
            logger.info(f"   â””â”€ min_score: {request.min_score}")
            
            # ğŸš€ NO_SCORING ëª¨ë“œ: ê²€ì¦ ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜
            if flags.NO_SCORING:
                logger.info("âš¡ NO_SCORING ëª¨ë“œ: ê²€ì¦ ìŠ¤í‚µ")
                results = []
                for paper in papers[:request.top_k]:
                    info = PaperInfo(
                        title=paper.get("title", "Unknown"),
                        authors=paper.get("authors", []),
                        year=paper.get("publication_year"),
                        citations=paper.get("cited_by_count", 0),
                        url=paper.get("url", ""),
                        abstract=paper.get("abstract", "No abstract available")[:500]
                    )
                    results.append(OpenAlexResponse(
                        lecture_id=request.lecture_id,
                        section_id=request.section_id,
                        paper_info=info,
                        reason="search",
                        score=10.0
                    ))
                logger.info(f"âœ… NO_SCORING ê²°ê³¼: {len(results)}ê°œ ë°˜í™˜")
                return results
            
            # 5. ì¡°ê±´ë¶€ ê²€ì¦
            if request.verify_openalex:
                # LLM ë³‘ë ¬ ê²€ì¦
                logger.info(f"âœ¨ LLM ë³‘ë ¬ ê²€ì¦ ì‹œì‘:")
                logger.info(f"   â”œâ”€ ëŒ€ìƒ: {len(papers)}ê°œ")
                logger.info(f"   â”œâ”€ ë™ì‹œì„±: {OpenAlexConfig.VERIFY_CONCURRENCY}")
                logger.info(f"   â””â”€ ëª¨ë¸: {OpenAlexConfig.LLM_MODEL}")
                results = await self._verify_papers_parallel(papers, request, query)
            else:
                # Heuristic ìŠ¤ì½”ì–´ë§
                logger.info(f"ğŸ”¢ Heuristic ìŠ¤ì½”ì–´ë§ ì‹œì‘ ({len(papers)}ê°œ)")
                results = self._heuristic_score(papers, query, request)
            
            # 6. ì ìˆ˜ í•„í„°ë§ (min_score ì´ìƒë§Œ ì„ íƒ)
            filtered_results = [r for r in results if r.score >= request.min_score]
            
            if len(filtered_results) < len(results):
                filtered_count = len(results) - len(filtered_results)
                logger.info(
                    f"ğŸ” ì ìˆ˜ í•„í„°ë§: {len(results)}ê°œ â†’ {len(filtered_results)}ê°œ "
                    f"(ì œì™¸: {filtered_count}ê°œ, min_score: {request.min_score})"
                )
                # ì œì™¸ëœ ë…¼ë¬¸ì˜ ì ìˆ˜ ë¶„í¬
                excluded_scores = sorted([r.score for r in results if r.score < request.min_score], reverse=True)
                if excluded_scores:
                    logger.info(f"   ì œì™¸ëœ ì ìˆ˜: {excluded_scores[:5]}" + ("..." if len(excluded_scores) > 5 else ""))
            
            # 7. ì ìˆ˜ ìˆœ ì •ë ¬ + top_k ë°˜í™˜ (í•„í„°ë§ëœ ê²°ê³¼ì—ì„œ)
            filtered_results.sort(key=lambda x: x.score, reverse=True)
            final_results = filtered_results[:request.top_k]
            
            if final_results:
                scores = [r.score for r in final_results]
                logger.info(
                    f"âœ… ë…¼ë¬¸ ì¶”ì²œ ì™„ë£Œ: {len(final_results)}ê°œ"
                )
                logger.info(f"   â”œâ”€ ì ìˆ˜ ë²”ìœ„: {min(scores):.1f} ~ {max(scores):.1f}")
                logger.info(f"   â””â”€ í‰ê·  ì ìˆ˜: {sum(scores)/len(scores):.1f}")
                # ìµœì¢… ê²°ê³¼ ìš”ì•½
                for i, result in enumerate(final_results, 1):
                    logger.info(f"   {i}. [{result.score:.1f}ì ] {result.paper_info.title[:60]}...")
            else:
                logger.warning(f"âš ï¸  min_score {request.min_score} ì´ìƒì¸ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤")
                if results:
                    max_score = max(r.score for r in results)
                    logger.warning(f"   ìµœê³  ì ìˆ˜: {max_score:.1f} (min_scoreë¥¼ {max_score:.1f} ì´í•˜ë¡œ ë‚®ì¶”ì„¸ìš”)")
            
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ ë…¼ë¬¸ ì¶”ì²œ ì‹¤íŒ¨: {e}")
            return []
    
    async def _generate_search_query(self, request: OpenAlexRequest) -> dict:
        """
        ì„¹ì…˜ ìš”ì•½ â†’ OpenAlex ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (LLM)
        
        Args:
            request: OpenAlexRequest
            
        Returns:
            {"tokens": ["term1", "term2"], "year_from": 2015}
        """
        try:
            # LLMì—ê²Œ ì „ë‹¬í•  ë°ì´í„° ì¤€ë¹„
            request_data = {
                "section_summary": request.section_summary,
                "previous_summaries": request.previous_summaries,
                "rag_context": request.rag_context,
            }
            
            logger.info("ğŸ¤– LLM ì¿¼ë¦¬ ìƒì„± ì‹œì‘")
            # LLM ì¿¼ë¦¬ ìƒì„±
            result = await self.llm_client.generate_query(request_data)
            
            logger.info(f"âœ… LLM ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ: {result}")
            
            # year_from ì¶”ê°€
            result["year_from"] = request.year_from
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return {"tokens": [], "year_from": request.year_from}
    
    async def _verify_papers_parallel(
        self, 
        papers: List[dict], 
        request: OpenAlexRequest,
        query: dict
    ) -> List[OpenAlexResponse]:
        """
        ë³‘ë ¬ LLM ê²€ì¦ (Semaphore ë™ì‹œì„± ì œì–´)
        
        Args:
            papers: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
            request: OpenAlexRequest
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (tokens í¬í•¨)
            
        Returns:
            List[OpenAlexResponse]: ê²€ì¦ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        semaphore = asyncio.Semaphore(OpenAlexConfig.VERIFY_CONCURRENCY)
        
        async def verify_with_limit(paper: dict):
            async with semaphore:
                return await self._verify_single_paper(paper, request, query)
        
        logger.info(f"âœ¨ ë³‘ë ¬ LLM ê²€ì¦ ì‹œì‘ (ë™ì‹œì„±: {OpenAlexConfig.VERIFY_CONCURRENCY})")
        
        results = await asyncio.gather(
            *[verify_with_limit(paper) for paper in papers],
            return_exceptions=True
        )
        
        # ì—ëŸ¬ ì²˜ë¦¬
        verified = []
        for paper, result in zip(papers, results):
            if isinstance(result, Exception):
                logger.error(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {result}")
                # Fallback: score=5.0
                verified.append(OpenAlexResponse(
                    lecture_id=request.lecture_id,
                    section_id=request.section_id,
                    paper_info=self._parse_paper_info(paper),
                    reason="ê²€ì¦ ì‹¤íŒ¨ (fallback)",
                    score=5.0
                ))
            else:
                verified.append(result)
        
        logger.info(f"âœ… ë³‘ë ¬ ê²€ì¦ ì™„ë£Œ: {len(verified)}ê°œ")
        
        return verified
    
    async def _verify_single_paper(
        self, 
        paper: dict, 
        request: OpenAlexRequest,
        query: dict
    ) -> OpenAlexResponse:
        """
        ë‹¨ì¼ ë…¼ë¬¸ ê²€ì¦ (LLM)
        
        Args:
            paper: ë…¼ë¬¸ ì •ë³´
            request: OpenAlexRequest
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (tokens í¬í•¨)
            
        Returns:
            OpenAlexResponse
        """
        try:
            # í‚¤ì›Œë“œ ë¬¸ìì—´ ìƒì„±
            keywords = ", ".join(query.get("tokens", []))
            
            # LLM ê²€ì¦
            result = await self.llm_client.score_paper(
                paper=paper,
                section_summary=request.section_summary,
                keywords=keywords,
                language=request.language
            )
            
            return OpenAlexResponse(
                lecture_id=request.lecture_id,
                section_id=request.section_id,
                paper_info=self._parse_paper_info(paper),
                reason=result.get("reason", "ê²€ì¦ ì™„ë£Œ"),
                score=result.get("score", 5.0)
            )
            
        except Exception as e:
            logger.error(f"âŒ ë…¼ë¬¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return OpenAlexResponse(
                lecture_id=request.lecture_id,
                section_id=request.section_id,
                paper_info=self._parse_paper_info(paper),
                reason="ê²€ì¦ ì‹¤íŒ¨ (ì˜¤ë¥˜)",
                score=5.0
            )
    
    def _heuristic_score(
        self, 
        papers: List[dict], 
        query: dict,
        request: OpenAlexRequest
    ) -> List[OpenAlexResponse]:
        """
        Heuristic ìŠ¤ì½”ì–´ë§ (LLM ì—†ì´ ë¹ ë¥¸ í‰ê°€)
        
        ì ìˆ˜ ê³„ì‚°:
        - ì œëª© í‚¤ì›Œë“œ ë§¤ì¹­: +3ì 
        - ì´ˆë¡ í‚¤ì›Œë“œ ë§¤ì¹­: +1ì 
        - relevance_score ê°€ì¤‘ì¹˜: +2ì 
        - ê¸°ë³¸ ì ìˆ˜: 5.0
        
        Args:
            papers: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (tokens í¬í•¨)
            request: OpenAlexRequest
            
        Returns:
            List[OpenAlexResponse]: ì ìˆ˜ê°€ ë¶€ì—¬ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("ğŸ”¢ Heuristic ìŠ¤ì½”ì–´ë§ ì‹œì‘...")
        
        tokens = [kw.lower() for kw in query.get("tokens", [])]
        results = []
        
        for paper in papers:
            title_lower = paper.get("title", "").lower()
            abstract_lower = paper.get("abstract", "").lower()
            
            # ê¸°ë³¸ ì ìˆ˜
            score = 5.0
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            for kw in tokens:
                if kw in title_lower:
                    score += 0.5
                if kw in abstract_lower:
                    score += 0.2
            
            # relevance_score ê°€ì¤‘ì¹˜
            relevance = paper.get("relevance_score", 0)
            score += min(relevance / 10, 2.0)  # ìµœëŒ€ +2ì 
            
            # 10ì  ì´ˆê³¼ ë°©ì§€
            score = min(score, 10.0)
            
            # ì´ìœ  ìƒì„± (ë‹¨ìˆœí™”)
            reason = "Heuristic"
            
            results.append(OpenAlexResponse(
                lecture_id=request.lecture_id,
                section_id=request.section_id,
                paper_info=self._parse_paper_info(paper),
                reason=reason,
                score=score
            ))
        
        logger.info(f"âœ… Heuristic ìŠ¤ì½”ì–´ë§ ì™„ë£Œ: {len(results)}ê°œ")
        
        return results
    
    def _parse_paper_info(self, paper: dict) -> PaperInfo:
        """
        ë…¼ë¬¸ ì •ë³´ë¥¼ PaperInfoë¡œ ë³€í™˜
        
        Args:
            paper: ë…¼ë¬¸ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            PaperInfo
        """
        return PaperInfo(
            url=paper.get("url", ""),
            title=paper.get("title", ""),
            abstract=paper.get("abstract", "")[:OpenAlexConfig.ABSTRACT_MAX_LENGTH],
            year=paper.get("year"),
            cited_by_count=paper.get("cited_by_count", 0),
            authors=paper.get("authors", [])
        )
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.api_client.close()
"""
OpenAlexKit ìœ í‹¸ë¦¬í‹°
"""
"""
ë…¼ë¬¸ í•„í„°ë§ ë° ì¬ë­í‚¹ ìœ í‹¸ë¦¬í‹°
"""
import re
import logging
from typing import List, Dict

logger = logging.getLogger(__name__)


def deduplicate_papers(papers: List[Dict]) -> List[Dict]:
    """
    ì¤‘ë³µ ë…¼ë¬¸ ì œê±° (DOI or ì •ê·œí™”ëœ ì œëª©)
    
    ìš°ì„ ìˆœìœ„:
    1. DOI ì¡´ì¬ â†’ DOIë¡œ ì¤‘ë³µ ì²´í¬
    2. DOI ì—†ìŒ â†’ ì •ê·œí™”ëœ ì œëª©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
    
    Args:
        papers: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì¤‘ë³µ ì œê±°ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    seen = set()
    unique = []
    
    for paper in papers:
        # DOI ìš°ì„ , ì—†ìœ¼ë©´ ì •ê·œí™”ëœ ì œëª©
        url = paper.get("url", "")
        
        # DOIê°€ ìˆìœ¼ë©´ DOIë¡œ ì¤‘ë³µ ì²´í¬
        if url and url.startswith("http"):
            key = url
        else:
            # DOI ì—†ìœ¼ë©´ ì œëª©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            key = _normalize_title(paper.get("title", ""))
        
        if key and key not in seen:
            seen.add(key)
            unique.append(paper)
    
    logger.info(f"ğŸ” ì¤‘ë³µ ì œê±°: {len(papers)}ê°œ â†’ {len(unique)}ê°œ")
    return unique


def rerank_papers(papers: List[Dict], query: Dict) -> List[Dict]:
    """
    ê°„ë‹¨í•œ ì¬ë­í‚¹ (í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜)
    
    ì ìˆ˜ ê³„ì‚°:
    - match_score = ì œëª© ë§¤ì¹­ * 3 + ì´ˆë¡ ë§¤ì¹­ * 1
    
    ì •ë ¬:
    - (match_score, relevance_score) ë‚´ë¦¼ì°¨ìˆœ
    
    Args:
        papers: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
        query: ê²€ìƒ‰ ì¿¼ë¦¬ (tokens í¬í•¨)
        
    Returns:
        ì¬ë­í‚¹ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    tokens = [kw.lower() for kw in query.get("tokens", [])]
    
    for paper in papers:
        title_lower = paper.get("title", "").lower()
        abstract_lower = paper.get("abstract", "").lower()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        match_score = 0
        for kw in tokens:
            if kw in title_lower:
                match_score += 3
            if kw in abstract_lower:
                match_score += 1
        
        paper["match_score"] = match_score
    
    # ë§¤ì¹­ ì ìˆ˜ + relevance_score ê¸°ì¤€ ì •ë ¬
    papers.sort(
        key=lambda x: (x.get("match_score", 0), x.get("relevance_score", 0)), 
        reverse=True
    )
    
    logger.info(f"ğŸ”„ ì¬ë­í‚¹ ì™„ë£Œ: ìƒìœ„ ë…¼ë¬¸ match_score={papers[0].get('match_score', 0) if papers else 0}")
    
    return papers


def _normalize_title(title: str) -> str:
    """
    ì œëª© ì •ê·œí™” (ì†Œë¬¸ì + íŠ¹ìˆ˜ë¬¸ì ì œê±°)
    
    Args:
        title: ë…¼ë¬¸ ì œëª©
        
    Returns:
        ì •ê·œí™”ëœ ì œëª©
    """
    return re.sub(r'[^\w\s]', '', title.lower()).strip()
"""
ë…¼ë¬¸ íŒŒì‹± ìœ í‹¸ë¦¬í‹°
"""
import logging
from typing import Dict, Optional

from ..config.openalex_config import OpenAlexConfig

logger = logging.getLogger(__name__)


def parse_abstract_inverted_index(inverted_index: Optional[Dict]) -> str:
    """
    Inverted index í˜•íƒœ ì´ˆë¡ì„ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    Args:
        inverted_index: {"word": [pos1, pos2, ...], ...}
        
    Returns:
        ì¼ë°˜ í…ìŠ¤íŠ¸ ì´ˆë¡ (ìµœëŒ€ ABSTRACT_MAX_LENGTH ì)
    """
    if not inverted_index:
        return ""
    
    try:
        # ë‹¨ì–´ì™€ ìœ„ì¹˜ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        word_positions = []
        for word, positions in inverted_index.items():
            for pos in positions:
                word_positions.append((pos, word))
        
        # ìœ„ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
        word_positions.sort()
        
        # í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
        abstract = " ".join([word for _, word in word_positions])
        
        # ì„¤ì •ëœ ìµœëŒ€ ê¸¸ì´ë¡œ ìë¥´ê¸°
        if len(abstract) > OpenAlexConfig.ABSTRACT_MAX_LENGTH:
            abstract = abstract[:OpenAlexConfig.ABSTRACT_MAX_LENGTH]
        
        return abstract
        
    except Exception as e:
        logger.error(f"âŒ ì´ˆë¡ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return ""
"""
OpenAlexKit - í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰ ë° ì¶”ì²œ ëª¨ë“ˆ
LiveNote í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ OpenAlex API ê¸°ë°˜ ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œ
"""
from setuptools import setup, find_packages

setup(
    name="openalexkit",
    version="0.1.0",
    description="OpenAlex API based academic paper recommendation module for LiveNote",
    author="LiveNote Team",
    author_email="",
    packages=find_packages(),
    python_requires=">=3.11",
    install_requires=[
        "httpx>=0.24.0",
        "pydantic>=2.0.0",
        "openai>=1.0.0",
        "python-dotenv>=1.0.0",
    ],
    extras_require={
        "dev": [
            "pytest>=7.0.0",
            "pytest-asyncio>=0.21.0",
        ]
    },
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Developers",
        "Programming Language :: Python :: 3.11",
        "Programming Language :: Python :: 3.12",
    ],
)
#!/bin/bash

# OpenAlexKit ëª¨ë“ˆ í™˜ê²½ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ”§ OpenAlexKit í™˜ê²½ ì„¤ì •"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# 1. ê°€ìƒí™˜ê²½ ìƒì„±
if [ ! -d ".venv" ]; then
    echo "ğŸ“¦ ê°€ìƒí™˜ê²½ ìƒì„± ì¤‘..."
    python3 -m venv .venv
    echo "âœ… ê°€ìƒí™˜ê²½ ìƒì„± ì™„ë£Œ"
else
    echo "âœ… ê°€ìƒí™˜ê²½ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤"
fi

# 2. ê°€ìƒí™˜ê²½ í™œì„±í™”
echo "ğŸ”Œ ê°€ìƒí™˜ê²½ í™œì„±í™” ì¤‘..."
source .venv/bin/activate

# 3. pip ì—…ê·¸ë ˆì´ë“œ
echo "â¬†ï¸  pip ì—…ê·¸ë ˆì´ë“œ ì¤‘..."
pip install --upgrade pip -q

# 4. ì˜ì¡´ì„± ì„¤ì¹˜
echo "ğŸ“š íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
pip install -e . -q

# 5. .env íŒŒì¼ í™•ì¸
if [ ! -f ".env" ]; then
    echo "âš ï¸  .env íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒì„±í•©ë‹ˆë‹¤..."
    echo "OPENAI_API_KEY=your-api-key-here" > .env
    echo "âŒ .env íŒŒì¼ì„ ì—´ì–´ì„œ OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!"
    echo "   íŒŒì¼ ìœ„ì¹˜: $(pwd)/.env"
else
    # API í‚¤ í™•ì¸
    if grep -q "your-api-key-here" .env || ! grep -q "OPENAI_API_KEY=sk-" .env; then
        echo "âš ï¸  .env íŒŒì¼ì˜ OPENAI_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”!"
    else
        echo "âœ… .env íŒŒì¼ ì„¤ì • ì™„ë£Œ"
    fi
fi

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ¨ í™˜ê²½ ì„¤ì • ì™„ë£Œ!"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸ“Œ ë‹¤ìŒ ë‹¨ê³„:"
echo "1. .env íŒŒì¼ì— OpenAI API í‚¤ ì„¤ì • (í•„ìš”ì‹œ)"
echo "2. ê°€ìƒí™˜ê²½ ì‹¤í–‰: source .venv/bin/activate"
echo "3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰: python test_openalex.py"
echo ""
"""
OpenAlexKit í†µí•© í…ŒìŠ¤íŠ¸ (íŒŒë¼ë¯¸í„° ë³€í˜• í…ŒìŠ¤íŠ¸)
- ì…ë ¥ íŒŒë¼ë¯¸í„° ëª…í™•íˆ í‘œì‹œ
- ì¶œë ¥ ê²°ê³¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
- ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì¡°í•© í…ŒìŠ¤íŠ¸
  * top_k ë³€í˜•: 1, 3, 5, 10 (4ê°œ)
  * sort_by ë³€í˜•: relevance, cited_by_count, hybrid (4ê°œ)
  * min_score ë³€í˜•: 1.0, 3.0, 5.0, 7.0 (4ê°œ)
  * hybrid+LLM+context: ì „ì²´ ì»¨í…ìŠ¤íŠ¸ í™œìš© (2ê°œ)
- ì´ 14ê°œ ì‹œë‚˜ë¦¬ì˜¤
"""
import asyncio
import logging
import os
import sys
from typing import List
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from openalexkit import (
    OpenAlexService,
    OpenAlexRequest,
    PreviousSectionSummary,
    RAGChunk
)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ë¡œê¹… ì„¤ì •
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
logging.basicConfig(
    level=logging.WARNING,  # í…ŒìŠ¤íŠ¸ ì¶œë ¥ ê¹”ë”í•˜ê²Œ
    format='%(message)s'
)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜ (14ê°œ - 4ê°€ì§€ íŒŒë¼ë¯¸í„° ë³€í˜•)
# - top_k ë³€í˜•: 4ê°œ ì‹œë‚˜ë¦¬ì˜¤ (top_k=1, 3, 5, 10)
# - sort_by ë³€í˜•: 4ê°œ ì‹œë‚˜ë¦¬ì˜¤ (relevance, cited_by_count, hybrid x2)
# - min_score ë³€í˜•: 4ê°œ ì‹œë‚˜ë¦¬ì˜¤ (1.0, 3.0, 5.0, 7.0)
# - hybrid+LLM+context: 2ê°œ ì‹œë‚˜ë¦¬ì˜¤ (ì „ì²´ ì»¨í…ìŠ¤íŠ¸ í™œìš©)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
TEST_SCENARIOS = [
    # â”â”â” Group 1: top_k ë³€í˜• (4ê°œ) â”â”â”
    {
        "name": "1ï¸âƒ£ [top_k=1] CS - ì•Œê³ ë¦¬ì¦˜ (ë‹¨ì¼ ë…¼ë¬¸ ì¶”ì²œ)",
        "request": OpenAlexRequest(
            lecture_id="topk_test_1",
            section_id=1,
            section_summary="í€µì†ŒíŠ¸ ì•Œê³ ë¦¬ì¦˜ì˜ ì‹œê°„ ë³µì¡ë„ì™€ ë¶„í•  ì •ë³µ ì „ëµ",
            language="ko",
            top_k=1,  # 1ê°œë§Œ
            verify_openalex=True,
            year_from=2000,
            sort_by="hybrid"
        )
    },
    {
        "name": "2ï¸âƒ£ [top_k=3] Physics - ì–‘ìì—­í•™ (3ê°œ ì¶”ì²œ)",
        "request": OpenAlexRequest(
            lecture_id="topk_test_3",
            section_id=1,
            section_summary="SchrÃ¶dinger equation and quantum superposition",
            language="en",
            top_k=3,  # 3ê°œ
            verify_openalex=True,
            year_from=2005,
            sort_by="hybrid"
        )
    },
    {
        "name": "3ï¸âƒ£ [top_k=5] ML - Transformer (5ê°œ ì¶”ì²œ)",
        "request": OpenAlexRequest(
            lecture_id="topk_test_5",
            section_id=1,
            section_summary="Attention mechanism in Transformer architecture",
            language="en",
            top_k=5,  # 5ê°œ (ê¸°ë³¸ê°’)
            verify_openalex=False,  # Heuristic (ë¹ ë¦„)
            year_from=2017,
            sort_by="hybrid"
        )
    },
    {
        "name": "4ï¸âƒ£ [top_k=10] Math - ì„ í˜•ëŒ€ìˆ˜ (10ê°œ ì¶”ì²œ)",
        "request": OpenAlexRequest(
            lecture_id="topk_test_10",
            section_id=1,
            section_summary="ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ì˜ ì‘ìš©: PCAì™€ ê·¸ë˜í”„ ì´ë¡ ",
            language="ko",
            top_k=10,  # 10ê°œ (ìµœëŒ€)
            verify_openalex=False,
            year_from=2010,
            sort_by="hybrid"
        )
    },
    
    # â”â”â” Group 2: sort_by ë³€í˜• (4ê°œ) â”â”â”
    {
        "name": "5ï¸âƒ£ [relevance] Chemistry - ìœ ê¸°í™”í•™ (ì—°ê´€ì„± ìš°ì„ )",
        "request": OpenAlexRequest(
            lecture_id="sort_relevance",
            section_id=1,
            section_summary="ë²¤ì   ê³ ë¦¬ì˜ ê³µëª… êµ¬ì¡°ì™€ ë°©í–¥ì¡±ì„±",
            language="ko",
            top_k=3,
            verify_openalex=False,
            year_from=2005,
            sort_by="relevance"  # ì—°ê´€ì„± ìš°ì„ 
        )
    },
    {
        "name": "6ï¸âƒ£ [cited_by_count] Biology - ì„¸í¬ìƒë¬¼í•™ (ì¸ìš©ìˆ˜ ìš°ì„ )",
        "request": OpenAlexRequest(
            lecture_id="sort_citation",
            section_id=1,
            section_summary="ë¯¸í† ì½˜ë“œë¦¬ì•„ì˜ ATP ìƒì„±ê³¼ ì „ìì „ë‹¬ê³„",
            language="ko",
            top_k=3,
            verify_openalex=True,
            year_from=2010,
            sort_by="cited_by_count"  # ì¸ìš©ìˆ˜ ìš°ì„ 
        )
    },
    {
        "name": "7ï¸âƒ£ [hybrid] Economics - ê²Œì„ì´ë¡  (ê· í˜• ì •ë ¬)",
        "request": OpenAlexRequest(
            lecture_id="sort_hybrid_1",
            section_id=1,
            section_summary="Nash equilibrium and prisoner's dilemma",
            language="en",
            top_k=3,
            verify_openalex=False,
            year_from=1990,
            sort_by="hybrid"  # ê· í˜•
        )
    },
    {
        "name": "8ï¸âƒ£ [hybrid] CS - ë°ì´í„°ë² ì´ìŠ¤ (ê· í˜• ì •ë ¬)",
        "request": OpenAlexRequest(
            lecture_id="sort_hybrid_2",
            section_id=1,
            section_summary="B-Tree index structure and range query optimization",
            language="en",
            top_k=3,
            verify_openalex=True,
            year_from=2015,
            sort_by="hybrid"  # ê· í˜•
        )
    },
    
    # â”â”â” Group 3: min_score ë³€í˜• (4ê°œ) â”â”â”
    {
        "name": "9ï¸âƒ£ [min_score=1.0] Psychology - ì¸ì§€ì‹¬ë¦¬í•™ (ë‚®ì€ ì„ê³„ê°’)",
        "request": OpenAlexRequest(
            lecture_id="minscore_1",
            section_id=1,
            section_summary="ì‘ì—… ê¸°ì–µê³¼ ì£¼ì˜ ì§‘ì¤‘ì˜ ì‹ ê²½ê³¼í•™ì  ë©”ì»¤ë‹ˆì¦˜",
            language="ko",
            top_k=5,
            verify_openalex=False,
            year_from=2010,
            sort_by="hybrid",
            min_score=1.0  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’ (ê±°ì˜ ëª¨ë‘ í†µê³¼)
        )
    },
    {
        "name": "ğŸ”Ÿ [min_score=3.0] History - ê·¼ëŒ€ì‚¬ (ê¸°ë³¸ ì„ê³„ê°’)",
        "request": OpenAlexRequest(
            lecture_id="minscore_3",
            section_id=1,
            section_summary="ì‚°ì—…í˜ëª…ì´ ìœ ëŸ½ ì‚¬íšŒì— ë¯¸ì¹œ ì˜í–¥",
            language="ko",
            top_k=5,
            verify_openalex=False,
            year_from=2000,
            sort_by="hybrid",
            min_score=3.0  # ê¸°ë³¸ê°’
        )
    },
    {
        "name": "1ï¸âƒ£1ï¸âƒ£ [min_score=5.0] Physics - ìƒëŒ€ì„±ì´ë¡  (ì¤‘ê°„ ì„ê³„ê°’)",
        "request": OpenAlexRequest(
            lecture_id="minscore_5",
            section_id=1,
            section_summary="Special relativity: time dilation and length contraction",
            language="en",
            top_k=5,
            verify_openalex=True,
            year_from=2000,
            sort_by="hybrid",
            min_score=5.0  # ì¤‘ê°„ ì„ê³„ê°’ (í’ˆì§ˆ ì¤‘ì‹œ)
        )
    },
    {
        "name": "1ï¸âƒ£2ï¸âƒ£ [min_score=7.0] AI - ë”¥ëŸ¬ë‹ (ë†’ì€ ì„ê³„ê°’)",
        "request": OpenAlexRequest(
            lecture_id="minscore_7",
            section_id=1,
            section_summary="Convolutional neural networks for image recognition",
            language="en",
            top_k=5,
            verify_openalex=True,
            year_from=2012,
            sort_by="hybrid",
            min_score=7.0  # ë†’ì€ ì„ê³„ê°’ (ë§¤ìš° ì—„ê²©, ì¼ë¶€ë§Œ í†µê³¼)
        )
    },
    
    # â”â”â” Group 4: hybrid + LLM + full context (2ê°œ) â”â”â”
    {
        "name": "1ï¸âƒ£3ï¸âƒ£ [hybrid+LLM+context] ML - ê°•í™”í•™ìŠµ (ì „ì²´ ì»¨í…ìŠ¤íŠ¸)",
        "request": OpenAlexRequest(
            lecture_id="hybrid_llm_1",
            section_id=3,
            section_summary="Q-learningê³¼ DQNì˜ ì°¨ì´: Experience Replayì™€ Target Networkì˜ ì—­í• ",
            language="ko",
            top_k=5,
            verify_openalex=True,  # LLM ê²€ì¦
            previous_summaries=[
                PreviousSectionSummary(
                    section_id=1,
                    summary="ê°•í™”í•™ìŠµì˜ ê¸°ë³¸ ê°œë…: ì—ì´ì „íŠ¸, í™˜ê²½, ìƒíƒœ, í–‰ë™, ë³´ìƒ"
                ),
                PreviousSectionSummary(
                    section_id=2,
                    summary="Markov Decision Processì™€ Bellman ë°©ì •ì‹"
                )
            ],
            rag_context=[
                RAGChunk(
                    text="3ì¥. Deep Q-Network (DQN)\n- Experience Replay: í•™ìŠµ ë°ì´í„°ì˜ ìƒê´€ê´€ê³„ ì œê±°\n- Target Network: í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ",
                    score=0.94
                ),
                RAGChunk(
                    text="Q-learningì€ í…Œì´ë¸” ê¸°ë°˜ ë°©ë²•ì´ê³ , DQNì€ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ Q-functionì„ ê·¼ì‚¬í•©ë‹ˆë‹¤.",
                    score=0.88
                )
            ],
            year_from=2013,
            sort_by="hybrid",
            min_score=5.0
        )
    },
    {
        "name": "1ï¸âƒ£4ï¸âƒ£ [hybrid+LLM+context] NLP - Transformer (ì „ì²´ ì»¨í…ìŠ¤íŠ¸)",
        "request": OpenAlexRequest(
            lecture_id="hybrid_llm_2",
            section_id=4,
            section_summary="BERTì™€ GPTì˜ ì°¨ì´ì : Bidirectional vs Autoregressive ì‚¬ì „í•™ìŠµ",
            language="ko",
            top_k=5,
            verify_openalex=True,  # LLM ê²€ì¦
            previous_summaries=[
                PreviousSectionSummary(
                    section_id=1,
                    summary="ìì—°ì–´ì²˜ë¦¬ì˜ ë°œì „: RNN â†’ LSTM â†’ Attention Mechanism"
                ),
                PreviousSectionSummary(
                    section_id=2,
                    summary="Transformer ì•„í‚¤í…ì²˜: Self-Attentionê³¼ Positional Encoding"
                ),
                PreviousSectionSummary(
                    section_id=3,
                    summary="ì „ì´í•™ìŠµ(Transfer Learning)ê³¼ ì‚¬ì „í•™ìŠµ(Pre-training)ì˜ ì¤‘ìš”ì„±"
                )
            ],
            rag_context=[
                RAGChunk(
                    text="BERT (Bidirectional Encoder Representations from Transformers)\n- Masked Language Model (MLM) ì‚¬ì „í•™ìŠµ\n- ì–‘ë°©í–¥ ì»¨í…ìŠ¤íŠ¸ í•™ìŠµ\n- ë¶„ë¥˜, NER ë“± í•˜ë¥˜ íƒœìŠ¤í¬ì— íš¨ê³¼ì ",
                    score=0.96
                ),
                RAGChunk(
                    text="GPT (Generative Pre-trained Transformer)\n- Causal Language Model (CLM) ì‚¬ì „í•™ìŠµ\n- ë‹¨ë°©í–¥(ì™¼ìª½â†’ì˜¤ë¥¸ìª½) ì»¨í…ìŠ¤íŠ¸\n- í…ìŠ¤íŠ¸ ìƒì„±ì— íŠ¹í™”",
                    score=0.91
                ),
                RAGChunk(
                    text="ì‚¬ì „í•™ìŠµ ëª¨ë¸ì€ ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ì—ì„œ ì–¸ì–´ì˜ íŒ¨í„´ì„ í•™ìŠµí•œ í›„, ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¯¸ì„¸ì¡°ì •(Fine-tuning)í•©ë‹ˆë‹¤.",
                    score=0.85
                )
            ],
            year_from=2017,
            sort_by="hybrid",
            min_score=6.0
        )
    }
]


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def print_separator(char="â”", length=80):
    """êµ¬ë¶„ì„  ì¶œë ¥"""
    print(char * length)


def print_header(text: str):
    """í—¤ë” ì¶œë ¥"""
    print_separator()
    print(f"ğŸ“Œ {text}")
    print_separator()


def print_input_params(request: OpenAlexRequest):
    """ì…ë ¥ íŒŒë¼ë¯¸í„° ì¶œë ¥"""
    print("\nğŸ”¹ ì…ë ¥ íŒŒë¼ë¯¸í„°:")
    print(f"   Lecture ID: {request.lecture_id}")
    print(f"   Section ID: {request.section_id}")
    print(f"   Section Summary: {request.section_summary}")
    
    if request.previous_summaries:
        print(f"   Previous Summaries: {len(request.previous_summaries)}ê°œ")
        for ps in request.previous_summaries:
            print(f"      - ì„¹ì…˜ {ps.section_id}: {ps.summary}")
    else:
        print(f"   Previous Summaries: (ì—†ìŒ)")
    
    if request.rag_context:
        print(f"   RAG Context: {len(request.rag_context)}ê°œ")
        for rc in request.rag_context:
            print(f"      - [{rc.score:.2f}] {rc.text}")
    else:
        print(f"   RAG Context: (ì—†ìŒ)")
    
    print(f"   Language: {request.language}")
    print(f"   Top K: {request.top_k}")
    print(f"   Verify OpenAlex: {'LLM' if request.verify_openalex else 'Heuristic'}")
    print(f"   Year From: {request.year_from}")
    print(f"   Sort By: {request.sort_by}")
    print(f"   Min Score: {request.min_score}")
    
    if request.exclude_ids:
        print(f"   Exclude IDs: {request.exclude_ids}")


def print_paper_result(idx: int, paper):
    """ë…¼ë¬¸ ê²°ê³¼ ì¶œë ¥ (ë³´ê¸° ì¢‹ê²Œ)"""
    print(f"\n   [{idx}] ì ìˆ˜: {paper.score:.1f}/10")
    print(f"       ì œëª©: {paper.paper_info.title}")
    
    # ì €ì (ìµœëŒ€ 3ëª…)
    authors = paper.paper_info.authors[:3] if paper.paper_info.authors else []
    authors_str = ", ".join(authors)
    if len(paper.paper_info.authors) > 3:
        authors_str += f" ì™¸ {len(paper.paper_info.authors) - 3}ëª…"
    print(f"       ì €ì: {authors_str if authors_str else 'N/A'}")
    
    print(f"       ì¶œíŒì—°ë„: {paper.paper_info.year}")
    print(f"       ì¸ìš©íšŸìˆ˜: {paper.paper_info.cited_by_count}")
    print(f"       ì´ˆë¡: {paper.paper_info.abstract}")
    print(f"       URL: {paper.paper_info.url}")
    print(f"       í‰ê°€: {paper.reason}")


async def run_single_test(scenario: dict, service: OpenAlexService) -> dict:
    """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    name = scenario["name"]
    request = scenario["request"]
    
    print_header(name)
    print_input_params(request)
    
    print("\nğŸ”¹ ì‹¤í–‰ ì¤‘...")
    import time
    start = time.time()
    
    try:
        results = await service.recommend_papers(request)
        elapsed = time.time() - start
        
        print(f"\nâœ… ì™„ë£Œ! (ì‹¤í–‰ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        print(f"\nğŸ”¹ ì¶œë ¥ ê²°ê³¼: {len(results)}ê°œ ë…¼ë¬¸")
        
        if results:
            for idx, paper in enumerate(results, 1):
                print_paper_result(idx, paper)
        else:
            print("   (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")
        
        return {
            "name": name,
            "status": "ì„±ê³µ",
            "count": len(results),
            "elapsed": elapsed
        }
    
    except Exception as e:
        elapsed = time.time() - start
        print(f"\nâŒ ì‹¤íŒ¨: {e}")
        return {
            "name": name,
            "status": "ì‹¤íŒ¨",
            "error": str(e),
            "elapsed": elapsed
        }


async def run_all_tests():
    """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    # API í‚¤ í™•ì¸
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY", "")
    
    print_header("OpenAlexKit í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"âœ… API í‚¤ ë¡œë“œ ì™„ë£Œ: {api_key[:20]}...")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: {len(TEST_SCENARIOS)}ê°œ\n")
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    service = OpenAlexService()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = []
    for idx, scenario in enumerate(TEST_SCENARIOS, 1):
        print(f"\n{'='*80}")
        print(f"í…ŒìŠ¤íŠ¸ {idx}/{len(TEST_SCENARIOS)}")
        result = await run_single_test(scenario, service)
        results.append(result)
        await asyncio.sleep(0.5)  # API Rate Limit ë°©ì§€
    
    # ì„œë¹„ìŠ¤ ì¢…ë£Œ
    await service.close()
    
    # ìš”ì•½ ì¶œë ¥
    print_header("í…ŒìŠ¤íŠ¸ ìš”ì•½")
    total_time = sum(r["elapsed"] for r in results)
    success_count = sum(1 for r in results if r["status"] == "ì„±ê³µ")
    
    print(f"âœ… ì„±ê³µ: {success_count}/{len(results)}")
    print(f"â±ï¸  ì´ ì‹¤í–‰ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"\nìƒì„¸ ê²°ê³¼:")
    
    for r in results:
        status_icon = "âœ…" if r["status"] == "ì„±ê³µ" else "âŒ"
        if r["status"] == "ì„±ê³µ":
            print(f"  {status_icon} {r['name']}: {r['count']}ê°œ ë…¼ë¬¸, {r['elapsed']:.2f}ì´ˆ")
        else:
            print(f"  {status_icon} {r['name']}: {r['error']}")
    
    print_separator()
    print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    asyncio.run(run_all_tests())
"""
YouTubeKit - ê°•ì˜ ì„¹ì…˜ ë§ì¶¤ ìœ íŠœë¸Œ ì¶”ì²œ ëª¨ë“ˆ
LiveNote í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ YouTube Data API ê¸°ë°˜ ë™ì˜ìƒ ì¶”ì²œ ì‹œìŠ¤í…œ
"""
from setuptools import setup, find_packages

setup(
    name="youtubekit",
    version="0.1.0",
    description="YouTube recommender for lecture sections (search, summarize, verify)",
    author="LiveNote Team",
    author_email="",
    packages=find_packages(),
    python_requires=">=3.10",
    install_requires=[
        "pydantic>=2.0.0",
        "python-dotenv>=1.0.0",
        "httpx>=0.24.0",
        "youtube-transcript-api>=0.6.1",
        "openai>=1.0.0",
        "rapidfuzz>=3.0.0",
    ],
    extras_require={
        "dev": [
            "pytest>=7.0.0",
            "pytest-asyncio>=0.21.0",
        ]
    },
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Developers",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Programming Language :: Python :: 3.12",
    ],
)

#!/usr/bin/env bash
set -e

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo " YouTubeKit Setup"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# 1) Create venv
if [ ! -d ".venv" ]; then
  echo "[1/5] Creating virtualenv (.venv)"
  python3 -m venv .venv
fi

# 2) Activate
echo "[2/5] Activating .venv"
source .venv/bin/activate
python --version

# 3) Upgrade pip
echo "[3/5] Upgrading pip"
python -m pip install --upgrade pip >/dev/null

# 4) Install package
echo "[4/5] Installing youtubekit (editable)"
pip install -e . || {
  echo "âš ï¸  Editable install failed. Trying standard install...";
  pip install .;
}

# 5) Ensure .env
if [ ! -f ".env" ]; then
  echo "[5/5] Creating .env"
  cat > .env <<'EOF'
OPENAI_API_KEY=
YOUTUBE_API_KEY=
# Optional flags
YT_OFFLINE_MODE=1
EOF
  echo "âš ï¸  Fill OPENAI_API_KEY and YOUTUBE_API_KEY in .env."
else
  echo "[5/5] .env already exists"
fi

echo "âœ… Setup complete. Run tests: python test_youtube.py"
"""YouTubeKit ì»´í¬ë„ŒíŠ¸ë³„ í…ŒìŠ¤íŠ¸ - ì„±ëŠ¥ ì§„ë‹¨ ë° API í‚¤ ê²€ì¦"""
import asyncio
import time
from dotenv import load_dotenv
import os

load_dotenv()

print("="*80)
print("ğŸ” YouTubeKit ì»´í¬ë„ŒíŠ¸ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
print("="*80)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# TEST 1: í™˜ê²½ ë³€ìˆ˜ í™•ì¸
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
print("\n[TEST 1] í™˜ê²½ ë³€ìˆ˜ í™•ì¸")
print("-" * 80)
openai_key = os.getenv("OPENAI_API_KEY", "")
youtube_key = os.getenv("YOUTUBE_API_KEY", "") or os.getenv("KEY", "")
offline_mode = os.getenv("YT_OFFLINE_MODE", "0")

print(f"âœ“ OPENAI_API_KEY: {'ì„¤ì •ë¨' if openai_key else 'âŒ ì—†ìŒ'} ({len(openai_key)} chars)")
print(f"âœ“ YouTube API KEY: {'ì„¤ì •ë¨' if youtube_key else 'âŒ ì—†ìŒ'} ({len(youtube_key)} chars)")
print(f"âœ“ YT_OFFLINE_MODE: {offline_mode}")

if not youtube_key:
    print("\nâŒ YouTube API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    exit(1)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# TEST 2: YouTube API ì§ì ‘ í˜¸ì¶œ í…ŒìŠ¤íŠ¸ (ê°€ì¥ ê¸°ë³¸)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
print("\n[TEST 2] YouTube API ì§ì ‘ í˜¸ì¶œ (ìˆœìˆ˜ HTTP)")
print("-" * 80)

async def test_youtube_api_direct():
    """YouTube APIë¥¼ ì§ì ‘ í˜¸ì¶œí•´ì„œ í‚¤ê°€ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
    import aiohttp
    
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        "key": youtube_key,
        "part": "snippet",
        "q": "python tutorial",
        "type": "video",
        "maxResults": 1
    }
    
    start = time.time()
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                elapsed = time.time() - start
                
                if resp.status == 200:
                    data = await resp.json()
                    print(f"âœ… YouTube API ì‘ë™ í™•ì¸! ({elapsed:.2f}ì´ˆ)")
                    if 'items' in data and len(data['items']) > 0:
                        video = data['items'][0]['snippet']
                        print(f"   ì œëª©: {video['title']}")
                        print(f"   ì±„ë„: {video['channelTitle']}")
                    return True
                else:
                    error_data = await resp.text()
                    print(f"âŒ API ì—ëŸ¬ (Status {resp.status}): {error_data}")
                    return False
    except asyncio.TimeoutError:
        elapsed = time.time() - start
        print(f"âŒ íƒ€ì„ì•„ì›ƒ! ({elapsed:.2f}ì´ˆ ê²½ê³¼)")
        return False
    except Exception as e:
        elapsed = time.time() - start
        print(f"âŒ ì—ëŸ¬ ({elapsed:.2f}ì´ˆ): {e}")
        return False

result = asyncio.run(test_youtube_api_direct())
if not result:
    print("\nâš ï¸  YouTube API í‚¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    # exit(1)  # ì£¼ì„ ì²˜ë¦¬ - ê³„ì† í…ŒìŠ¤íŠ¸ ì§„í–‰

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# TEST 3: YouTubeClient ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
print("\n[TEST 3] YouTubeClient ì´ˆê¸°í™”")
print("-" * 80)

try:
    from youtubekit.api.youtube_client import YouTubeAPIClient
    from youtubekit.config.youtube_config import YouTubeConfig
    
    start = time.time()
    config = YouTubeConfig()
    client = YouTubeAPIClient(api_key=config.YOUTUBE_API_KEY)
    elapsed = time.time() - start
    print(f"âœ… YouTubeAPIClient ì´ˆê¸°í™” ì„±ê³µ ({elapsed:.4f}ì´ˆ)")
except Exception as e:
    print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    import traceback
    traceback.print_exc()

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# TEST 4: YouTubeClient.search_videos() ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
print("\n[TEST 4] YouTubeClient.search_videos() ì„±ëŠ¥")
print("-" * 80)

async def test_search_videos():
    from youtubekit.api.youtube_client import YouTubeAPIClient
    from youtubekit.config.youtube_config import YouTubeConfig
    
    config = YouTubeConfig()
    client = YouTubeAPIClient(api_key=config.YOUTUBE_API_KEY)
    
    start = time.time()
    try:
        videos = await client.search_videos("python tutorial", lang="en", max_results=3)
        elapsed = time.time() - start
        
        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(videos)}ê°œ ì˜ìƒ ({elapsed:.2f}ì´ˆ)")
        for idx, v in enumerate(videos, 1):
            print(f"   [{idx}] {v.title[:60]}...")
        return videos
    except Exception as e:
        elapsed = time.time() - start
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ): {e}")
        import traceback
        traceback.print_exc()
        return []

videos = asyncio.run(test_search_videos())

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# TEST 5: OpenAI LLM ìš”ì•½ í…ŒìŠ¤íŠ¸ (ë³‘ëª© ì§€ì  ì˜ì‹¬)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
print("\n[TEST 5] OpenAI LLM ìš”ì•½ ìƒì„± (ë³‘ëª© ì˜ì‹¬ ì§€ì )")
print("-" * 80)

async def test_llm_summarize():
    from youtubekit.llm.openai_client import YouTubeLLMClient
    from youtubekit.config.youtube_config import YouTubeConfig
    
    if not videos:
        print("âš ï¸  ì´ì „ í…ŒìŠ¤íŠ¸ì—ì„œ ì˜ìƒì„ ê°€ì ¸ì˜¤ì§€ ëª»í•´ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        return
    
    config = YouTubeConfig()
    llm = YouTubeLLMClient(api_key=config.OPENAI_API_KEY)
    
    video = videos[0]
    
    print(f"ì˜ìƒ: {video.title}")
    print(f"ì„¤ëª… ê¸¸ì´: {len(video.description)} chars")
    
    start = time.time()
    try:
        result = await llm.summarize_content(
            title=video.title,
            content=video.description,
            language="en"
        )
        elapsed = time.time() - start
        
        summary = result.get("extract", "")
        print(f"âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")
        print(f"   ìš”ì•½: {summary[:200]}...")
        return summary
    except Exception as e:
        elapsed = time.time() - start
        print(f"âŒ ìš”ì•½ ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ): {e}")
        import traceback
        traceback.print_exc()
        return None

summary = asyncio.run(test_llm_summarize())

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# TEST 6: LLM ê²€ì¦ í…ŒìŠ¤íŠ¸ (ê°€ì¥ ì˜¤ë˜ ê±¸ë¦¼ ì˜ˆìƒ)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
print("\n[TEST 6] OpenAI LLM ê²€ì¦ (verify_yt=True ì‹œë‚˜ë¦¬ì˜¤)")
print("-" * 80)

async def test_llm_verify():
    from youtubekit.llm.openai_client import YouTubeLLMClient
    from youtubekit.config.youtube_config import YouTubeConfig
    
    if not videos or not summary:
        print("âš ï¸  ì´ì „ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¡œ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        return
    
    config = YouTubeConfig()
    llm = YouTubeLLMClient(api_key=config.OPENAI_API_KEY)
    
    video = videos[0]
    lecture_summary = "Python programming tutorial for beginners"
    
    print(f"ê°•ì˜ ìš”ì•½: {lecture_summary}")
    print(f"ì˜ìƒ: {video.title}")
    
    start = time.time()
    try:
        result = await llm.score_video(
            lecture_summary=lecture_summary,
            title=video.title,
            extract=summary,
            language="en"
        )
        elapsed = time.time() - start
        
        score = result.get("score", 0)
        reason = result.get("reason", "")
        print(f"âœ… LLM ê²€ì¦ ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")
        print(f"   ì ìˆ˜: {score}/10.0")
        print(f"   ì´ìœ : {reason[:100]}...")
        return result
    except Exception as e:
        elapsed = time.time() - start
        print(f"âŒ ê²€ì¦ ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ): {e}")
        import traceback
        traceback.print_exc()
        return None

verify_result = asyncio.run(test_llm_verify())

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ìµœì¢… ìš”ì•½
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
print("\n" + "="*80)
print("ğŸ“Š ì„±ëŠ¥ ë¶„ì„ ìš”ì•½")
print("="*80)
print("""
ì˜ˆìƒ ë³‘ëª© ì§€ì :
1. YouTube API ê²€ìƒ‰: 1-3ì´ˆ (ì •ìƒ)
2. OpenAI LLM ìš”ì•½ (ê° ì˜ìƒë§ˆë‹¤): 2-5ì´ˆ âš ï¸
3. OpenAI LLM ê²€ì¦ (ê° ì˜ìƒë§ˆë‹¤): 3-6ì´ˆ âš ï¸âš ï¸

ë§Œì•½ top_k=3, verify_yt=Trueë¼ë©´:
- ê²€ìƒ‰: 2ì´ˆ
- ìš”ì•½ 3ê°œ: 3ì´ˆ x 3 = 9ì´ˆ
- ê²€ì¦ 3ê°œ: 5ì´ˆ x 3 = 15ì´ˆ
- ì´í•©: ì•½ 26ì´ˆ ì´ìƒ!

í•´ê²° ë°©ì•ˆ:
1. ë³‘ë ¬ ì²˜ë¦¬ (asyncio.gather) - ê°€ì¥ íš¨ê³¼ì 
2. verify_yt=False ì‚¬ìš© (Heuristicë§Œ) - 15ì´ˆ ì ˆì•½
3. top_k ì¤„ì´ê¸°
4. LLM ëª¨ë¸ ë³€ê²½ (gpt-4o-mini -> gpt-3.5-turbo)
""")
"""YouTubeKit ì¢…í•© í…ŒìŠ¤íŠ¸ - ì‹¤ì œ YouTube API ì‚¬ìš© (ìˆœì°¨ ì‹¤í–‰)"""
import asyncio
import logging
import time
from youtubekit import YouTubeService, YouTubeRequest

logging.basicConfig(level=logging.WARNING, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

TEST_SCENARIOS = [
    {
        "name": "1ï¸âƒ£ [top_k=1] CS - Stack Algorithm (ë‹¨ì¼ ì˜ìƒ ì¶”ì²œ)",
        "request": YouTubeRequest(
            lecture_id="yt_test_1",
            section_id=1,
            lecture_summary="Stack data structure: LIFO operations, push and pop methods, and practical applications",
            language="en",
            top_k=1,
            verify_yt=True,
            yt_lang="en",
            min_score=5.0
        )
    },
    {
        "name": "2ï¸âƒ£ [top_k=3] ML - Transformer (3ê°œ ì˜ìƒ ì¶”ì²œ)",
        "request": YouTubeRequest(
            lecture_id="yt_test_2",
            section_id=1,
            lecture_summary="Transformer architecture with self-attention mechanism for NLP tasks",
            language="en",
            top_k=3,
            verify_yt=True,
            yt_lang="en",
            min_score=5.0
        )
    },
    {
        "name": "3ï¸âƒ£ [top_k=5] í•œêµ­ì‚¬ - ì¡°ì„ ì‹œëŒ€ (5ê°œ ì˜ìƒ ì¶”ì²œ)",
        "request": YouTubeRequest(
            lecture_id="yt_test_3",
            section_id=1,
            lecture_summary="ì¡°ì„ ì‹œëŒ€ì˜ ì •ì¹˜ ì²´ì œì™€ ì„±ë¦¬í•™ì˜ ë°œì „, ê³¼ê±°ì œë„ì™€ ì–‘ë°˜ ì‚¬íšŒ êµ¬ì¡°",
            language="ko",
            top_k=5,
            verify_yt=False,
            yt_lang="ko",
            min_score=3.0
        )
    },
    {
        "name": "4ï¸âƒ£ [LLM ê²€ì¦] Python - Decorators",
        "request": YouTubeRequest(
            lecture_id="yt_test_4",
            section_id=1,
            lecture_summary="Python decorators: function wrappers, @property, @staticmethod, and practical examples",
            language="en",
            top_k=3,
            verify_yt=True,
            yt_lang="en",
            min_score=5.0
        )
    },
    {
        "name": "5ï¸âƒ£ [Heuristic] JavaScript - Async/Await",
        "request": YouTubeRequest(
            lecture_id="yt_test_5",
            section_id=1,
            lecture_summary="JavaScript async/await syntax for handling asynchronous operations and promises",
            language="en",
            top_k=3,
            verify_yt=False,
            yt_lang="en",
            min_score=5.0
        )
    },
    {
        "name": "6ï¸âƒ£ [English] Quantum Mechanics",
        "request": YouTubeRequest(
            lecture_id="yt_test_6",
            section_id=1,
            lecture_summary="Quantum superposition and wave-particle duality in quantum mechanics",
            language="en",
            top_k=3,
            verify_yt=True,
            yt_lang="en",
            min_score=5.0
        )
    },
    {
        "name": "7ï¸âƒ£ [Korean] ë°ì´í„°ë² ì´ìŠ¤ - SQL",
        "request": YouTubeRequest(
            lecture_id="yt_test_7",
            section_id=1,
            lecture_summary="SQL ê¸°ë³¸ ì¿¼ë¦¬: SELECT, JOIN, WHERE ì ˆê³¼ ì¸ë±ìŠ¤ ìµœì í™”",
            language="ko",
            top_k=3,
            verify_yt=True,
            yt_lang="ko",
            min_score=5.0
        )
    },
    {
        "name": "8ï¸âƒ£ [min_score=3.0] React - Hooks",
        "request": YouTubeRequest(
            lecture_id="yt_test_8",
            section_id=1,
            lecture_summary="React Hooks: useState, useEffect, and custom hooks for state management",
            language="en",
            top_k=5,
            verify_yt=False,
            yt_lang="en",
            min_score=3.0
        )
    },
    {
        "name": "9ï¸âƒ£ [min_score=7.0] Docker - Kubernetes",
        "request": YouTubeRequest(
            lecture_id="yt_test_9",
            section_id=1,
            lecture_summary="Docker containerization and Kubernetes orchestration for microservices deployment",
            language="en",
            top_k=5,
            verify_yt=True,
            yt_lang="en",
            min_score=7.0
        )
    },
    {
        "name": "ğŸ”Ÿ [exclude_titles] Git - Version Control",
        "request": YouTubeRequest(
            lecture_id="yt_test_10",
            section_id=1,
            lecture_summary="Git version control: branching, merging, and collaborative workflows",
            language="en",
            top_k=5,
            verify_yt=False,
            yt_lang="en",
            min_score=5.0,
            exclude_titles=["Git Tutorial for Beginners"]
        )
    },
    {
        "name": "1ï¸âƒ£1ï¸âƒ£ [Full Fields] FastAPI - REST API Development",
        "request": YouTubeRequest(
            lecture_id="yt_test_11",
            section_id=2,
            lecture_summary="FastAPI framework for building REST APIs with automatic documentation and type validation",
            language="en",
            top_k=3,
            verify_yt=True,
            yt_lang="en",
            min_score=6.0,
            previous_summaries=[],  # ì´ì „ ì„¹ì…˜ ìš”ì•½ ì—†ìŒ
            rag_context=[],  # RAG ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ
            exclude_titles=["FastAPI Crash Course"]
        )
    },
    {
        "name": "1ï¸âƒ£2ï¸âƒ£ [Full Fields + Context] TensorFlow - Neural Networks",
        "request": YouTubeRequest(
            lecture_id="yt_test_12",
            section_id=3,
            lecture_summary="TensorFlow deep learning framework for building and training neural networks with GPU acceleration",
            language="en",
            top_k=4,
            verify_yt=True,
            yt_lang="en",
            min_score=6.5,
            previous_summaries=[
                {"section_id": 1, "summary": "Introduction to machine learning fundamentals and supervised learning"},
                {"section_id": 2, "summary": "Neural network basics: perceptron, activation functions, backpropagation"}
            ],
            rag_context=[
                {"text": "TensorFlow is an open-source platform for machine learning", "score": 0.92, "source": "lecture_notes.pdf"},
                {"text": "GPU acceleration speeds up neural network training significantly", "score": 0.88, "source": "textbook_ch5.pdf"}
            ],
            exclude_titles=["TensorFlow Tutorial", "Deep Learning Basics"]
        )
    },
    {
        "name": "1ï¸âƒ£3ï¸âƒ£ [Full Fields Korean] ë¸”ë¡ì²´ì¸ - ìŠ¤ë§ˆíŠ¸ ì»¨íŠ¸ë™íŠ¸",
        "request": YouTubeRequest(
            lecture_id="yt_test_13",
            section_id=4,
            lecture_summary="ìŠ¤ë§ˆíŠ¸ ì»¨íŠ¸ë™íŠ¸ ê°œë°œ: Solidity ì–¸ì–´, ì´ë”ë¦¬ì›€ í”Œë«í¼, íƒˆì¤‘ì•™í™” ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•",
            language="ko",
            top_k=5,
            verify_yt=True,
            yt_lang="ko",
            min_score=5.5,
            previous_summaries=[
                {"section_id": 1, "summary": "ë¸”ë¡ì²´ì¸ ê¸°ì´ˆ: ë¶„ì‚° ì›ì¥, í•©ì˜ ì•Œê³ ë¦¬ì¦˜, ì•”í˜¸í™” í•´ì‹œ"},
                {"section_id": 2, "summary": "ì´ë”ë¦¬ì›€ í”Œë«í¼ êµ¬ì¡°ì™€ ê°€ìŠ¤ ê°œë…"},
                {"section_id": 3, "summary": "Solidity ì–¸ì–´ ê¸°ë³¸ ë¬¸ë²•ê³¼ ë°ì´í„° íƒ€ì…"}
            ],
            rag_context=[
                {"text": "ìŠ¤ë§ˆíŠ¸ ì»¨íŠ¸ë™íŠ¸ëŠ” ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ë””ì§€í„¸ ê³„ì•½", "score": 0.95, "source": "blockchain_course.pdf"},
                {"text": "SolidityëŠ” ì´ë”ë¦¬ì›€ ìŠ¤ë§ˆíŠ¸ ì»¨íŠ¸ë™íŠ¸ ì‘ì„± ì–¸ì–´", "score": 0.90, "source": "ethereum_docs.pdf"}
            ],
            exclude_titles=["ë¸”ë¡ì²´ì¸ ì…ë¬¸", "ë¹„íŠ¸ì½”ì¸ ê¸°ì´ˆ"]
        )
    },
]

def print_separator(char="â”", length=80):
    print(char * length)

def print_request_info(name: str, req: YouTubeRequest):
    print(f"\n{'='*80}")
    print(f"ğŸ“º {name}")
    print(f"{'='*80}")
    print(f"ğŸ“‹ ìš”ì²­ ì •ë³´:")
    print(f"   â€¢ lecture_id: {req.lecture_id}")
    print(f"   â€¢ section_id: {req.section_id}")
    print(f"   â€¢ lecture_summary: {req.lecture_summary}")
    print(f"   â€¢ language: {req.language}")
    print(f"   â€¢ top_k: {req.top_k}")
    print(f"   â€¢ verify_yt: {req.verify_yt} {'(LLM ê²€ì¦)' if req.verify_yt else '(Heuristic)'}")
    print(f"   â€¢ yt_lang: {req.yt_lang}")
    print(f"   â€¢ min_score: {req.min_score}")
    if req.exclude_titles:
        print(f"   â€¢ exclude_titles: {req.exclude_titles}")
    print_separator()

def print_video_results(results, processing_time: float):
    if not results:
        print("âŒ ê²°ê³¼ ì—†ìŒ (min_score ì„ê³„ê°’ ë¯¸ë‹¬ ë˜ëŠ” ê²€ìƒ‰ ì‹¤íŒ¨)\n")
        return
    
    print(f"\nâœ… ì´ {len(results)}ê°œ ì˜ìƒ ì¶”ì²œ (ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ)\n")
    
    for idx, resp in enumerate(results, 1):
        vi = resp.video_info
        print(f"  [{idx}] ğŸ¬ {vi.title}")
        print(f"      ğŸ“Š ì ìˆ˜: {resp.score:.1f}/10.0")
        print(f"      ğŸ”— URL: {vi.url}")
        print(f"      ğŸŒ ì–¸ì–´: {vi.lang}")
        print(f"      ğŸ’¡ ì¶”ì²œ ì´ìœ : {resp.reason}")
        print(f"      ğŸ“ ìš”ì•½:")
        
        extract_lines = vi.extract.split('. ')
        for line in extract_lines:
            if line.strip():
                print(f"         â€¢ {line.strip()}")
        print()

async def run_test_scenario(scenario_idx: int, scenario: dict):
    """ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰ (ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥)"""
    name = scenario["name"]
    req = scenario["request"]
    
    try:
        print_request_info(name, req)
        
        service = YouTubeService()
        start_time = time.time()
        results = await service.recommend_videos(req)
        processing_time = time.time() - start_time
        
        print_video_results(results, processing_time)
        
        return {
            "scenario_idx": scenario_idx,
            "name": name,
            "success": True,
            "processing_time": processing_time,
            "result_count": len(results)
        }
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\n")
        logger.error(f"Error in scenario {scenario_idx}: {e}", exc_info=True)
        return {
            "scenario_idx": scenario_idx,
            "name": name,
            "success": False,
            "error": str(e)
        }

async def run_all_tests_sequential():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ (ê¸°ì¡´ ë°©ì‹)"""
    print("\n" + "="*80)
    print("ğŸ¥ YouTubeKit ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘ (ìˆœì°¨ ì‹¤í–‰ ëª¨ë“œ)")
    print("="*80)
    print(f"ì´ {len(TEST_SCENARIOS)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
    print("="*80 + "\n")
    
    success_count = 0
    total_start = time.time()
    
    for idx, scenario in enumerate(TEST_SCENARIOS, 1):
        result = await run_test_scenario(idx, scenario)
        if result.get("success", False):
            success_count += 1
        
        if idx < len(TEST_SCENARIOS):
            await asyncio.sleep(1)
    
    total_time = time.time() - total_start
    
    print("\n" + "="*80)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ (ìˆœì°¨ ì‹¤í–‰)")
    print("="*80)
    print(f"âœ… ì„±ê³µ: {success_count}/{len(TEST_SCENARIOS)} ì‹œë‚˜ë¦¬ì˜¤")
    print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"âš¡ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(TEST_SCENARIOS):.2f}ì´ˆ/ì‹œë‚˜ë¦¬ì˜¤")
    print("="*80 + "\n")
    
    if success_count == len(TEST_SCENARIOS):
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
    else:
        print(f"âš ï¸  {len(TEST_SCENARIOS) - success_count}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤íŒ¨")

if __name__ == "__main__":
    asyncio.run(run_all_tests_sequential())
"""
YouTubeKit ë‹¨ì¼ í…ŒìŠ¤íŠ¸ (API ì¿¼í„° ì ˆì•½ìš©)
ëª¨ë“  í•„ë“œë¥¼ í¬í•¨í•œ ì™„ì „í•œ í…ŒìŠ¤íŠ¸ - LLM vs Heuristic ë¹„êµ
"""
import asyncio
import time
from youtubekit import YouTubeService, YouTubeRequest, PreviousSummary, RAGChunk


async def run_test(verify_yt: bool, test_name: str):
    """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    
    print("=" * 80)
    print(f"ğŸ§ª {test_name}")
    print("=" * 80)
    print()
    
    # ì´ì „ ì„¹ì…˜ ìš”ì•½ (previous_summaries)
    previous_summaries = [
        PreviousSummary(
            section_id=0,
            summary="Introduction to Python programming: variables, data types, and basic syntax"
        ),
        PreviousSummary(
            section_id=1,
            summary="Control flow in Python: if statements, for loops, and while loops"
        )
    ]
    
    # RAG ì»¨í…ìŠ¤íŠ¸ (ê°•ì˜ ìë£Œì—ì„œ ì¶”ì¶œ)
    rag_context = [
        RAGChunk(
            text="List comprehension is a concise way to create lists in Python. "
                 "It consists of brackets containing an expression followed by a for clause.",
            metadata={"source": "lecture_notes.pdf", "page": 15}
        ),
        RAGChunk(
            text="Example: squares = [x**2 for x in range(10)] creates a list of squares from 0 to 81.",
            metadata={"source": "lecture_notes.pdf", "page": 15}
        ),
        RAGChunk(
            content="List comprehensions can also include conditional logic using if statements. "
                    "This makes them very powerful for data filtering and transformation.",
            metadata={"source": "code_examples.py", "line": 42}
        )
    ]
    
    # ì œì™¸í•  ì˜ìƒ ì œëª©ë“¤
    exclude_titles = [
        "Python Tutorial for Beginners",  # ë„ˆë¬´ ê¸°ì´ˆ
        "Complete Python Course",  # ë„ˆë¬´ í¬ê´„ì 
    ]
    
    # ì™„ì „í•œ ìš”ì²­ ê°ì²´ ìƒì„±
    req = YouTubeRequest(
        lecture_id="python_adv_001",
        section_id=2,
        lecture_summary="Advanced Python list comprehensions: nested loops, conditional expressions, "
                       "and performance optimization techniques for data processing",
        language="en",
        top_k=3,
        verify_yt=verify_yt,  # LLM vs Heuristic
        yt_lang="en",
        min_score=6.0,
        previous_summaries=previous_summaries,
        rag_context=rag_context,
        exclude_titles=exclude_titles
    )
    
    print("ğŸ“‹ ìš”ì²­ ì •ë³´:")
    print(f"   â€¢ verify_yt: {req.verify_yt} ({'LLM ê²€ì¦' if verify_yt else 'Heuristic ì ìˆ˜'})")
    print(f"   â€¢ lecture_summary: {req.lecture_summary[:70]}...")
    print(f"   â€¢ top_k: {req.top_k}")
    print(f"   â€¢ previous_summaries: {len(previous_summaries)}ê°œ")
    print(f"   â€¢ rag_context: {len(rag_context)}ê°œ ì²­í¬")
    print(f"   â€¢ exclude_titles: {len(exclude_titles)}ê°œ")
    print()
    print("â”€" * 80)
    
    # ì„œë¹„ìŠ¤ ì‹¤í–‰
    service = YouTubeService()
    
    start_time = time.time()
    
    try:
        results = await service.recommend_videos(req)
        
        elapsed = time.time() - start_time
        
        print()
        print("=" * 80)
        print(f"âœ… ì´ {len(results)}ê°œ ì˜ìƒ ì¶”ì²œ (ì²˜ë¦¬ ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        print("=" * 80)
        print()
        
        for idx, r in enumerate(results, 1):
            print(f"  [{idx}] ğŸ¬ {r.video_info.title}")
            print(f"      ğŸ“Š ì ìˆ˜: {r.score:.1f}/10.0 (ë°©ë²•: {r.reason})")
            print(f"      ğŸ”— URL: {r.video_info.url}")
            print(f"      ğŸ“ ìš”ì•½: {r.video_info.extract[:100]}...")
            print()
        
        print("=" * 80)
        print(f"â±ï¸  {test_name} ì™„ë£Œ - {elapsed:.2f}ì´ˆ")
        print("=" * 80)
        print()
        
        return elapsed, results
        
    except Exception as e:
        elapsed = time.time() - start_time
        print()
        print("=" * 80)
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        print("=" * 80)
        print(f"ì—ëŸ¬: {e}")
        
        import traceback
        traceback.print_exc()
        
        return elapsed, None


async def test_comparison():
    """LLM vs Heuristic ë¹„êµ í…ŒìŠ¤íŠ¸"""
    
    print("\n")
    print("ğŸ”¬" * 40)
    print("LLM vs Heuristic ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("ğŸ”¬" * 40)
    print("\n")
    
    # 1. LLM ê²€ì¦ í…ŒìŠ¤íŠ¸
    time_llm, results_llm = await run_test(
        verify_yt=True, 
        test_name="í…ŒìŠ¤íŠ¸ 1: LLM ê²€ì¦ (verify_yt=True)"
    )
    
    print("\n" + "â”€" * 80 + "\n")
    
    # 2. Heuristic í…ŒìŠ¤íŠ¸
    time_heuristic, results_heuristic = await run_test(
        verify_yt=False, 
        test_name="í…ŒìŠ¤íŠ¸ 2: Heuristic ì ìˆ˜ (verify_yt=False)"
    )
    
    # ê²°ê³¼ ë¹„êµ
    print("\n")
    print("ğŸ“Š" * 40)
    print("ìµœì¢… ë¹„êµ ê²°ê³¼")
    print("ğŸ“Š" * 40)
    print()
    
    print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„:")
    print(f"   â€¢ LLM ê²€ì¦:     {time_llm:.2f}ì´ˆ")
    print(f"   â€¢ Heuristic:    {time_heuristic:.2f}ì´ˆ")
    print(f"   â€¢ ì‹œê°„ ì°¨ì´:    {abs(time_llm - time_heuristic):.2f}ì´ˆ")
    print(f"   â€¢ ë¹ ë¥¸ ë°©ë²•:    {'Heuristic' if time_heuristic < time_llm else 'LLM'} "
          f"({min(time_llm, time_heuristic):.2f}ì´ˆ)")
    print()
    
    if results_llm and results_heuristic:
        print(f"ğŸ“Š ì ìˆ˜ ë¹„êµ:")
        print(f"   â€¢ LLM í‰ê· :     {sum(r.score for r in results_llm)/len(results_llm):.1f}/10.0")
        print(f"   â€¢ Heuristic í‰ê· : {sum(r.score for r in results_heuristic)/len(results_heuristic):.1f}/10.0")
        print()
        
        print(f"ğŸ¯ ì¶”ì²œ ì˜ìƒ:")
        print(f"   â€¢ LLM:         {len(results_llm)}ê°œ")
        print(f"   â€¢ Heuristic:   {len(results_heuristic)}ê°œ")
        print()
    
    print("=" * 80)
    print("ğŸ‰ ë¹„êµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(test_comparison())
"""
YouTubeKit - ê°•ì˜ ì„¹ì…˜ ë§ì¶¤ ìœ íŠœë¸Œ ì¶”ì²œ ëª¨ë“ˆ

LiveNote í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ YouTube Data API ê¸°ë°˜ ë™ì˜ìƒ ì¶”ì²œ ì‹œìŠ¤í…œ
"""

from .service import YouTubeService
from .models import (
    YouTubeRequest,
    YouTubeResponse,
    YouTubeVideoInfo,
    PreviousSummary,
    RAGChunk,
)

__version__ = "0.1.0"

__all__ = [
    "YouTubeService",
    "YouTubeRequest",
    "YouTubeResponse",
    "YouTubeVideoInfo",
    "PreviousSummary",
    "RAGChunk",
]

from .youtube_client import YouTubeAPIClient, YouTubeSearchItem, YouTubeVideoDetail

__all__ = ["YouTubeAPIClient", "YouTubeSearchItem", "YouTubeVideoDetail"]

"""
YouTube API í´ë¼ì´ì–¸íŠ¸ (Data API v3 + Transcript)
"""
from __future__ import annotations

import asyncio
import logging
import time
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

from ..config.youtube_config import YouTubeConfig

logger = logging.getLogger(__name__)


@dataclass
class YouTubeSearchItem:
    """YouTube ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ"""
    video_id: str
    title: str
    description: str
    channel_title: str
    publish_time: str


@dataclass
class YouTubeVideoDetail:
    """YouTube ë™ì˜ìƒ ìƒì„¸ ì •ë³´"""
    video_id: str
    title: str
    description: str
    default_lang: Optional[str]
    view_count: int
    duration_iso8601: Optional[str]
    channel_title: str
    publish_time: str

    def url(self) -> str:
        return f"https://www.youtube.com/watch?v={self.video_id}"


class YouTubeAPIClient:
    """YouTube Data API v3 í´ë¼ì´ì–¸íŠ¸"""
    
    BASE = "https://www.googleapis.com/youtube/v3"

    def __init__(self, api_key: Optional[str] = None, timeout: float | None = None):
        self.api_key = api_key or YouTubeConfig.YOUTUBE_API_KEY
        self.timeout = timeout or YouTubeConfig.TIMEOUT

    async def search_videos(
        self, q: str, lang: str, max_results: int = 8
    ) -> List[YouTubeSearchItem]:
        """YouTube ë™ì˜ìƒ ê²€ìƒ‰"""
        if YouTubeConfig.OFFLINE_MODE or not self.api_key:
            # Offline stub
            return [
                YouTubeSearchItem(
                    video_id=f"stub_{i}",
                    title=f"{q} tutorial {i}",
                    description=f"This is a stub video about {q}.",
                    channel_title="StubChannel",
                    publish_time="2024-01-01T00:00:00Z",
                )
                for i in range(1, min(max_results, 5) + 1)
            ]

        params = {
            "part": "snippet",
            "type": "video",
            "q": q,
            "relevanceLanguage": lang,
            "maxResults": max_results,
            "key": self.api_key,
        }

        from importlib import import_module
        httpx = import_module("httpx")
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            resp = await client.get(f"{self.BASE}/search", params=params)
            resp.raise_for_status()
            data = resp.json()

        items: List[YouTubeSearchItem] = []
        for it in data.get("items", []):
            vid = it.get("id", {}).get("videoId")
            sn = it.get("snippet", {})
            if not vid or not sn:
                continue
            items.append(
                YouTubeSearchItem(
                    video_id=vid,
                    title=sn.get("title", ""),
                    description=sn.get("description", ""),
                    channel_title=sn.get("channelTitle", ""),
                    publish_time=sn.get("publishedAt", ""),
                )
            )
        return items

    async def get_videos(self, ids: List[str]) -> List[YouTubeVideoDetail]:
        """YouTube ë™ì˜ìƒ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        if not ids:
            return []

        if YouTubeConfig.OFFLINE_MODE or not self.api_key:
            # Offline stub
            return [
                YouTubeVideoDetail(
                    video_id=vid,
                    title=f"Stub Video {vid}",
                    description="This is a stub description.",
                    default_lang="en",
                    view_count=1000 + i * 100,
                    duration_iso8601="PT10M",
                    channel_title="StubChannel",
                    publish_time="2024-01-01T00:00:00Z",
                )
                for i, vid in enumerate(ids)
            ]

        params = {
            "part": "snippet,contentDetails,statistics",
            "id": ",".join(ids),
            "key": self.api_key,
        }
        
        from importlib import import_module
        httpx = import_module("httpx")
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            resp = await client.get(f"{self.BASE}/videos", params=params)
            resp.raise_for_status()
            data = resp.json()

        details: List[YouTubeVideoDetail] = []
        for it in data.get("items", []):
            vid = it.get("id")
            sn = it.get("snippet", {})
            stats = it.get("statistics", {})
            cd = it.get("contentDetails", {})
            if not vid:
                continue
            details.append(
                YouTubeVideoDetail(
                    video_id=vid,
                    title=sn.get("title", ""),
                    description=sn.get("description", ""),
                    default_lang=sn.get("defaultLanguage") or sn.get("defaultAudioLanguage"),
                    view_count=int(stats.get("viewCount", 0)),
                    duration_iso8601=cd.get("duration"),
                    channel_title=sn.get("channelTitle", ""),
                    publish_time=sn.get("publishedAt", ""),
                )
            )
        return details

    async def fetch_transcript(
        self, video_id: str, preferred_langs: List[str] | None = None
    ) -> str | None:
        """
        ìë§‰ ê°€ì ¸ì˜¤ê¸° (youtube_transcript_api ì‚¬ìš©)
        
        ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜ (ìë§‰ ì—†ìŒ/ë¹„ê³µê°œ/ì˜¤ë¥˜)
        """
        if YouTubeConfig.OFFLINE_MODE:
            return None
            
        preferred_langs = preferred_langs or ["en", "ko"]
        
        # ğŸ”§ Sync í•¨ìˆ˜ë¥¼ asyncë¡œ ë³€í™˜ (run_in_executor ì‚¬ìš©)
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None, 
            self._fetch_transcript_sync, 
            video_id, 
            preferred_langs
        )
    
    def _fetch_transcript_sync(self, video_id: str, preferred_langs: List[str]) -> str | None:
        """ìë§‰ ê°€ì ¸ì˜¤ê¸° (ë™ê¸° ë²„ì „) - ë²„ì „ í˜¸í™˜"""
        try:
            from youtube_transcript_api import YouTubeTranscriptApi
            from youtube_transcript_api._errors import (
                NoTranscriptFound,
                VideoUnavailable,
                TranscriptsDisabled,
            )
        except ImportError:
            logger.debug("youtube_transcript_api ë¯¸ì„¤ì¹˜: ìë§‰ ìƒëµ (%s)", video_id)
            return None

        try:
            # ë°©ë²• 1: ì¸ìŠ¤í„´ìŠ¤ ë©”ì„œë“œ ì‹œë„ (ìµœì‹  ë²„ì „)
            try:
                api = YouTubeTranscriptApi()
                fetched = api.fetch(video_id, languages=preferred_langs)
                texts = [snippet.text for snippet in fetched]
                return " ".join(texts).strip()[:4000]
            except (AttributeError, NoTranscriptFound):
                pass  # ë‹¤ìŒ ë°©ë²• ì‹œë„

            # ë°©ë²• 2: í´ë˜ìŠ¤ ë©”ì„œë“œ ì‹œë„ (êµ¬ë²„ì „ í˜¸í™˜)
            try:
                chunks = YouTubeTranscriptApi.get_transcript(video_id, languages=preferred_langs)
                texts = [c.get("text", "") for c in chunks]
                return " ".join(texts).strip()[:4000]
            except (AttributeError, NoTranscriptFound):
                pass  # ë‹¤ìŒ ë°©ë²• ì‹œë„

            # ë°©ë²• 3: list ë©”ì„œë“œë¡œ ìˆ˜ë™/ìë™ ìë§‰ ëª¨ë‘ ì‹œë„
            try:
                transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)

                # ìˆ˜ë™ ìë§‰ ìš°ì„ 
                for lang in preferred_langs:
                    try:
                        transcript = transcript_list.find_transcript([lang])
                        chunks = transcript.fetch()
                        texts = [c["text"] for c in chunks]
                        return " ".join(texts).strip()[:4000]
                    except Exception:
                        continue

                # ìë™ ìƒì„± ìë§‰
                try:
                    transcript = transcript_list.find_generated_transcript(preferred_langs)
                    chunks = transcript.fetch()
                    texts = [c["text"] for c in chunks]
                    return " ".join(texts).strip()[:4000]
                except Exception:
                    pass
            except Exception:
                pass

            # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
            logger.debug(f"ìë§‰ ì—†ìŒ (ëª¨ë“  ë°©ë²• ì‹¤íŒ¨): {video_id}")
            return None

        except NoTranscriptFound:
            logger.debug(f"ìë§‰ ì—†ìŒ: {video_id}")
            return None
        except VideoUnavailable:
            logger.warning(f"ë¹„ê³µê°œ/ì‚­ì œëœ ì˜ìƒ: {video_id}")
            return None
        except TranscriptsDisabled:
            logger.debug(f"ìë§‰ ë¹„í™œì„±í™”: {video_id}")
            return None
        except Exception as e:
            logger.warning(f"ìë§‰ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({video_id}): {e}")
            return None
"""
YouTube ì„¤ì • ëª¨ë“ˆ
"""
from . import flags
from .youtube_config import YouTubeConfig
from .prompts import (
    QUERY_GENERATION_PROMPT,
    SUMMARY_PROMPT,
    SUMMARY_NO_TRANSCRIPT_PROMPT,
    SCORE_VIDEO_PROMPT,
)

__all__ = [
    "flags",
    "YouTubeConfig",
    "QUERY_GENERATION_PROMPT",
    "SUMMARY_PROMPT",
    "SUMMARY_NO_TRANSCRIPT_PROMPT",
    "SCORE_VIDEO_PROMPT",
]

"""
YouTube Provider ì „ìš© í”Œë˜ê·¸
"""

# â”â”â” ê²€ì¦ ìŠ¤ìœ„ì¹˜ â”â”â”
#NO_SCORING = True  # Trueì´ë©´ ê²€ì¦ ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
NO_SCORING = False  # Trueì´ë©´ ê²€ì¦ ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
VERIFY_YT_DEFAULT = False  # ê¸°ë³¸ê°’: Heuristic (False), LLM (True)
USE_TRANSCRIPT = False    # ìë§‰ ì‚¬ìš© ì—¬ë¶€ (False: ì œëª©/ì„¤ëª…ë§Œ ì‚¬ìš©)

# â”â”â” ì¿¼ë¦¬ ìƒì„± ì„¤ì • â”â”â”
QUERY_MIN = 1  # ìµœì†Œ ê²€ìƒ‰ ì¿¼ë¦¬ ê°œìˆ˜
QUERY_MAX = 2  # ìµœëŒ€ ê²€ìƒ‰ ì¿¼ë¦¬ ê°œìˆ˜

# â”â”â” ê²€ìƒ‰ ê²°ê³¼ ì„¤ì • â”â”â”
MAX_SEARCH_RESULTS = 8  # YouTube API ê²€ìƒ‰ ì‹œ ìµœëŒ€ ê²°ê³¼ ìˆ˜. 8

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# íœ´ë¦¬ìŠ¤í‹± í•„í„° ê°€ì¤‘ì¹˜ (í•©ê³„ = 1.0)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

WEIGHT_TITLE_MATCH = 0.5  # ì œëª© ìœ ì‚¬ë„
WEIGHT_VIEWS = 0.3         # ì¡°íšŒìˆ˜
WEIGHT_RECENCY = 0.2       # ìµœì‹ ì„±
"""
YouTube LLM í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
"""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± í”„ë¡¬í”„íŠ¸
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

QUERY_GENERATION_PROMPT = """
Generate {query_min}-{query_max} YouTube search queries (use {query_language}) focused on: "{lecture_summary}"

Settings:
- Query language: {query_language}
- Video language preference: {yt_lang}

Additional context (reference only):
- Previous: {previous_summaries}
- RAG: {rag_context}

Return JSON:
{{
  "queries": ["query1", "query2", ...],
  "rationale": "one sentence"
}}

Focus on lecture_summary. Keep queries concise with key technical terms.
""".strip()


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ì˜ìƒ ìš”ì•½ í”„ë¡¬í”„íŠ¸
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

SUMMARY_PROMPT = """
Summarize YouTube video in exactly 3 sentences ({language}).
Make it informative so students understand the content without watching the video.

Title: "{title}"
Content: "{content}"

Return JSON:
{{
  "extract": "Sentence 1. Sentence 2. Sentence 3."
}}

Keep it concise and helpful for learners.
""".strip()


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ìë§‰ ì—†ì´ ìš”ì•½ í”„ë¡¬í”„íŠ¸ (ì œëª©/ì„¤ëª…ë§Œìœ¼ë¡œ ì˜ˆìƒ)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

SUMMARY_NO_TRANSCRIPT_PROMPT = """
Based on title and description only, predict video content in exactly 2 sentences ({language}).

Title: "{title}"
Description: "{description}"
Channel: "{channel}"

Return JSON:
{{
  "extract": "Sentence 1. Sentence 2."
}}

Be brief and informative.
""".strip()


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ì˜ìƒ ì ìˆ˜ í‰ê°€ í”„ë¡¬í”„íŠ¸
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

SCORE_VIDEO_PROMPT = """
Evaluate video relevance to lecture section.

Lecture Summary: {lecture_summary}
Video Title: {title}
Video Summary: {extract}

Scoring (ì—„ê²©í•œ ê¸°ì¤€):
- 10: ê°•ì˜ì—ì„œ ë‹¤ë£¬ ê°œë…ì˜ **ê³µì‹ íŠœí† ë¦¬ì–¼** ë˜ëŠ” **ì°½ì‹œì ì§ì ‘ ì„¤ëª…**
     (Official tutorial OR creator's explanation of the concept)
- 9: ê°•ì˜ í•µì‹¬ ê°œë…ì„ **ì§ì ‘** ë‹¤ë£¨ê³ , ì˜ˆì œ/ì‹¤ìŠµ í¬í•¨
- 7-8: í•µì‹¬ ê°œë… ë‹¤ë£¨ì§€ë§Œ **ë¶€ë¶„ì ** ë˜ëŠ” ì´ë¡  ìœ„ì£¼
- 5-6: ê´€ë ¨ ë°°ê²½ì§€ì‹ì´ì§€ë§Œ ê°•ì˜ ì£¼ì œì™€ ì•½ê°„ ë²—ì–´ë‚¨
- 3-4: í‚¤ì›Œë“œë§Œ ê²¹ì¹˜ê³  ë‹¤ë¥¸ ë§¥ë½
- 1-2: ê±°ì˜ ë¬´ê´€

Return JSON (reason in {language}, one sentence, no line breaks):
{{
  "score": <number>,
  "reason": "ëª…í™•í•œ í•œ ë¬¸ì¥ í‰ê°€"
}}
""".strip()
"""
YouTube ëª¨ë“ˆ ì„¤ì •
"""
import os
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class YouTubeConfig:
    """YouTube ëª¨ë“ˆ ì„¤ì •"""
    
    # â”â”â” OpenAI API â”â”â”
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
    
    # â”â”â” YouTube Data API v3 â”â”â”
    YOUTUBE_API_KEY: str = os.getenv("YOUTUBE_API_KEY") or os.getenv("KEY", "")
    TIMEOUT: int = 10  # HTTP íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    
    # â”â”â” ê¸°ë³¸ê°’ â”â”â”
    DEFAULT_LANGUAGE: str = "ko"
    DEFAULT_TOP_K: int = 5
    DEFAULT_YT_LANG: str = "en"
    
    # â”â”â” ì œí•œ â”â”â”
    MAX_TOP_K: int = 10   # ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜
    # MAX_SEARCH_RESULTSëŠ” flags.pyì—ì„œ ì •ì˜ (ì¤‘ë³µ ì œê±°)
    
    # â”â”â” LLM ì„¤ì • â”â”â”
    #LLM_MODEL: str = "gpt-4o-mini"
    LLM_MODEL: str = "gpt-4o"
    LLM_TEMPERATURE: float = 0.1
    MAX_TOKENS_QUERY: int = 300   # ì¿¼ë¦¬ ìƒì„±ìš©
    MAX_TOKENS_SUMMARY: int = 400  # ìš”ì•½ ìƒì„±ìš©
    MAX_TOKENS_SCORE: int = 300   # ìŠ¤ì½”ì–´ë§ìš©
    
    # â”â”â” ì½˜í…ì¸  ê¸¸ì´ ì œí•œ â”â”â”
    MAX_CONTENT_LENGTH: int = 500  # ìš”ì•½ í”„ë¡¬í”„íŠ¸ì— ë„£ì„ ìµœëŒ€ ê¸€ì ìˆ˜ (í† í° ì ˆì•½)
    
    # â”â”â” ë³‘ë ¬ ì²˜ë¦¬ â”â”â”
    VERIFY_CONCURRENCY: int = 20  # ë™ì‹œ ê²€ì¦ ìˆ˜ (Semaphore ì œí•œ)
    
    # â”â”â” Offline ëª¨ë“œ â”â”â”
    OFFLINE_MODE: bool = os.getenv("YT_OFFLINE_MODE", "0") == "1"
    
    @classmethod
    def validate(cls):
        """ì„¤ì • ê²€ì¦"""
        from . import flags
        
        if not cls.OFFLINE_MODE:
            if not cls.OPENAI_API_KEY:
                raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if not cls.YOUTUBE_API_KEY:
                raise ValueError("YOUTUBE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if flags.MAX_SEARCH_RESULTS < 1:
            raise ValueError(f"MAX_SEARCH_RESULTSëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤: {flags.MAX_SEARCH_RESULTS}")
        
        if cls.VERIFY_CONCURRENCY < 1:
            raise ValueError(
                f"VERIFY_CONCURRENCYëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤: {cls.VERIFY_CONCURRENCY}"
            )
from .openai_client import YouTubeLLMClient

__all__ = ["YouTubeLLMClient"]

"""
YouTube LLM í´ë¼ì´ì–¸íŠ¸ (OpenAI)
"""
from __future__ import annotations

import json
from typing import Any, Dict

from ..config.youtube_config import YouTubeConfig
from ..config import flags
from ..config import (
    QUERY_GENERATION_PROMPT,
    SUMMARY_PROMPT,
    SUMMARY_NO_TRANSCRIPT_PROMPT,
    SCORE_VIDEO_PROMPT,
)


class YouTubeLLMClient:
    """YouTube ì¶”ì²œì„ ìœ„í•œ LLM í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, api_key: str | None = None):
        self.api_key = api_key or YouTubeConfig.OPENAI_API_KEY
        self.client = None
        
        if not YouTubeConfig.OFFLINE_MODE and self.api_key:
            try:
                from openai import AsyncOpenAI
                self.client = AsyncOpenAI(api_key=self.api_key)
            except Exception:
                # OpenAI SDK ì—†ìœ¼ë©´ stub ëª¨ë“œ
                self.client = None

    async def _chat_json(self, prompt: str, max_tokens: int) -> Dict[str, Any]:
        """LLM JSON ì‘ë‹µ ìš”ì²­"""
        if YouTubeConfig.OFFLINE_MODE or not self.client:
            # Offline/í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ê¸°ë³¸ stub ë°˜í™˜
            try:
                return json.loads(prompt)
            except Exception:
                return {"stub": True}

        resp = await self.client.chat.completions.create(
            model=YouTubeConfig.LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=YouTubeConfig.LLM_TEMPERATURE,
            max_tokens=max_tokens,
            response_format={"type": "json_object"},
        )
        content = resp.choices[0].message.content
        return json.loads(content)

    async def generate_queries(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        if YouTubeConfig.OFFLINE_MODE:
            # Stub: ìš”ì•½ì—ì„œ ê°„ë‹¨íˆ ì¶”ì¶œ
            summary = request_data.get("lecture_summary", "topic")
            return {
                "queries": [summary[:40], f"explained {summary[:30]} examples"],
                "rationale": "Derived from lecture summary (stub).",
            }

        query_language = request_data.get("yt_lang") or request_data.get("language") or "en"

        prompt = QUERY_GENERATION_PROMPT.format(
            query_min=flags.QUERY_MIN,
            query_max=flags.QUERY_MAX,
            lecture_summary=request_data.get("lecture_summary", ""),
            query_language=query_language,
            yt_lang=request_data.get("yt_lang", "en"),
            previous_summaries=request_data.get("previous_summaries", []),
            rag_context=request_data.get("rag_context", []),
        )
        return await self._chat_json(prompt, YouTubeConfig.MAX_TOKENS_QUERY)

    async def summarize_content(
        self, *, title: str, content: str, language: str
    ) -> Dict[str, Any]:
        """ì˜ìƒ ë‚´ìš© ìš”ì•½ (3ë¬¸ì¥)"""
        if YouTubeConfig.OFFLINE_MODE:
            text = content.strip().replace("\n", " ")
            if not text:
                text = title
            # ê°„ë‹¨í•œ 3ë¬¸ì¥ ì¶”ì¶œ
            sent = text.split(". ")
            extract = ". ".join(sent[:3])[:400]
            return {"extract": extract or title}

        # ì½˜í…ì¸  ê¸¸ì´ ì œí•œ (í† í° ì ˆì•½)
        truncated_content = content[:YouTubeConfig.MAX_CONTENT_LENGTH]
        if len(content) > YouTubeConfig.MAX_CONTENT_LENGTH:
            truncated_content += "..."
        
        prompt = SUMMARY_PROMPT.format(
            title=title, content=truncated_content, language=language
        )
        return await self._chat_json(prompt, YouTubeConfig.MAX_TOKENS_SUMMARY)

    async def summarize_content_no_transcript(
        self, *, title: str, description: str, channel: str, language: str
    ) -> Dict[str, Any]:
        """ìë§‰ ì—†ì´ ì œëª©/ì„¤ëª…ë§Œìœ¼ë¡œ ìš”ì•½ (2ë¬¸ì¥)"""
        if YouTubeConfig.OFFLINE_MODE:
            text = (description or title).strip().replace("\n", " ")
            sent = text.split(". ")
            extract = ". ".join(sent[:2])[:300]
            return {"extract": extract or title}

        # ì„¤ëª… ê¸¸ì´ ì œí•œ
        truncated_desc = description[:500]
        if len(description) > 500:
            truncated_desc += "..."
        
        prompt = SUMMARY_NO_TRANSCRIPT_PROMPT.format(
            title=title, 
            description=truncated_desc, 
            channel=channel,
            language=language
        )
        return await self._chat_json(prompt, YouTubeConfig.MAX_TOKENS_SUMMARY)

    async def score_video(
        self, *, lecture_summary: str, title: str, extract: str, language: str
    ) -> Dict[str, Any]:
        """ì˜ìƒ ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚° (LLM)"""
        if YouTubeConfig.OFFLINE_MODE:
            # Stub: ê°„ë‹¨í•œ heuristic
            base = 7.0 if title and extract and lecture_summary else 5.0
            reason = "Aligned with key terms (stub)."
            return {"score": base, "reason": reason}

        prompt = SCORE_VIDEO_PROMPT.format(
            lecture_summary=lecture_summary,
            title=title,
            extract=extract,
            language=language,
        )
        return await self._chat_json(prompt, YouTubeConfig.MAX_TOKENS_SCORE)
"""
YouTubeKit ë°ì´í„° ëª¨ë¸ ì •ì˜
"""
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field, field_validator

from .config import flags


class RAGChunk(BaseModel):
    """RAG ê²€ìƒ‰ ê²°ê³¼ ì²­í¬"""
    text: Optional[str] = Field(default=None, description="ì²­í¬ í…ìŠ¤íŠ¸ ë‚´ìš©")
    content: Optional[str] = Field(default=None, description="ì²­í¬ í…ìŠ¤íŠ¸ ë‚´ìš© (textì˜ ë³„ì¹­)")
    score: float = Field(default=0.0, description="ìœ ì‚¬ë„ ì ìˆ˜")
    source: Optional[str] = Field(default=None, description="ì¶œì²˜ (íŒŒì¼ëª… ë“±)")
    metadata: Optional[Dict[str, Any]] = Field(default=None, description="ë©”íƒ€ë°ì´í„°")
    
    @field_validator('text', mode='before')
    @classmethod
    def use_content_if_text_missing(cls, v, info):
        """textê°€ ì—†ìœ¼ë©´ content ì‚¬ìš©"""
        if v is None and 'content' in info.data:
            return info.data['content']
        return v


class PreviousSummary(BaseModel):
    """ì´ì „ ì„¹ì…˜ ìš”ì•½"""
    section_id: int = Field(..., description="ì„¹ì…˜ ë²ˆí˜¸")
    summary: str = Field(..., description="ì„¹ì…˜ ìš”ì•½ í…ìŠ¤íŠ¸")
    timestamp: Optional[int] = Field(default=None, description="íƒ€ì„ìŠ¤íƒ¬í”„ (ms)")


class YouTubeVideoInfo(BaseModel):
    """YouTube ë™ì˜ìƒ ìƒì„¸ ì •ë³´"""
    url: str = Field(..., description="ë™ì˜ìƒ URL")
    title: str = Field(..., description="ë™ì˜ìƒ ì œëª©")
    extract: str = Field(..., description="ë™ì˜ìƒ ìš”ì•½ (3ë¬¸ì¥ ì •ë„)")
    lang: str = Field(..., description="ë™ì˜ìƒ ì–¸ì–´ (ko/en)")
    
    @field_validator('title', 'extract')
    @classmethod
    def normalize_newlines(cls, v: str) -> str:
        """ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜"""
        return v.replace('\n', ' ').replace('\r', ' ')


class YouTubeRequest(BaseModel):
    """YouTube ë™ì˜ìƒ ì¶”ì²œ ìš”ì²­"""
    
    # â”â”â” í•„ìˆ˜ í•„ë“œ â”â”â”
    lecture_id: str = Field(..., description="ê°•ì˜ ì„¸ì…˜ ID (ì¶”ì ìš©)")
    section_id: int = Field(..., ge=1, description="í˜„ì¬ ì„¹ì…˜ ë²ˆí˜¸")
    lecture_summary: str = Field(..., min_length=10, description="í˜„ì¬ ê°•ì˜ ì„¹ì…˜ ìš”ì•½")
    
    # â”â”â” ì„ íƒ í•„ë“œ â”â”â”
    language: str = Field(default="ko", description="ì‘ë‹µ ì–¸ì–´ (ko/en)")
    top_k: int = Field(default=5, ge=1, le=10, description="ì¶”ì²œ ë™ì˜ìƒ ê°œìˆ˜")
    verify_yt: bool = Field(
        default=flags.VERIFY_YT_DEFAULT, 
        description="LLM ê²€ì¦ ì—¬ë¶€ (True: LLM, False: Heuristic)"
    )
    
    # â”â”â” ì»¨í…ìŠ¤íŠ¸ í•„ë“œ â”â”â”
    previous_summaries: List[PreviousSummary] = Field(
        default_factory=list,
        description="ì´ì „ Nê°œ ì„¹ì…˜ ìš”ì•½ (ì»¨í…ìŠ¤íŠ¸ í™•ì¥ìš©)"
    )
    rag_context: List[RAGChunk] = Field(
        default_factory=list,
        description="RAG ê²€ìƒ‰ ê²°ê³¼ (ê°•ì˜ë…¸íŠ¸/ì´ì „ ì„¹ì…˜)"
    )
    
    # â”â”â” ê²€ìƒ‰ ì œì–´ í•„ë“œ â”â”â”
    yt_lang: str = Field(default="en", description="YouTube ê²€ìƒ‰ ì–¸ì–´ (en/ko)")
    exclude_titles: List[str] = Field(
        default_factory=list,
        description="ì œì™¸í•  ë™ì˜ìƒ ì œëª© ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ë°©ì§€)"
    )
    min_score: float = Field(
        default=5.0,
        ge=0.0,
        le=10.0,
        description="ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’ (ì´ ì ìˆ˜ ë¯¸ë§Œ ë™ì˜ìƒ ì œì™¸)"
    )
    
    # â”â”â” ë³„ì¹­ ì§€ì› â”â”â”
    tok_k: Optional[int] = Field(default=None, description="top_k ë³„ì¹­")
    
    @field_validator("top_k")
    @classmethod
    def clamp_top_k(cls, v: int) -> int:
        return max(1, min(10, v))
    
    def effective_top_k(self) -> int:
        """tok_kê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ top_k ì‚¬ìš©"""
        return int(self.tok_k) if self.tok_k is not None else int(self.top_k)


class YouTubeResponse(BaseModel):
    """YouTube ë™ì˜ìƒ ì¶”ì²œ ì‘ë‹µ"""
    lecture_id: str = Field(..., description="ê°•ì˜ ì„¸ì…˜ ID")
    section_id: int = Field(..., description="ì„¹ì…˜ ë²ˆí˜¸")
    video_info: YouTubeVideoInfo = Field(..., description="ë™ì˜ìƒ ì •ë³´")
    reason: str = Field(..., description="ì¶”ì²œ ì´ìœ  (1-2ë¬¸ì¥)")
    score: float = Field(..., ge=0.0, le=15.0, description="ê´€ë ¨ë„ ì ìˆ˜ (0-10, LLMì´ ì´ˆê³¼ ê°€ëŠ¥)")
    
    @field_validator('reason')
    @classmethod
    def normalize_newlines(cls, v: str) -> str:
        """ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜"""
        return v.replace('\n', ' ').replace('\r', ' ')
from __future__ import annotations

import asyncio
import logging
from typing import List

from .api import YouTubeAPIClient
from .llm import YouTubeLLMClient
from .models import (
    YouTubeRequest,
    YouTubeResponse,
    YouTubeVideoInfo,
)
from .utils import normalize_title, deduplicate_items, heuristic_score
from .config import flags

logger = logging.getLogger(__name__)


class YouTubeService:
    """High-level service for lecture-aware YouTube recommendations."""

    def __init__(self, yt_client: YouTubeAPIClient | None = None, llm: YouTubeLLMClient | None = None):
        from .config.youtube_config import YouTubeConfig
        self.yt = yt_client or YouTubeAPIClient(api_key=YouTubeConfig.YOUTUBE_API_KEY)
        self.llm = llm or YouTubeLLMClient(api_key=YouTubeConfig.OPENAI_API_KEY)

    async def recommend_videos(self, request: YouTubeRequest) -> List[YouTubeResponse]:
        from .config.youtube_config import YouTubeConfig
        from .config import flags
        
        # 1) Build queries (LLM or stub)
        q_payload = await self.llm.generate_queries(
            {
                "lecture_summary": request.lecture_summary,
                "language": request.language,
                "yt_lang": request.yt_lang,
                "previous_summaries": [s.model_dump() for s in request.previous_summaries],
                "rag_context": [c.model_dump() for c in request.rag_context],
            }
        )
        queries = list(dict.fromkeys([q.strip() for q in q_payload.get("queries", []) if q.strip()]))
        
        # ğŸ”§ Query ê°œìˆ˜ ì œí•œ (QUERY_MAX)
        if len(queries) > flags.QUERY_MAX:
            logger.info(f"Query ê°œìˆ˜ ì œí•œ: {len(queries)}ê°œ â†’ {flags.QUERY_MAX}ê°œ")
            queries = queries[:flags.QUERY_MAX]
        
        if not queries:
            queries = [request.lecture_summary[:60]]

        # 2) Search videos (fan-out) then fetch details
        # ğŸš€ OPTIMIZATION: Search all queries in parallel
        async def search_single_query(q: str):
            """Search videos for a single query in parallel"""
            items = await self.yt.search_videos(
                q=q, 
                lang=request.yt_lang, 
                max_results=flags.MAX_SEARCH_RESULTS  # ğŸ”§ flagsì—ì„œ ê°€ì ¸ì˜´
            )
            return [(normalize_title(it.title), it) for it in items]
        
        # Execute all searches in parallel
        search_results = await asyncio.gather(*[search_single_query(q) for q in queries])
        
        # Flatten results from all queries
        search_items = []
        for items in search_results:
            search_items.extend(items)

        # Apply exclusion by title early
        excludes = set(normalize_title(t) for t in request.exclude_titles)
        search_items = [(k, v) for (k, v) in search_items if k not in excludes]

        # Deduplicate
        dedup = deduplicate_items((k, v) for (k, v) in search_items)
        if not dedup:
            return []

        ids = [it.video_id for it in dedup]
        details = await self.yt.get_videos(ids)
        detail_map = {d.video_id: d for d in details}

        # ğŸš€ NO_SCORING ëª¨ë“œ: ê²€ì¦ ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜
        if flags.NO_SCORING:
            logger.info("âš¡ NO_SCORING ëª¨ë“œ: ê²€ì¦ ìŠ¤í‚µ (description ì‚¬ìš©)")
            results = []
            for it in dedup[:request.top_k]:
                d = detail_map.get(it.video_id)
                if not d:
                    continue
                
                vi = YouTubeVideoInfo(
                    url=d.url(),
                    title=d.title,
                    extract=d.description or "No description available",
                    lang=request.yt_lang
                )
                results.append(YouTubeResponse(
                    lecture_id=request.lecture_id,
                    section_id=request.section_id,
                    video_info=vi,
                    reason="search",
                    score=10.0
                ))
            logger.info(f"âœ… NO_SCORING ê²°ê³¼: {len(results)}ê°œ ë°˜í™˜")
            return results

        # 3) Build candidate list with summary + (optional) LLM score
        # ğŸš€ OPTIMIZATION: Process videos in parallel with Semaphore (ë™ì‹œì„± ì œí•œ)
        semaphore = asyncio.Semaphore(YouTubeConfig.VERIFY_CONCURRENCY)
        
        async def process_single_video(it):
            """Process one video (summary + optional LLM verification) in parallel"""
            async with semaphore:  # ğŸ”§ ë™ì‹œì„± ì œí•œ
                def best_heuristic_score(title: str, view_count: int, publish_time: str) -> float:
                    """ëª¨ë“  ê²€ìƒ‰ì–´ ì¤‘ ìµœê³  íœ´ë¦¬ìŠ¤í‹± ì ìˆ˜ ê³„ì‚°"""
                    candidate_queries = queries or [request.lecture_summary[:60]]
                    scores = [
                        heuristic_score(
                            title=title,
                            query=q,
                            view_count=view_count,
                            publish_time=publish_time,
                        )
                        for q in candidate_queries
                    ]
                    return max(scores) if scores else 0.0

                d = detail_map.get(it.video_id)
                if not d:
                    # Fall back to basic snippet if details missing
                    title = it.title
                    content = it.description
                    lang = request.yt_lang
                    url = f"https://www.youtube.com/watch?v={it.video_id}"
                    
                    # ğŸ”§ ìë§‰ ì—†ì´ ìš”ì•½ (ì œëª©/ì„¤ëª…ë§Œ)
                    sum_payload = await self.llm.summarize_content_no_transcript(
                        title=title, 
                        description=content, 
                        channel="Unknown",
                        language=request.language
                    )
                    extract = sum_payload.get("extract", content[:300])
                    
                    # Heuristic only
                    base = best_heuristic_score(title=title, view_count=0, publish_time=it.publish_time)
                    if base < request.min_score:
                        return None
                    
                    vi = YouTubeVideoInfo(url=url, title=title, extract=extract, lang=lang)
                    return YouTubeResponse(
                        lecture_id=request.lecture_id,
                        section_id=request.section_id,
                        video_info=vi,
                        reason="Heuristic",
                        score=base,
                    )

                # ğŸ”§ ìë§‰ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
                if flags.USE_TRANSCRIPT:
                    transcript = await self.yt.fetch_transcript(d.video_id, preferred_langs=[request.yt_lang, "en", "ko"])  # type: ignore[arg-type]
                    content_src = transcript or (d.description or d.title)
                    # ìë§‰ì´ ìˆìœ¼ë©´ ì •ìƒ ìš”ì•½
                    sum_payload = await self.llm.summarize_content(
                        title=d.title, content=content_src, language=request.language
                    )
                else:
                    # ìë§‰ ì—†ì´ ì œëª©/ì„¤ëª…ë§Œìœ¼ë¡œ ìš”ì•½
                    sum_payload = await self.llm.summarize_content_no_transcript(
                        title=d.title,
                        description=d.description or "",
                        channel=d.channel_title,
                        language=request.language
                    )
                
                extract = sum_payload.get("extract", (d.description or d.title)[:300])

                # ğŸ”§ verify_yt ëª¨ë“œì— ë”°ë¼ ì ìˆ˜ ê²°ì •
                if request.verify_yt:
                    # âœ… verify_yt=True: LLM ì ìˆ˜ë§Œ ì‚¬ìš©
                    ver = await self.llm.score_video(
                        lecture_summary=request.lecture_summary, 
                        title=d.title, 
                        extract=extract, 
                        language=request.language
                    )
                    score = float(ver.get("score", 5.0) or 5.0)
                    reason = ver.get("reason", "LLM verification")
                else:
                    # âœ… verify_yt=False: Heuristicë§Œ ì‚¬ìš©
                    score = best_heuristic_score(
                        title=d.title,
                        view_count=d.view_count,
                        publish_time=d.publish_time,
                    )
                    reason = "Heuristic"

                if score < request.min_score:
                    return None

                vi = YouTubeVideoInfo(
                    url=d.url(),
                    title=d.title,
                    extract=extract,
                    lang=d.default_lang or request.yt_lang,
                )
                return YouTubeResponse(
                    lecture_id=request.lecture_id,
                    section_id=request.section_id,
                    video_info=vi,
                    reason=reason,
                    score=round(score, 2),
                )
        
        # ğŸš€ Process all videos in parallel with asyncio.gather
        candidate_results = await asyncio.gather(*[process_single_video(it) for it in dedup], return_exceptions=True)
        
        # ğŸ”§ Filter out None and exceptions (with error logging)
        candidates: List[YouTubeResponse] = []
        for idx, result in enumerate(candidate_results):
            if isinstance(result, Exception):
                logger.warning(
                    f"ì˜ìƒ ì²˜ë¦¬ ì‹¤íŒ¨ (idx={idx}, video_id={dedup[idx].video_id if idx < len(dedup) else 'unknown'}): {result}",
                    exc_info=result
                )
            elif result is not None:
                candidates.append(result)

        # 4) Sort and cap by effective top_k
        candidates.sort(key=lambda r: r.score, reverse=True)
        return candidates[: request.effective_top_k()]
from .filters import normalize_title, deduplicate_items, heuristic_score

__all__ = [
    "normalize_title",
    "deduplicate_items",
    "heuristic_score",
]

"""
YouTube ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (í•„í„°ë§, ì¤‘ë³µ ì œê±°, Heuristic ì ìˆ˜)
"""
from __future__ import annotations

import re
import math
from typing import Dict, Iterable, List, Tuple

from rapidfuzz import fuzz
from ..config import flags


def normalize_title(t: str) -> str:
    """ì œëª© ì •ê·œí™” (ì†Œë¬¸ì + íŠ¹ìˆ˜ë¬¸ì ì œê±°)"""
    t = t.lower()
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"[^a-z0-9ê°€-í£ _\-]", "", t)
    return t.strip()


def deduplicate_items(items: Iterable[Tuple[str, Dict]]) -> List[Dict]:
    """ì¤‘ë³µ ì œê±° (í‚¤ ê¸°ë°˜)"""
    seen = set()
    out: List[Dict] = []
    for key, payload in items:
        if key in seen:
            continue
        seen.add(key)
        out.append(payload)
    return out


def _views_score(view_count: int) -> float:
    """ì¡°íšŒìˆ˜ ì ìˆ˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼ â†’ [0, 1])"""
    if view_count <= 0:
        return 0.0
    return min(1.0, math.log10(view_count + 1) / 6.0)


def _recency_score(publish_time: str) -> float:
    """ìµœì‹ ì„± ì ìˆ˜ (ì—°ë„ ê¸°ë°˜ â†’ [0, 1])"""
    # Format: 2024-01-01T00:00:00Z
    try:
        year = int(publish_time[:4])
        return max(0.0, min(1.0, (year - 2015) / 10.0))
    except Exception:
        return 0.5


def heuristic_score(
    *, title: str, query: str, view_count: int, publish_time: str
) -> float:
    """
    Heuristic ì ìˆ˜ ê³„ì‚° (0~10)
    
    - ì œëª© ìœ ì‚¬ë„ (flags.WEIGHT_TITLE_MATCH)
    - ì¡°íšŒìˆ˜ (flags.WEIGHT_VIEWS)
    - ìµœì‹ ì„± (flags.WEIGHT_RECENCY)
    """
    # ì œëª© ìœ ì‚¬ë„ [0, 1]
    sim = fuzz.token_set_ratio(title, query) / 100.0
    
    # ì¡°íšŒìˆ˜, ìµœì‹ ì„± ì ìˆ˜
    v = _views_score(view_count)
    r = _recency_score(publish_time)
    
    # ê°€ì¤‘ í‰ê·  â†’ 0~10 ìŠ¤ì¼€ì¼
    raw = (
        flags.WEIGHT_TITLE_MATCH * sim 
        + flags.WEIGHT_VIEWS * v 
        + flags.WEIGHT_RECENCY * r
    )
    
    return max(0.0, min(10.0, raw * 10.0))
#!/usr/bin/env bash
# LiveNote Docker ê°„í¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

set -e

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "${PROJECT_ROOT}"

echo "ğŸ³ LiveNote Docker ì‹¤í–‰ ë„êµ¬"
echo "================================"

# .env íŒŒì¼ í™•ì¸
if [ ! -f ".env" ]; then
    echo "âš ï¸  .env íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!"
    if [ -f ".env.example" ]; then
        echo "ğŸ“„ .env.exampleì„ .envë¡œ ë³µì‚¬í•©ë‹ˆë‹¤..."
        cp .env.example .env
        echo "âœ… .env íŒŒì¼ ìƒì„± ì™„ë£Œ"
        echo ""
        echo "âš ï¸  ì¤‘ìš”: .env íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ ì‹¤ì œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”!"
        echo "   vi .env"
        echo ""
        read -p "ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n) " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            exit 1
        fi
    else
        echo "âŒ .env.example íŒŒì¼ë„ ì—†ìŠµë‹ˆë‹¤!"
        exit 1
    fi
fi

# Docker ì„¤ì¹˜ í™•ì¸
if ! command -v docker &> /dev/null; then
    echo "âŒ Dockerê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!"
    echo "   https://docs.docker.com/get-docker/"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Composeê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!"
    echo "   https://docs.docker.com/compose/install/"
    exit 1
fi

echo ""
echo "ì„ íƒí•˜ì„¸ìš”:"
echo "1) ë¹Œë“œ & ì‹¤í–‰ (í¬ê·¸ë¼ìš´ë“œ)"
echo "2) ë¹Œë“œ & ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)"
echo "3) ì¤‘ì§€"
echo "4) ë¡œê·¸ ë³´ê¸°"
echo "5) ì™„ì „ ì‚­ì œ (ë°ì´í„° í¬í•¨)"
echo ""
read -p "ì„ íƒ (1-5): " choice

case $choice in
    1)
        echo "ğŸš€ ì„œë²„ë¥¼ ë¹Œë“œí•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤ (Ctrl+Cë¡œ ì¢…ë£Œ)..."
        docker-compose up --build
        ;;
    2)
        echo "ğŸš€ ì„œë²„ë¥¼ ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤..."
        docker-compose up -d --build
        echo ""
        echo "âœ… ì„œë²„ ì‹¤í–‰ ì™„ë£Œ!"
        echo "   â€¢ ìƒíƒœ í™•ì¸: curl http://localhost:8003/health"
        echo "   â€¢ API ë¬¸ì„œ: http://localhost:8003/docs"
        echo "   â€¢ ë¡œê·¸ ë³´ê¸°: docker-compose logs -f"
        echo "   â€¢ ì¤‘ì§€: docker-compose down"
        ;;
    3)
        echo "â¸ï¸  ì„œë²„ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤..."
        docker-compose down
        echo "âœ… ì¤‘ì§€ ì™„ë£Œ"
        ;;
    4)
        echo "ğŸ“‹ ë¡œê·¸ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤ (Ctrl+Cë¡œ ì¢…ë£Œ)..."
        docker-compose logs -f
        ;;
    5)
        echo "âš ï¸  ê²½ê³ : ëª¨ë“  ì»¨í…Œì´ë„ˆì™€ ë°ì´í„°ê°€ ì‚­ì œë©ë‹ˆë‹¤!"
        read -p "ì •ë§ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes ì…ë ¥): " confirm
        if [ "$confirm" = "yes" ]; then
            echo "ğŸ—‘ï¸  ì‚­ì œ ì¤‘..."
            docker-compose down -v
            docker rmi livenote-gateway 2>/dev/null || true
            echo "âœ… ì‚­ì œ ì™„ë£Œ"
        else
            echo "ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
        fi
        ;;
    *)
        echo "âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤."
        exit 1
        ;;
esac
#!/bin/bash
# EC2 ì´ˆê¸° ì„¤ì • ìŠ¤í¬ë¦½íŠ¸

set -e

echo "ğŸš€ LiveNote API EC2 ì´ˆê¸° ì„¤ì • ì‹œì‘..."

# Docker ì„¤ì¹˜
echo "[1/5] Docker ì„¤ì¹˜ ì¤‘..."
sudo yum update -y
sudo yum install -y docker
sudo service docker start
sudo usermod -a -G docker ec2-user

# Docker Compose ì„¤ì¹˜
echo "[2/5] Docker Compose ì„¤ì¹˜ ì¤‘..."
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# ìŠ¤ì™‘ ë©”ëª¨ë¦¬ ì„¤ì • (t2.microìš©)
echo "[3/5] ìŠ¤ì™‘ ë©”ëª¨ë¦¬ ì„¤ì • ì¤‘..."
if [ ! -f /swapfile ]; then
  sudo dd if=/dev/zero of=/swapfile bs=128M count=16
  sudo chmod 600 /swapfile
  sudo mkswap /swapfile
  sudo swapon /swapfile
  echo '/swapfile swap swap defaults 0 0' | sudo tee -a /etc/fstab
  echo "âœ… ìŠ¤ì™‘ ë©”ëª¨ë¦¬ 2GB ìƒì„± ì™„ë£Œ"
else
  echo "â­ï¸  ìŠ¤ì™‘ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤"
fi

# ë””ë ‰í† ë¦¬ ìƒì„±
echo "[4/5] ë””ë ‰í† ë¦¬ ìƒì„± ì¤‘..."
mkdir -p ~/livenote/server_storage/{uploads,chroma_data}

# .env í…œí”Œë¦¿ ìƒì„±
echo "[5/5] .env í…œí”Œë¦¿ ìƒì„± ì¤‘..."
cat > ~/livenote/.env << 'EOF'
# OpenAI API Key (í•„ìˆ˜)
OPENAI_API_KEY=your_openai_api_key_here

# RAG ì„¤ì •
RAG_PERSIST_DIR=server_storage/chroma_data
UPLOAD_DIR=server_storage/uploads

# ì„ íƒì‚¬í•­
# RAG_CHUNK_SIZE=1000
# RAG_CHUNK_OVERLAP=200
# RAG_MAX_TOKENS=500
EOF

echo ""
echo "âœ… ì„¤ì • ì™„ë£Œ!"
echo ""
echo "ğŸ“ ë‹¤ìŒ ë‹¨ê³„:"
echo "1. ì¬ë¡œê·¸ì¸: exit í›„ ë‹¤ì‹œ ì ‘ì† (Docker ê·¸ë£¹ ì ìš©)"
echo "2. .env ìˆ˜ì •: nano ~/livenote/.env"
echo "3. Docker Hubì—ì„œ ì‹¤í–‰:"
echo "   cd ~/livenote"
echo "   docker pull hwkimcode/livenote_ai_api:latest"
echo "   docker run -d --name livenote-api -p 8003:8003 --env-file .env -v ./server_storage:/app/server_storage --restart unless-stopped hwkimcode/livenote_ai_api:latest"
echo ""
echo "ë˜ëŠ” GitHubì—ì„œ ë¹Œë“œ:"
echo "   git clone https://github.com/Team-GongGong-s/module_intergration.git ~/livenote"
echo "   cd ~/livenote"
echo "   nano .env"
echo "   docker-compose up -d --build"
"""
FastAPI ì„œë²„ íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
"""

from .app import create_app

__all__ = ["create_app"]
"""
FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
"""
from __future__ import annotations

from contextlib import asynccontextmanager
from typing import Any

from fastapi import FastAPI

from .config import AppSettings
from .routes import qa_router, rag_router, rec_router


def _ensure_service(service: Any, factory_path: str):
    """ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ í™•ë³´"""
    if service is not None:
        return service
    
    module_name, attr = factory_path.rsplit(".", 1)
    module = __import__(module_name, fromlist=[attr])
    factory = getattr(module, attr)
    return factory()


def create_app(
    settings: AppSettings | None = None,
    *,
    rag_service=None,
    qa_service=None,
    openalex_service=None,
    wiki_service=None,
    youtube_service=None,
    google_service=None,
) -> FastAPI:
    """FastAPI ì•± ìƒì„±"""
    base_settings = settings or AppSettings()
    
    @asynccontextmanager
    async def lifespan(app: FastAPI):
        # ì„œë¹„ìŠ¤ ë° ì„¤ì • ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
        _rag = _ensure_service(rag_service, "cap1_RAG_module.ragkit.service.RAGService")
        _qa = _ensure_service(qa_service, "cap1_QA_module.qakit.service.QAService")
        _openalex = _ensure_service(openalex_service, "cap1_openalex_module.openalexkit.service.OpenAlexService")
        _wiki = _ensure_service(wiki_service, "cap1_wiki_module.wikikit.service.WikiService")
        _youtube = _ensure_service(youtube_service, "cap1_youtube_module.youtubekit.service.YouTubeService")
        _google = _ensure_service(google_service, "cap1_google_module.googlekit.service.GoogleService")
        
        app.state.app_settings = base_settings
        app.state.rag_service = _rag
        app.state.qa_service = _qa
        app.state.openalex_service = _openalex
        app.state.wiki_service = _wiki
        app.state.youtube_service = _youtube
        app.state.google_service = _google
        
        try:
            yield
        finally:
            if openalex_service is None and hasattr(_openalex, "close"):
                await _openalex.close()
    
    app = FastAPI(
        title="LiveNote AI Gateway",
        version="1.0.0",
        description="LiveNote AI ì„œë¹„ìŠ¤ë¥¼ ìœ„í•œ í†µí•© API Gateway",
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        lifespan=lifespan
    )
    
    app.include_router(rag_router)
    app.include_router(qa_router)
    app.include_router(rec_router)
    
    @app.get("/health")
    async def health_check():
        """ê°„ë‹¨í•œ í—¬ìŠ¤ ì²´í¬"""
        return {"status": "ok"}
    
    return app
"""
FastAPI ì„œë²„ ì„¤ì • ì •ì˜
"""
from __future__ import annotations

from typing import List

from pydantic import BaseModel, Field


class RAGSettings(BaseModel):
    """RAG ê´€ë ¨ ì„¤ì •"""
    
    # RAG ì»¬ë ‰ì…˜ IDë¥¼ ë§Œë“¤ ë•Œ ì‚¬ìš©í•  ì ‘ë‘ì‚¬ (lecture_id ê²°í•©)
    collection_prefix: str = Field(default="lecture", description="ì»¬ë ‰ì…˜ ID ì ‘ë‘ì‚¬")
    # QAìš© RAG ê²€ìƒ‰ ê°œìˆ˜
    qa_retrieve_top_k: int = Field(default=2, ge=1, description="QA ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  ì²­í¬ ê°œìˆ˜")
    # RECìš© RAG ê²€ìƒ‰ ê°œìˆ˜
    rec_retrieve_top_k: int = Field(default=2, ge=1, description="REC ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  ì²­í¬ ê°œìˆ˜")


class QASettings(BaseModel):
    """QA ì„œë¹„ìŠ¤ ì„¤ì •"""
    
    language: str = Field(default="ko", description="QA ìƒì„± ì–¸ì–´")
    question_types: List[str] = Field(
        default_factory=lambda: ["ì‘ìš©", "ë¹„êµ", "ê°œë…", "ì‹¬í™”"],
        description="ìƒì„±í•  ì§ˆë¬¸ ìœ í˜•"
    )
    qa_top_k: int = Field(default=4, ge=1, description="QA ìƒì„± ê°œìˆ˜")


class OpenAlexSettings(BaseModel):
    """OpenAlex ì¶”ì²œ ì„¤ì •"""
    
    top_k: int = Field(default=2, ge=1, le=10, description="ë…¼ë¬¸ ì¶”ì²œ ê°œìˆ˜")
    verify: bool = Field(default=True, description="LLM ê²€ì¦ ì—¬ë¶€")
    #verify: bool = Field(default=False, description="LLM ê²€ì¦ ì—¬ë¶€")
    year_from: int = Field(default=1960, description="ê²€ìƒ‰ ìµœì†Œ ì—°ë„")
    sort_by: str = Field(default="hybrid", description="ì •ë ¬ ê¸°ì¤€")
    min_score: float = Field(default=5.0, ge=0.0, le=10.0, description="ìµœì†Œ ì ìˆ˜")
    language: str = Field(default="ko", description="ì‘ë‹µ ì–¸ì–´")


class WikiSettings(BaseModel):
    """ìœ„í‚¤ ì¶”ì²œ ì„¤ì •"""
    
    top_k: int = Field(default=2, ge=1, le=10, description="Wiki ì¶”ì²œ ê°œìˆ˜")
    verify: bool = Field(default=False, description="LLM ê²€ì¦ ì—¬ë¶€")
    #verify: bool = Field(default=True, description="LLM ê²€ì¦ ì—¬ë¶€")
    wiki_lang: str = Field(default="en", description="Wikipedia ê²€ìƒ‰ ì–¸ì–´")
    language: str = Field(default="ko", description="ì‘ë‹µ ì–¸ì–´")
    min_score: float = Field(default=5.0, ge=0.0, le=10.0, description="ìµœì†Œ ì ìˆ˜")
    fallback_to_ko: bool = Field(default=True, description="ë¶€ì¡± ì‹œ ì–¸ì–´ fallback ì—¬ë¶€")


class YouTubeSettings(BaseModel):
    """YouTube ì¶”ì²œ ì„¤ì •"""
    
    top_k: int = Field(default=2, ge=1, le=10, description="YouTube ì¶”ì²œ ê°œìˆ˜")
    verify: bool = Field(default=True, description="LLM ê²€ì¦ ì—¬ë¶€")
    #verify: bool = Field(default=False, description="LLM ê²€ì¦ ì—¬ë¶€")
    yt_lang: str = Field(default="en", description="YouTube ê²€ìƒ‰ ì–¸ì–´")
    language: str = Field(default="ko", description="ì‘ë‹µ ì–¸ì–´")
    min_score: float = Field(default=7.0, ge=0.0, le=10.0, description="ìµœì†Œ ì ìˆ˜")


class GoogleSettings(BaseModel):
    """Google ê²€ìƒ‰ ì¶”ì²œ ì„¤ì •"""
    
    top_k: int = Field(default=2, ge=1, le=10, description="Google ì¶”ì²œ ê°œìˆ˜")
    verify: bool = Field(default=True, description="LLM ê²€ì¦ ì—¬ë¶€")
    #verify: bool = Field(default=False, description="LLM ê²€ì¦ ì—¬ë¶€")
    search_lang: str = Field(default="en", description="Google ê²€ìƒ‰ ì–¸ì–´")
    language: str = Field(default="ko", description="ì‘ë‹µ ì–¸ì–´")
    min_score: float = Field(default=3.0, ge=0.0, le=10.0, description="ìµœì†Œ ì ìˆ˜")


class RECSettings(BaseModel):
    """REC í†µí•© ì„¤ì •"""
    
    openalex: OpenAlexSettings = Field(default_factory=OpenAlexSettings)
    wiki: WikiSettings = Field(default_factory=WikiSettings)
    youtube: YouTubeSettings = Field(default_factory=YouTubeSettings)
    google: GoogleSettings = Field(default_factory=GoogleSettings)


class AppSettings(BaseModel):
    """ì„œë²„ ì „ì²´ ì„¤ì •"""
    
    rag: RAGSettings = Field(default_factory=RAGSettings)
    qa: QASettings = Field(default_factory=QASettings)
    rec: RECSettings = Field(default_factory=RECSettings)
"""
FastAPI ì˜ì¡´ì„± í—¬í¼
"""
from __future__ import annotations

from fastapi import Request

from .config import AppSettings

async def get_settings(request: Request) -> AppSettings:
    """ì•± ì„¤ì • ì¡°íšŒ"""
    return request.app.state.app_settings


async def get_rag_service(request: Request):
    """RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return request.app.state.rag_service


async def get_qa_service(request: Request):
    """QA ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return request.app.state.qa_service


async def get_openalex_service(request: Request):
    """OpenAlex ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return request.app.state.openalex_service


async def get_wiki_service(request: Request):
    """Wikipedia ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return request.app.state.wiki_service


async def get_youtube_service(request: Request):
    """YouTube ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return request.app.state.youtube_service


async def get_google_service(request: Request):
    """Google ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤"""
    return request.app.state.google_service
"""
FastAPI ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
"""
from __future__ import annotations

from .app import create_app

app = create_app()
"""
FastAPI ë¼ìš°í„° ëª¨ìŒ
"""

from .rag import router as rag_router
from .qa import router as qa_router
from .rec import router as rec_router

__all__ = ["rag_router", "qa_router", "rec_router"]
"""
QA ìƒì„± API (Callback)
"""
from __future__ import annotations

import asyncio
import logging
from typing import Optional, List

import httpx
from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import Field, HttpUrl, validator

from cap1_QA_module.qakit.models import QARequest, PreviousQA

from ..config import AppSettings
from ..dependencies import get_qa_service, get_rag_service, get_settings
from ..utils import CamelModel, build_collection_id, to_qa_rag_context

router = APIRouter(prefix="/qa", tags=["QA"])
logger = logging.getLogger(__name__)


class PreviousQAItem(CamelModel):
    """ì´ì „ QA í•­ëª© (API ì…ë ¥ìš©)"""
    type: str = Field(..., description="ì§ˆë¬¸ ìœ í˜• (ê°œë…/ì‘ìš©/ë¹„êµ/ì‹¬í™”/ì‹¤ìŠµ)")
    question: str = Field(..., description="ì§ˆë¬¸ ë‚´ìš©")
    answer: str = Field(..., description="ë‹µë³€ ë‚´ìš©")


class QAGenerateRequest(CamelModel):
    """QA ìƒì„± ì…ë ¥"""

    lecture_id: int = Field(..., ge=1, description="ê°•ì˜ ID")
    summary_id: Optional[int] = Field(default=None, description="ìš”ì•½ ID")
    section_index: int = Field(..., ge=0, description="ì„¹ì…˜ ì¸ë±ìŠ¤(0-base)")
    section_summary: str = Field(..., min_length=10, description="ì„¹ì…˜ ìš”ì•½")
    subject: Optional[str] = Field(default=None, description="ì„ íƒ ê³¼ëª© ì •ë³´")
    callback_url: HttpUrl = Field(..., description="ì½œë°± URL")
    previous_qa: Optional[List[PreviousQAItem]] = Field(
        default_factory=list,
        description="ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ì´ì „ QA ëª©ë¡"
    )

    @validator("section_summary")
    def validate_section_summary(cls, value: str) -> str:
        if not value.strip():
            raise ValueError("section_summaryëŠ” ë¹„ì–´ ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return value


@router.post("/generate", status_code=status.HTTP_202_ACCEPTED)
async def generate_qa(
    request: QAGenerateRequest,
    rag_service=Depends(get_rag_service),
    qa_service=Depends(get_qa_service),
    settings: AppSettings = Depends(get_settings),
):
    """QA ìƒì„± ì½œë°± ì—”ë“œí¬ì¸íŠ¸"""
    lecture_id_str = str(request.lecture_id)
    collection_id = build_collection_id(settings.rag.collection_prefix, lecture_id_str)
    section_id_for_provider = request.section_index + 1  # QA ëª¨ë“ˆì€ 1-base

    def _retrieve():
        return rag_service.retrieve(
            collection_id=collection_id,
            query=request.section_summary,
            top_k=settings.rag.qa_retrieve_top_k
        )

    try:
        rag_chunks = await asyncio.to_thread(_retrieve)
    except Exception as exc:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {exc}"
        ) from exc

    qa_question_types = settings.qa.question_types[: settings.qa.qa_top_k]
    if not qa_question_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="ìƒì„±í•  ì§ˆë¬¸ ìœ í˜•ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        )

    qa_request = QARequest(
        lecture_id=lecture_id_str,
        section_id=section_id_for_provider,
        section_summary=request.section_summary,
        subject=request.subject,
        language=settings.qa.language,
        question_types=qa_question_types,
        qa_count=len(qa_question_types),
        rag_context=to_qa_rag_context(rag_chunks),
        previous_qa=[
            PreviousQA(type=item.type, question=item.question, answer=item.answer)
            for item in request.previous_qa
        ]
    )

    async def run_and_callback():
        qna_items: List[dict] = []
        try:
            async for event_type, q_type, payload in qa_service.stream_questions(qa_request):
                if event_type == "qa":
                    qna_items.append(
                        {
                            "type": q_type,
                            "question": payload.get("question"),
                            "answer": payload.get("answer"),
                        }
                    )
        except Exception as exc:  # pragma: no cover - ì™¸ë¶€ ëª¨ë“ˆ ì˜ˆì™¸
            logger.exception("QA ìƒì„± ì‹¤íŒ¨: %s", exc)
        finally:
            await post_qna_callback(request, qna_items)

    asyncio.create_task(run_and_callback())
    return {"status": "accepted", "collection_id": collection_id}


async def post_qna_callback(request: QAGenerateRequest, qna_items: List[dict]):
    """ì½œë°± URLë¡œ QA ê²°ê³¼ ì „ì†¡"""
    payload = {
        "lectureId": request.lecture_id,
        "summaryId": request.summary_id,
        "sectionIndex": request.section_index,
        "qnaList": qna_items,
    }
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            await client.post(str(request.callback_url), json=payload)
    except Exception as exc:  # pragma: no cover - ë„¤íŠ¸ì›Œí¬ ì˜ˆì™¸
        logger.exception("QA ì½œë°± ì „ì†¡ ì‹¤íŒ¨: %s", exc)
"""
RAG ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸
"""
from __future__ import annotations

import asyncio
import json
from pathlib import Path
from typing import Any, List, Optional

from fastapi import APIRouter, Depends, File, Form, HTTPException, UploadFile, status
from pydantic import BaseModel, Field, validator

from ..config import AppSettings
from ..dependencies import get_rag_service, get_settings
from ..utils import build_collection_id

router = APIRouter(prefix="/rag", tags=["RAG"])

UPLOAD_ROOT = Path("server_storage/uploads")


class TextUpsertItem(BaseModel):
    """í…ìŠ¤íŠ¸ ì—…ì„œíŠ¸ ì…ë ¥ ì²­í¬"""
    
    text: str = Field(..., min_length=1, description="ì €ì¥í•  í…ìŠ¤íŠ¸")
    id: Optional[str] = Field(default=None, description="ì„ íƒì  ID")
    section_id: Optional[str] = Field(default=None, description="ì„¹ì…˜ ID")
    metadata: dict[str, Any] = Field(default_factory=dict, description="ì¶”ê°€ ë©”íƒ€ë°ì´í„°")


class TextUpsertRequest(BaseModel):
    """í…ìŠ¤íŠ¸ ì—…ì„œíŠ¸ ìš”ì²­"""
    
    lecture_id: str = Field(..., min_length=1, description="ê°•ì˜ ID")
    items: List[TextUpsertItem] = Field(..., min_length=1, description="ì—…ì„œíŠ¸í•  ì²­í¬ë“¤")

    @validator("lecture_id")
    def validate_lecture_id(cls, value: str) -> str:
        if not value.strip():
            raise ValueError("lecture_idëŠ” ë¹„ì–´ ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return value


async def _ensure_upload_dir(lecture_id: str) -> Path:
    """ì—…ë¡œë“œ ë””ë ‰í„°ë¦¬ ìƒì„±"""
    target_dir = UPLOAD_ROOT / lecture_id
    target_dir.mkdir(parents=True, exist_ok=True)
    return target_dir


@router.post("/pdf-upsert", status_code=status.HTTP_200_OK)
async def upsert_pdf(
    lecture_id: str = Form(..., description="ê°•ì˜ ID"),
    file: UploadFile = File(..., description="ì—…ë¡œë“œí•  PDF íŒŒì¼"),
    base_metadata: str | None = Form(None, description="PDF ì „ì²´ì— ì ìš©í•  ë©”íƒ€ë°ì´í„°(JSON)"),
    rag_service=Depends(get_rag_service),
    settings: AppSettings = Depends(get_settings),
):
    """PDF ë¬¸ì„œë¥¼ ì—…ì„œíŠ¸"""
    if file.content_type != "application/pdf":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
    
    lecture_id = lecture_id.strip()
    if not lecture_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="lecture_idëŠ” ë¹„ì–´ ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
    
    metadata_dict: Optional[dict[str, Any]] = None
    if base_metadata:
        try:
            metadata_dict = json.loads(base_metadata)
        except json.JSONDecodeError as exc:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"base_metadata JSON íŒŒì‹± ì‹¤íŒ¨: {exc}"
            ) from exc
    
    upload_dir = await _ensure_upload_dir(lecture_id)
    safe_name = Path(file.filename or "uploaded.pdf").name
    pdf_path = upload_dir / safe_name
    
    content = await file.read()
    if not content:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="ë¹ˆ íŒŒì¼ì€ ì—…ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
    pdf_path.write_bytes(content)
    
    collection_id = build_collection_id(settings.rag.collection_prefix, lecture_id)
    
    def _run():
        return rag_service.upsert_pdf(
            collection_id=collection_id,
            pdf_path=str(pdf_path),
            base_metadata=metadata_dict
        )
    
    result = await asyncio.to_thread(_run)
    return {"collection_id": collection_id, "result": result}


@router.post("/text-upsert", status_code=status.HTTP_200_OK)
async def upsert_text(
    request: TextUpsertRequest,
    rag_service=Depends(get_rag_service),
    settings: AppSettings = Depends(get_settings),
):
    """í…ìŠ¤íŠ¸ ìš”ì•½ë³¸ ì—…ì„œíŠ¸"""
    collection_id = build_collection_id(settings.rag.collection_prefix, request.lecture_id)
    
    upsert_items = []
    for item in request.items:
        metadata = dict(item.metadata or {})
        if item.section_id:
            metadata.setdefault("section_id", item.section_id)
        if not metadata:
            metadata["source"] = "text"
        upsert_items.append(
            {
                "text": item.text,
                "id": item.id,
                "metadata": metadata,
                "section_id": item.section_id,
            }
        )
    
    def _run():
        return rag_service.upsert_text(collection_id=collection_id, items=upsert_items)
    
    result = await asyncio.to_thread(_run)
    return {"collection_id": collection_id, "result": result}
"""
REC í†µí•© API (Callback)
"""
from __future__ import annotations

import asyncio
import logging
from typing import List, Optional

import httpx
from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import Field, HttpUrl, validator

from cap1_openalex_module.openalexkit.models import (
    OpenAlexRequest,
    OpenAlexResponse,
    PreviousSectionSummary as OpenAlexPreviousSummary,
)
from cap1_wiki_module.wikikit.models import (
    PreviousSummary as WikiPreviousSummary,
    WikiRequest,
    WikiResponse,
)
from cap1_youtube_module.youtubekit.models import (
    PreviousSummary as YouTubePreviousSummary,
    YouTubeRequest,
    YouTubeResponse,
)
from cap1_google_module.googlekit.models import (
    PreviousSummary as GooglePreviousSummary,
    GoogleRequest,
    GoogleResponse,
)

from ..config import AppSettings
from ..dependencies import (
    get_openalex_service,
    get_rag_service,
    get_settings,
    get_wiki_service,
    get_youtube_service,
    get_google_service,
)
from ..utils import (
    CamelModel,
    build_collection_id,
    to_openalex_rag_chunks,
    to_wiki_rag_chunks,
    to_youtube_rag_chunks,
    to_google_rag_chunks,
)

router = APIRouter(prefix="/rec", tags=["REC"])
logger = logging.getLogger(__name__)


class PreviousSummary(CamelModel):
    """ì´ì „ ì„¹ì…˜ ìš”ì•½ ì •ë³´"""

    section_index: int = Field(..., ge=0, description="ì„¹ì…˜ ID")
    summary: str = Field(..., min_length=5, description="ìš”ì•½ ë‚´ìš©")
    timestamp: Optional[int] = Field(default=None, description="íƒ€ì„ìŠ¤íƒ¬í”„(ms)")


class RECRequest(CamelModel):
    """REC í†µí•© ìš”ì²­"""

    lecture_id: int = Field(..., ge=1, description="ê°•ì˜ ID")
    summary_id: Optional[int] = Field(default=None, description="ìš”ì•½ ID")
    section_index: int = Field(..., ge=0, description="ì„¹ì…˜ ì¸ë±ìŠ¤(0-base)")
    section_summary: str = Field(..., min_length=10, description="ì„¹ì…˜ ìš”ì•½")
    callback_url: HttpUrl = Field(..., description="ì½œë°± URL")
    previous_summaries: List[PreviousSummary] = Field(default_factory=list, description="ì´ì „ ìš”ì•½")
    yt_exclude: List[str] = Field(default_factory=list, description="ì œì™¸í•  ìœ íŠœë¸Œ ì œëª©")
    wiki_exclude: List[str] = Field(default_factory=list, description="ì œì™¸í•  ìœ„í‚¤ ì œëª©")
    paper_exclude: List[str] = Field(default_factory=list, description="ì œì™¸í•  ë…¼ë¬¸ ID")
    google_exclude: List[str] = Field(default_factory=list, description="ì œì™¸í•  êµ¬ê¸€ URL")

    @validator("section_summary")
    def validate_section_summary(cls, value: str) -> str:
        if not value.strip():
            raise ValueError("section_summaryëŠ” ë¹„ì–´ ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return value


@router.post("/recommend", status_code=status.HTTP_202_ACCEPTED)
async def recommend_resources(
    request: RECRequest,
    rag_service=Depends(get_rag_service),
    openalex_service=Depends(get_openalex_service),
    wiki_service=Depends(get_wiki_service),
    youtube_service=Depends(get_youtube_service),
    google_service=Depends(get_google_service),
    settings: AppSettings = Depends(get_settings),
):
    """ë…¼ë¬¸/ìœ„í‚¤/ìœ íŠœë¸Œ/êµ¬ê¸€ ì¶”ì²œ ì½œë°± ì—”ë“œí¬ì¸íŠ¸"""
    lecture_id_str = str(request.lecture_id)
    collection_id = build_collection_id(settings.rag.collection_prefix, lecture_id_str)
    section_id_for_provider = request.section_index + 1  # ì™¸ë¶€ ëª¨ë“ˆì€ 1-base

    def _retrieve():
        return rag_service.retrieve(
            collection_id=collection_id,
            query=request.section_summary,
            top_k=settings.rag.rec_retrieve_top_k,
        )

    try:
        rag_chunks = await asyncio.to_thread(_retrieve)
    except Exception as exc:  # pragma: no cover - íŒŒë¼ë¯¸í„° ê²€ì¦
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {exc}",
        ) from exc

    openalex_prev = [
        OpenAlexPreviousSummary(
            section_id=ps.section_index + 1,
            summary=ps.summary,
            timestamp=ps.timestamp,
        )
        for ps in request.previous_summaries
    ]
    wiki_prev = [
        WikiPreviousSummary(
            section_id=ps.section_index + 1,
            summary=ps.summary,
            timestamp=ps.timestamp,
        )
        for ps in request.previous_summaries
    ]
    youtube_prev = [
        YouTubePreviousSummary(
            section_id=ps.section_index + 1,
            summary=ps.summary,
            timestamp=ps.timestamp,
        )
        for ps in request.previous_summaries
    ]
    google_prev = [
        GooglePreviousSummary(
            section_id=ps.section_index + 1,
            summary=ps.summary,
            timestamp=str(ps.timestamp) if ps.timestamp is not None else None,
        )
        for ps in request.previous_summaries
    ]

    openalex_request = OpenAlexRequest(
        lecture_id=lecture_id_str,
        section_id=section_id_for_provider,
        section_summary=request.section_summary,
        language=settings.rec.openalex.language,
        top_k=settings.rec.openalex.top_k,
        verify_openalex=settings.rec.openalex.verify,
        previous_summaries=openalex_prev,
        rag_context=to_openalex_rag_chunks(rag_chunks),
        year_from=settings.rec.openalex.year_from,
        exclude_ids=request.paper_exclude,
        sort_by=settings.rec.openalex.sort_by,
        min_score=settings.rec.openalex.min_score,
    )

    wiki_request = WikiRequest(
        lecture_id=lecture_id_str,
        section_id=section_id_for_provider,
        lecture_summary=request.section_summary,
        language=settings.rec.wiki.language,
        top_k=settings.rec.wiki.top_k,
        verify_wiki=settings.rec.wiki.verify,
        previous_summaries=wiki_prev,
        rag_context=to_wiki_rag_chunks(rag_chunks),
        wiki_lang=settings.rec.wiki.wiki_lang,
        fallback_to_ko=settings.rec.wiki.fallback_to_ko,
        exclude_titles=request.wiki_exclude,
        min_score=settings.rec.wiki.min_score,
    )

    youtube_request = YouTubeRequest(
        lecture_id=lecture_id_str,
        section_id=section_id_for_provider,
        lecture_summary=request.section_summary,
        language=settings.rec.youtube.language,
        top_k=settings.rec.youtube.top_k,
        verify_yt=settings.rec.youtube.verify,
        previous_summaries=youtube_prev,
        rag_context=to_youtube_rag_chunks(rag_chunks),
        yt_lang=settings.rec.youtube.yt_lang,
        exclude_titles=request.yt_exclude,
        min_score=settings.rec.youtube.min_score,
    )

    google_request = GoogleRequest(
        lecture_id=lecture_id_str,
        section_id=section_id_for_provider,
        lecture_summary=request.section_summary,
        language=settings.rec.google.language,
        top_k=settings.rec.google.top_k,
        verify_google=settings.rec.google.verify,
        previous_summaries=google_prev,
        rag_context=to_google_rag_chunks(rag_chunks),
        search_lang=settings.rec.google.search_lang,
        exclude_urls=request.google_exclude,
        min_score=settings.rec.google.min_score,
    )

    async def provider_task(source: str, coro):
        try:
            result = await coro
            await post_resources_callback(request, map_resources(source, result))
        except Exception as exc:  # pragma: no cover - ì™¸ë¶€ ì„œë¹„ìŠ¤ ì˜ˆì™¸
            logger.exception("REC provider %s ì‹¤íŒ¨: %s", source, exc)
            await post_resources_callback(request, [])

    asyncio.create_task(
        provider_task("openalex", openalex_service.recommend_papers(openalex_request))
    )
    asyncio.create_task(
        provider_task("wiki", wiki_service.recommend_pages(wiki_request))
    )
    asyncio.create_task(
        provider_task("youtube", youtube_service.recommend_videos(youtube_request))
    )
    asyncio.create_task(
        provider_task("google", google_service.recommend_results(google_request))
    )

    return {"status": "accepted", "collection_id": collection_id}


def map_resources(source: str, result: List) -> List[dict]:
    """provider ì‘ë‹µì„ ì½œë°±ìš© ë¦¬ì†ŒìŠ¤ í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    mapped = []
    if source == "openalex":
        for item in result:  # type: OpenAlexResponse
            mapped.append(
                {
                    "type": "paper",
                    "title": item.paper_info.title,
                    "url": item.paper_info.url,
                    "description": item.paper_info.abstract,
                    "score": item.score,
                    "reason": item.reason,
                    "detail": item.paper_info.model_dump(),
                }
            )
    elif source == "wiki":
        for item in result:  # type: WikiResponse
            mapped.append(
                {
                    "type": "wiki",
                    "title": item.page_info.title,
                    "url": item.page_info.url,
                    "description": item.page_info.extract,
                    "score": item.score,
                    "reason": item.reason,
                    "detail": item.page_info.model_dump(),
                }
            )
    elif source == "youtube":
        for item in result:  # type: YouTubeResponse
            mapped.append(
                {
                    "type": "video",
                    "title": item.video_info.title,
                    "url": item.video_info.url,
                    "description": item.video_info.extract,
                    "score": item.score,
                    "reason": item.reason,
                    "detail": item.video_info.model_dump(),
                }
            )
    elif source == "google":
        for item in result:  # type: GoogleResponse
            mapped.append(
                {
                    "type": "google",
                    "title": item.search_result.title,
                    "url": item.search_result.url,
                    "description": item.search_result.snippet,
                    "score": item.score,
                    "reason": item.reason,
                    "detail": item.search_result.model_dump(),
                }
            )
    return mapped


async def post_resources_callback(request: RECRequest, resources: List[dict]):
    """ì½œë°± URLë¡œ ì¶”ì²œ ìë£Œ ì „ì†¡"""
    payload = {
        "lectureId": request.lecture_id,
        "summaryId": request.summary_id,
        "sectionIndex": request.section_index,
        "resources": resources,
    }
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            await client.post(str(request.callback_url), json=payload)
    except Exception as exc:  # pragma: no cover - ë„¤íŠ¸ì›Œí¬ ì˜ˆì™¸
        logger.exception("ì½œë°± ì „ì†¡ ì‹¤íŒ¨: %s", exc)
"""
ì„œë²„ ìœ í‹¸ í•¨ìˆ˜ ëª¨ìŒ
"""
from __future__ import annotations

import json
from typing import Any, Iterable, List

from cap1_QA_module.qakit.models import RAGChunk as QARAGChunk, RAGContext as QARAGContext
from cap1_openalex_module.openalexkit.models import RAGChunk as OpenAlexRAGChunk
from cap1_wiki_module.wikikit.models import RAGChunk as WikiRAGChunk
from cap1_youtube_module.youtubekit.models import RAGChunk as YouTubeRAGChunk
from cap1_google_module.googlekit.models import RAGChunk as GoogleRAGChunk
from pydantic import BaseModel, ConfigDict


def build_collection_id(prefix: str, lecture_id: str) -> str:
    """RAG ì»¬ë ‰ì…˜ ID ìƒì„±"""
    lecture_id = lecture_id.strip()
    if not lecture_id:
        raise ValueError("lecture_idëŠ” ë¹„ì–´ ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return f"{prefix}_{lecture_id}"


def _as_metadata(chunk: Any) -> dict:
    """ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    metadata = getattr(chunk, "metadata", None)
    if metadata is None:
        return {}
    if isinstance(metadata, dict):
        return metadata
    return dict(metadata)


def to_qa_rag_context(chunks: Iterable[Any]) -> QARAGContext:
    """RAG ì²­í¬ë¥¼ QA ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    qa_chunks = []
    for chunk in chunks:
        qa_chunks.append(
            QARAGChunk(
                text=getattr(chunk, "text", ""),
                score=getattr(chunk, "score", 0.0),
                metadata=_as_metadata(chunk)
            )
        )
    return QARAGContext(chunks=qa_chunks)


def to_openalex_rag_chunks(chunks: Iterable[Any]) -> List[OpenAlexRAGChunk]:
    """RAG ì²­í¬ë¥¼ OpenAlex ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    return [
        OpenAlexRAGChunk(text=getattr(chunk, "text", ""), score=getattr(chunk, "score", 0.0), metadata=_as_metadata(chunk))
        for chunk in chunks
    ]


def to_wiki_rag_chunks(chunks: Iterable[Any]) -> List[WikiRAGChunk]:
    """RAG ì²­í¬ë¥¼ Wikipedia ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    return [
        WikiRAGChunk(text=getattr(chunk, "text", ""), score=getattr(chunk, "score", 0.0), metadata=_as_metadata(chunk))
        for chunk in chunks
    ]


def to_youtube_rag_chunks(chunks: Iterable[Any]) -> List[YouTubeRAGChunk]:
    """RAG ì²­í¬ë¥¼ YouTube ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    return [
        YouTubeRAGChunk(text=getattr(chunk, "text", ""), score=getattr(chunk, "score", 0.0), metadata=_as_metadata(chunk))
        for chunk in chunks
    ]


def to_google_rag_chunks(chunks: Iterable[Any]) -> List[GoogleRAGChunk]:
    """RAG ì²­í¬ë¥¼ Google ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    return [
        GoogleRAGChunk(text=getattr(chunk, "text", ""), score=getattr(chunk, "score", 0.0), metadata=_as_metadata(chunk))
        for chunk in chunks
    ]


def format_sse(data: dict, event: str | None = None) -> bytes:
    """SSE í¬ë§·ìœ¼ë¡œ ì§ë ¬í™”"""
    serialized = json.dumps(data, ensure_ascii=False)
    if event:
        payload = f"event: {event}\n"
    else:
        payload = ""
    payload += f"data: {serialized}\n\n"
    return payload.encode("utf-8")


def to_camel(string: str) -> str:
    """snake_case â†’ camelCase ë³€í™˜"""
    parts = string.split("_")
    return parts[0] + "".join(word.capitalize() for word in parts[1:])


class CamelModel(BaseModel):
    """camelCase ìš”ì²­/ì‘ë‹µì„ ìœ„í•œ ë² ì´ìŠ¤ ëª¨ë¸"""
    model_config = ConfigDict(alias_generator=to_camel, populate_by_name=True)
#!/usr/bin/env bash
# FastAPI ì„œë²„ ì‹¤í–‰ í™˜ê²½ ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸

set -euo pipefail

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
VENV_DIR="${PROJECT_ROOT}/.venv"
REQUIREMENTS_FILE="${PROJECT_ROOT}/requirements.server.txt"
ENV_EXAMPLE="${PROJECT_ROOT}/.env.example"

PYTHON_BIN_DEFAULT="python3.11"
PYTHON_BIN="${PYTHON_BIN:-$PYTHON_BIN_DEFAULT}"
if ! command -v "${PYTHON_BIN}" >/dev/null 2>&1; then
  echo "âš ï¸  ${PYTHON_BIN} ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(${PYTHON_BIN_DEFAULT})ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤."
  PYTHON_BIN="${PYTHON_BIN_DEFAULT}"
fi
if ! command -v "${PYTHON_BIN}" >/dev/null 2>&1; then
  echo "âŒ ${PYTHON_BIN_DEFAULT} ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PYTHON_BIN í™˜ê²½ë³€ìˆ˜ë¡œ ë²„ì „ì„ ì§€ì •í•˜ì„¸ìš”."
  exit 1
fi

echo "ğŸ”§ í”„ë¡œì íŠ¸ ë£¨íŠ¸: ${PROJECT_ROOT}"
echo "ğŸ”§ ê°€ìƒí™˜ê²½ ê²½ë¡œ: ${VENV_DIR}"
echo "ğŸ”§ Python: ${PYTHON_BIN}"

# Git Submodule ì´ˆê¸°í™” í™•ì¸
if [ -f "${PROJECT_ROOT}/.gitmodules" ]; then
  echo "ğŸ“¦ Git Submodule í™•ì¸ ì¤‘..."
  
  # ì„œë¸Œëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
  SUBMODULE_EMPTY=false
  while IFS= read -r line; do
    if [[ "$line" =~ path[[:space:]]*=[[:space:]]*(.+) ]]; then
      SUBMODULE_PATH="${BASH_REMATCH[1]}"
      SUBMODULE_PATH="${SUBMODULE_PATH// /}"  # ê³µë°± ì œê±°
      
      # ì„œë¸Œëª¨ë“ˆ ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
      if [ -d "${PROJECT_ROOT}/${SUBMODULE_PATH}" ] && [ -z "$(ls -A "${PROJECT_ROOT}/${SUBMODULE_PATH}" 2>/dev/null)" ]; then
        SUBMODULE_EMPTY=true
        break
      fi
    fi
  done < "${PROJECT_ROOT}/.gitmodules"
  
  # ì„œë¸Œëª¨ë“ˆì´ ë¹„ì–´ìˆìœ¼ë©´ ì´ˆê¸°í™”
  if [ "$SUBMODULE_EMPTY" = true ]; then
    echo "ğŸ“¦ Git Submodule ì´ˆê¸°í™” ì¤‘..."
    git -C "${PROJECT_ROOT}" submodule update --init --recursive
    echo "âœ… Git Submodule ì´ˆê¸°í™” ì™„ë£Œ"
  else
    echo "âœ… Git Submodule ì´ë¯¸ ì´ˆê¸°í™”ë¨"
  fi
fi

NEED_CREATE=true
if [ -d "${VENV_DIR}" ] && [ -x "${VENV_DIR}/bin/python" ]; then
  VENV_PY_VERSION="$("${VENV_DIR}/bin/python" -c 'import sys; print(f"{sys.version_info.major}.{sys.version_info.minor}")')"
  if [ "${VENV_PY_VERSION}" = "3.11" ]; then
    NEED_CREATE=false
  else
    echo "âš ï¸  ê¸°ì¡´ ê°€ìƒí™˜ê²½ì€ Python ${VENV_PY_VERSION} ë²„ì „ì…ë‹ˆë‹¤. ì¬ìƒì„±í•©ë‹ˆë‹¤."
    rm -rf "${VENV_DIR}"
  fi
fi

if [ "${NEED_CREATE}" = true ]; then
  echo "ğŸ“¦ ê°€ìƒí™˜ê²½ ìƒì„± ì¤‘ (Python 3.11)..."
  "${PYTHON_BIN}" -m venv "${VENV_DIR}"
fi

echo "ğŸ“¦ ê°€ìƒí™˜ê²½ í™œì„±í™”..."
# shellcheck source=/dev/null
source "${VENV_DIR}/bin/activate"

echo "ğŸ“¦ pip ì—…ê·¸ë ˆì´ë“œ..."
pip install --upgrade pip setuptools wheel

if [ ! -f "${REQUIREMENTS_FILE}" ]; then
  echo "âŒ ${REQUIREMENTS_FILE} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
  exit 1
fi

echo "ğŸ“¦ ì„œë²„ í†µí•© ì˜ì¡´ì„± ì„¤ì¹˜..."
pip install -r "${REQUIREMENTS_FILE}"

echo "ğŸ“¦ ë¡œì»¬ ëª¨ë“ˆ ì„¤ì¹˜ (editable mode)..."
pip install -e "${PROJECT_ROOT}/cap1_RAG_module"
pip install -e "${PROJECT_ROOT}/cap1_QA_module"
pip install -e "${PROJECT_ROOT}/cap1_openalex_module"
pip install -e "${PROJECT_ROOT}/cap1_wiki_module"
pip install -e "${PROJECT_ROOT}/cap1_youtube_module"
pip install -e "${PROJECT_ROOT}/cap1_google_module"

if [ ! -f "${PROJECT_ROOT}/.env" ]; then
  if [ -f "${ENV_EXAMPLE}" ]; then
    echo "ğŸ“„ .env íŒŒì¼ì´ ì—†ì–´ .env.exampleì„ ë³µì‚¬í•©ë‹ˆë‹¤."
    cp "${ENV_EXAMPLE}" "${PROJECT_ROOT}/.env"
  fi
fi

ACTIVATE_SCRIPT="${VENV_DIR}/bin/activate"
ENV_MARKER="# >>> project .env >>>"
if [ -f "${PROJECT_ROOT}/.env" ] && [ -f "${ACTIVATE_SCRIPT}" ] && ! grep -q "${ENV_MARKER}" "${ACTIVATE_SCRIPT}"; then
  cat <<'EOF' >> "${ACTIVATE_SCRIPT}"
# >>> project .env >>>
if [ -f "$VIRTUAL_ENV/../.env" ]; then
  _OLD_IFS="$IFS"
  set -a
  . "$VIRTUAL_ENV/../.env"
  set +a
  IFS="$_OLD_IFS"
fi
# <<< project .env <<<
EOF
fi

cat <<'EOF'
âœ… ì„¤ì¹˜ ì™„ë£Œ!

1) í™˜ê²½ ë³€ìˆ˜ í™•ì¸/ìˆ˜ì •:   vi .env
2) ê°€ìƒí™˜ê²½ í™œì„±í™”:       source .venv/bin/activate
3) ì„œë²„ ì‹¤í–‰:             uvicorn server.main:app --reload

í•„ìš” ì‹œ PYTHON_BIN=python3.11 ./setup.sh ì²˜ëŸ¼ Python ë²„ì „ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
EOF
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
VENV_DIR="${ROOT_DIR}/.venv"

if [ -f "${VENV_DIR}/bin/activate" ]; then
  # shellcheck disable=SC1090
  source "${VENV_DIR}/bin/activate"
else
  echo "[!] .venv/bin/activate ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ./setup.sh ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”." >&2
  exit 1
fi

HOST=${HOST:-127.0.0.1}
PORT=${PORT:-8000}
UVICORN_BIN="${VENV_DIR}/bin/uvicorn"
APP="server.main:app"
LOG_FILE="${ROOT_DIR}/tmp_uvicorn.log"

started_server=false

if ! pgrep -f "${APP}" >/dev/null 2>&1; then
  echo "[*] FastAPI ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤..."
  "${UVICORN_BIN}" "${APP}" --host "${HOST}" --port "${PORT}" >/tmp/tmp_uvicorn.log 2>&1 &
  SERVER_PID=$!
  started_server=true
  sleep 3
else
  echo "[*] ê¸°ì¡´ì— ì‹¤í–‰ ì¤‘ì¸ FastAPI ì„œë²„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
fi

cleanup() {
  if [ "${started_server}" = true ] && kill -0 "${SERVER_PID}" >/dev/null 2>&1; then
    echo
    echo "[*] FastAPI ì„œë²„ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤ (PID: ${SERVER_PID})..."
    kill "${SERVER_PID}" >/dev/null 2>&1 || true
  fi
}
trap cleanup EXIT

echo "[*] Health ì²´í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤..."
curl -s "http://$HOST:$PORT/health" || { echo "[!] Health ì²´í¬ ì‹¤íŒ¨" >&2; exit 1; }
echo

LECTURE_ID="mp101"

echo "[*] ë©€í‹°í”„ë¡œì„¸ì‹± 30ì´ˆ ë¶„ëŸ‰ í…ìŠ¤íŠ¸ 3ê°œë¥¼ ì—…ì„œíŠ¸í•©ë‹ˆë‹¤..."
curl -sS -X POST "http://$HOST:$PORT/rag/text-upsert" \
     -H "Content-Type: application/json" \
     -d @- <<JSON
{
  "lecture_id": "${LECTURE_ID}",
  "items": [
    {
      "text": "ë©€í‹°í”„ë¡œì„¸ì‹±ì€ ì—¬ëŸ¬ ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ CPU ì½”ì–´ë¥¼ ì™„ì „íˆ í™œìš©í•˜ëŠ” ë³‘ë ¬ ì²˜ë¦¬ ë°©ì‹ì…ë‹ˆë‹¤. 30ì´ˆì§œë¦¬ ë°ëª¨ì—ì„œëŠ” ê° í”„ë¡œì„¸ìŠ¤ê°€ ë…ë¦½ëœ ë©”ëª¨ë¦¬ ê³µê°„ì„ ì‚¬ìš©í•˜ë©° ìƒí˜¸ ê°„ì„­ ì—†ì´ íƒœìŠ¤í¬ë¥¼ ë¶„ë‹´í•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
      "metadata": {"duration":"30s","topic":"ë©€í‹°í”„ë¡œì„¸ì‹± ê¸°ì´ˆ"}
    },
    {
      "text": "ë‘ ë²ˆì§¸ ë°ëª¨ì—ì„œëŠ” ì´ë¯¸ì§€ ë³€í™˜ ì‘ì—…ì„ ë„¤ ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤. ê° í”„ë¡œì„¸ìŠ¤ê°€ ë³„ë„ì˜ íì—ì„œ ì—…ë¬´ë¥¼ ê°€ì ¸ê°€ê³ , ì™„ë£Œëœ ê²°ê³¼ë¥¼ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ìˆ˜ì§‘í•´ í•©ì¹˜ëŠ” ê³¼ì •ê¹Œì§€ 30ì´ˆ ì•ˆì— ì‹œì—°ë©ë‹ˆë‹¤.",
      "metadata": {"duration":"30s","topic":"ë³‘ë ¬ ì´ë¯¸ì§€ ì²˜ë¦¬"}
    },
    {
      "text": "ì„¸ ë²ˆì§¸ ì˜ˆì‹œì—ì„œëŠ” ê³¼í•™ ê³„ì‚°ì„ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ë¡œ ë¶„í• í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ ê°„ í†µì‹ ì€ íŒŒì´í”„ë¥¼ í†µí•´ ì´ë¤„ì§€ê³ , ê³„ì‚° ì™„ë£Œ í›„ ê²°ê³¼ë¥¼ ë³‘í•©í•˜ëŠ” íë¦„ì„ 30ì´ˆ ë™ì•ˆ ìˆœì°¨ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.",
      "metadata": {"duration":"30s","topic":"ê³¼í•™ ê³„ì‚° ë¶„í• "}
    }
  ]
}
JSON
echo

SECTION_SUMMARY="í•˜ì´í¼ ìŠ¤ë ˆë”©ì€ í•˜ë‚˜ì˜ ë¬¼ë¦¬ ì½”ì–´ê°€ ë‘ ê°œì˜ ëª…ë ¹ íë¦„ì„ ë²ˆê°ˆì•„ ì‹¤í–‰í•˜ì—¬ ìì›ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì“°ë„ë¡ ì„¤ê³„ëœ ë™ì‹œ ë©€í‹°ìŠ¤ë ˆë”© ê¸°ìˆ ì´ë‹¤."

echo "[*] QA ìŠ¤íŠ¸ë¦¼ì„ ìš”ì²­í•©ë‹ˆë‹¤ (í•˜ì´í¼ ìŠ¤ë ˆë”©, ì´ì „ ì§ˆë¬¸ í¬í•¨)..."
curl --no-buffer --max-time 30 -sS -N -X POST "http://$HOST:$PORT/qa/generate" \
     -H "Content-Type: application/json" \
     -d @- <<JSON
{
  "lecture_id": "${LECTURE_ID}",
  "section_id": 1,
  "section_summary": "${SECTION_SUMMARY}",
  "subject": "ìš´ì˜ì²´ì œ",
  "previous_qa": [
    {
      "type": "ê°œë…",
      "question": "ë©€í‹°í”„ë¡œì„¸ì‹±ê³¼ ë©€í‹°ìŠ¤ë ˆë”©ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
      "answer": "ë©€í‹°í”„ë¡œì„¸ì‹±ì€ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ê°€ ë…ë¦½ëœ ë©”ëª¨ë¦¬ ê³µê°„ì—ì„œ ì‹¤í–‰ë˜ëŠ” ë°˜ë©´, ë©€í‹°ìŠ¤ë ˆë”©ì€ í•˜ë‚˜ì˜ í”„ë¡œì„¸ìŠ¤ ë‚´ì—ì„œ ì—¬ëŸ¬ ìŠ¤ë ˆë“œê°€ ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•˜ë©° ì‹¤í–‰ë©ë‹ˆë‹¤."
    }
  ]
}
JSON
echo

echo "[*] REC ìŠ¤íŠ¸ë¦¼ì„ ìš”ì²­í•©ë‹ˆë‹¤ (í•˜ì´í¼ ìŠ¤ë ˆë”©)..."
curl --no-buffer --max-time 30 -sS -N -X POST "http://$HOST:$PORT/rec/recommend" \
     -H "Content-Type: application/json" \
     -d @- <<JSON
{
  "lecture_id": "${LECTURE_ID}",
  "section_id": 1,
  "section_summary": "${SECTION_SUMMARY}",
  "previous_summaries": [],
  "yt_exclude": [],
  "wiki_exclude": [],
  "paper_exclude": []
}
JSON
echo

echo "[*] í…ŒìŠ¤íŠ¸ ì™„ë£Œ"
from __future__ import annotations

import asyncio
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pytest
from fastapi import FastAPI
from httpx import AsyncClient, ASGITransport

ROOT_DIR = Path(__file__).resolve().parents[1]
if str(ROOT_DIR) not in sys.path:
    sys.path.append(str(ROOT_DIR))

from server.app import create_app
from server.config import AppSettings


@dataclass
class SimpleModel:
    """model_dumpë¥¼ ì œê³µí•˜ëŠ” ë‹¨ìˆœ ë² ì´ìŠ¤"""
    
    def model_dump(self) -> Dict[str, Any]:
        from dataclasses import asdict
        return asdict(self)


@dataclass
class PaperInfo(SimpleModel):
    url: str
    title: str
    abstract: str
    year: Optional[int] = None
    cited_by_count: int = 0
    authors: List[str] = field(default_factory=list)


@dataclass
class OpenAlexResponse(SimpleModel):
    lecture_id: str
    section_id: int
    paper_info: PaperInfo
    reason: str
    score: float


@dataclass
class WikiPageInfo(SimpleModel):
    url: str
    title: str
    extract: str
    lang: str
    page_id: int


@dataclass
class WikiResponse(SimpleModel):
    lecture_id: str
    section_id: int
    page_info: WikiPageInfo
    reason: str
    score: float


@dataclass
class YouTubeVideoInfo(SimpleModel):
    url: str
    title: str
    extract: str
    lang: str


@dataclass
class YouTubeResponse(SimpleModel):
    lecture_id: str
    section_id: int
    video_info: YouTubeVideoInfo
    reason: str
    score: float


@dataclass
class StubRetrievedChunk:
    """ë‹¨ìˆœ RAG ì²­í¬"""
    id: str
    text: str
    score: float
    metadata: Dict[str, Any]


class StubRAGService:
    """RAG ì„œë¹„ìŠ¤ ìŠ¤í…"""
    
    def __init__(self):
        self.pdf_calls: List[Dict[str, Any]] = []
        self.text_calls: List[Dict[str, Any]] = []
        self.retrieve_calls: List[Dict[str, Any]] = []
        self.retrieve_result: List[StubRetrievedChunk] = []
        self.upsert_pdf_result: Dict[str, Any] = {"count": 3, "status": "ok"}
        self.upsert_text_result: Dict[str, Any] = {"count": 2, "status": "ok"}
    
    def upsert_pdf(self, collection_id: str, pdf_path: str, base_metadata: Optional[Dict[str, Any]] = None):
        self.pdf_calls.append(
            {
                "collection_id": collection_id,
                "pdf_path": pdf_path,
                "base_metadata": base_metadata,
            }
        )
        return self.upsert_pdf_result
    
    def upsert_text(self, collection_id: str, items: List[Any]):
        self.text_calls.append(
            {
                "collection_id": collection_id,
                "items": items,
            }
        )
        return self.upsert_text_result
    
    def retrieve(self, collection_id: str, query: str, top_k: int, filters=None):
        self.retrieve_calls.append(
            {
                "collection_id": collection_id,
                "query": query,
                "top_k": top_k,
                "filters": filters,
            }
        )
        return list(self.retrieve_result)


class StubQAService:
    """QA ì„œë¹„ìŠ¤ ìŠ¤í…"""
    
    def __init__(self):
        self.events: List[Tuple[str, str, Dict[str, Any]]] = []
        self.request_payloads: List[Any] = []
    
    async def stream_questions(self, request):
        self.request_payloads.append(request)
        for event in self.events:
            if isinstance(event, Exception):
                raise event
            event_type, q_type, payload = event
            # ì‘ì€ ì§€ì—°ìœ¼ë¡œ ìˆœì„œë¥¼ ì œì–´
            await asyncio.sleep(payload.get("_delay", 0))
            clean_payload = dict(payload)
            clean_payload.pop("_delay", None)
            yield event_type, q_type, clean_payload


class StubOpenAlexService:
    """OpenAlex ì„œë¹„ìŠ¤ ìŠ¤í…"""
    
    def __init__(self):
        self.responses: List[OpenAlexResponse] = []
        self.delay: float = 0.0
        self.requests: List[OpenAlexResponse] = []
    
    async def recommend_papers(self, request):
        self.requests.append(request)
        if self.delay:
            await asyncio.sleep(self.delay)
        return list(self.responses)


class StubWikiService:
    """Wiki ì„œë¹„ìŠ¤ ìŠ¤í…"""
    
    def __init__(self):
        self.responses: List[WikiResponse] = []
        self.delay: float = 0.0
        self.requests: List[WikiResponse] = []
    
    async def recommend_pages(self, request):
        self.requests.append(request)
        if self.delay:
            await asyncio.sleep(self.delay)
        return list(self.responses)


class StubYouTubeService:
    """YouTube ì„œë¹„ìŠ¤ ìŠ¤í…"""
    
    def __init__(self):
        self.responses: List[YouTubeResponse] = []
        self.delay: float = 0.0
        self.requests: List[Any] = []
    
    async def recommend_videos(self, request):
        self.requests.append(request)
        if self.delay:
            await asyncio.sleep(self.delay)
        return list(self.responses)


@dataclass
class TestContext:
    """í…ŒìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©í•  ìŠ¤í… ëª¨ìŒ"""
    
    rag: StubRAGService = field(default_factory=StubRAGService)
    qa: StubQAService = field(default_factory=StubQAService)
    openalex: StubOpenAlexService = field(default_factory=StubOpenAlexService)
    wiki: StubWikiService = field(default_factory=StubWikiService)
    youtube: StubYouTubeService = field(default_factory=StubYouTubeService)
    settings: AppSettings = field(default_factory=AppSettings)


@pytest.fixture
def test_context() -> TestContext:
    ctx = TestContext()
    # ê¸°ë³¸ ì„¤ì • ì¡°ì •
    ctx.settings.rag.collection_prefix = "test"
    ctx.settings.rag.qa_retrieve_top_k = 2
    ctx.settings.rag.rec_retrieve_top_k = 3
    ctx.settings.qa.language = "ko"
    ctx.settings.qa.question_types = ["ì‘ìš©", "ë¹„êµ", "ì‹¬í™”"]
    ctx.settings.qa.qa_top_k = 3
    ctx.settings.rec.openalex.top_k = 2
    ctx.settings.rec.openalex.verify = True
    ctx.settings.rec.openalex.min_score = 3.0
    ctx.settings.rec.openalex.sort_by = "hybrid"
    ctx.settings.rec.wiki.top_k = 2
    ctx.settings.rec.wiki.verify = False
    ctx.settings.rec.wiki.wiki_lang = "en"
    ctx.settings.rec.wiki.min_score = 3.0
    ctx.settings.rec.youtube.top_k = 2
    ctx.settings.rec.youtube.verify = True
    ctx.settings.rec.youtube.yt_lang = "en"
    ctx.settings.rec.youtube.min_score = 3.0
    return ctx


@pytest.fixture
def fastapi_app(test_context: TestContext) -> FastAPI:
    return create_app(
        test_context.settings,
        rag_service=test_context.rag,
        qa_service=test_context.qa,
        openalex_service=test_context.openalex,
        wiki_service=test_context.wiki,
        youtube_service=test_context.youtube,
    )


@pytest.fixture
async def async_client(fastapi_app: FastAPI):
    transport = ASGITransport(app=fastapi_app)
    async with AsyncClient(transport=transport, base_url="http://testserver") as client:
        yield client
from __future__ import annotations

import json
from typing import List, Tuple

import pytest

from tests.conftest import StubRetrievedChunk


async def collect_sse(response) -> List[Tuple[str, dict]]:
    events: List[Tuple[str, dict]] = []
    current = None
    async for line in response.aiter_lines():
        if not line:
            continue
        if line.startswith("event:"):
            current = line.split(":", 1)[1].strip()
        elif line.startswith("data:"):
            data = json.loads(line.split(":", 1)[1].strip())
            events.append((current, data))
    return events


def _prepare_chunks() -> List[StubRetrievedChunk]:
    return [
        StubRetrievedChunk(id="c1", text="ìŠ¤íƒì€ LIFO êµ¬ì¡°ë‹¤.", score=0.91, metadata={"section_id": "1"}),
        StubRetrievedChunk(id="c2", text="íëŠ” FIFO êµ¬ì¡°ì™€ ë¹„êµëœë‹¤.", score=0.82, metadata={"section_id": "2"}),
    ]


@pytest.mark.anyio
async def test_qa_stream_orders_events(async_client, test_context):
    test_context.rag.retrieve_result = _prepare_chunks()
    test_context.qa.events = [
        ("qa", "ì‘ìš©", {"type": "ì‘ìš©", "question": "ì‘ìš©Q", "answer": "ì‘ìš©A", "_delay": 0.05}),
        ("qa", "ë¹„êµ", {"type": "ë¹„êµ", "question": "ë¹„êµQ", "answer": "ë¹„êµA", "_delay": 0.0}),
        ("qa", "ì‹¬í™”", {"type": "ì‹¬í™”", "question": "ì‹¬í™”Q", "answer": "ì‹¬í™”A", "_delay": 0.02}),
    ]
    payload = {"lecture_id": "lec", "section_id": 1, "section_summary": "ìŠ¤íƒê³¼ íì˜ ì°¨ì´ë¥¼ ìì„¸íˆ ì„¤ëª…í•œë‹¤."}
    async with async_client.stream("POST", "/qa/generate", json=payload) as response:
        assert response.status_code == 200
        events = await collect_sse(response)
    
    # ì²« ì´ë²¤íŠ¸ëŠ” ì»¨í…ìŠ¤íŠ¸ ì •ë³´
    assert events[0][0] == "qa_context"
    partial = [evt for evt in events if evt[0] == "qa_partial"]
    order = [evt[1]["qa"]["type"] for evt in partial]
    assert order == ["ë¹„êµ", "ì‹¬í™”", "ì‘ìš©"]
    assert test_context.rag.retrieve_calls[-1]["top_k"] == test_context.settings.rag.qa_retrieve_top_k


@pytest.mark.anyio
async def test_qa_stream_emits_error(async_client, test_context):
    test_context.rag.retrieve_result = _prepare_chunks()
    test_context.qa.events = [
        ("qa", "ì‘ìš©", {"type": "ì‘ìš©", "question": "Q1", "answer": "A1"}),
        ("error", "ë¹„êµ", {"error": "LLM ì‹¤íŒ¨"}),
    ]
    payload = {"lecture_id": "lec", "section_id": 2, "section_summary": "ìë£Œêµ¬ì¡° ê°„ì˜ ë¹„êµì™€ ì°¨ì´ë¥¼ ì„œìˆ í•œë‹¤."}
    async with async_client.stream("POST", "/qa/generate", json=payload) as response:
        assert response.status_code == 200
        events = await collect_sse(response)
    
    error_events = [evt for evt in events if evt[0] == "qa_error"]
    assert error_events
    assert error_events[0][1]["type"] == "ë¹„êµ"


@pytest.mark.anyio
async def test_qa_stream_reports_completion(async_client, test_context):
    test_context.rag.retrieve_result = _prepare_chunks()
    test_context.qa.events = [
        ("qa", "ì‘ìš©", {"type": "ì‘ìš©", "question": "Q1", "answer": "A1"}),
        ("qa", "ë¹„êµ", {"type": "ë¹„êµ", "question": "Q2", "answer": "A2"}),
        ("qa", "ì‹¬í™”", {"type": "ì‹¬í™”", "question": "Q3", "answer": "A3"}),
    ]
    payload = {"lecture_id": "lec", "section_id": 3, "section_summary": "ì‹¬í™” ë‚´ìš©ì„ ë‹¤ë£¨ë©° ì‘ìš© ì˜ˆì‹œë¥¼ ì œì‹œí•œë‹¤."}
    async with async_client.stream("POST", "/qa/generate", json=payload) as response:
        events = await collect_sse(response)
    
    complete = [evt for evt in events if evt[0] == "qa_complete"]
    assert complete
    assert complete[0][1]["total"] == 3


@pytest.mark.anyio
async def test_qa_stream_builds_request_with_context(async_client, test_context):
    test_context.rag.retrieve_result = _prepare_chunks()
    test_context.qa.events = [
        ("qa", "ì‘ìš©", {"type": "ì‘ìš©", "question": "Q1", "answer": "A1"}),
    ]
    payload = {
        "lecture_id": "lec",
        "section_id": 4,
        "section_summary": "ìš”ì•½ ë‚´ìš©ì„ ì¶©ë¶„íˆ ì œê³µí•˜ì—¬ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•œë‹¤.",
    }
    await async_client.post("/qa/generate", json=payload)
    stored = test_context.qa.request_payloads[-1]
    assert stored.qa_count == len(test_context.settings.qa.question_types)
    assert len(stored.rag_context.chunks) == len(test_context.rag.retrieve_result)


@pytest.mark.anyio
async def test_qa_stream_fails_when_rag_raises(async_client, test_context, monkeypatch):
    def raise_error(*args, **kwargs):
        raise RuntimeError("no collection")
    
    monkeypatch.setattr(test_context.rag, "retrieve", raise_error)
    payload = {
        "lecture_id": "lec",
        "section_id": 5,
        "section_summary": "ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¬ë‹¤.",
    }
    response = await async_client.post("/qa/generate", json=payload)
    assert response.status_code == 400
    assert "RAG" in response.json()["detail"]
#!/usr/bin/env bash
set -euo pipefail

# Docker ì»¨í…Œì´ë„ˆë¡œ ì‹¤í–‰ ì¤‘ì¸ ì„œë²„ í…ŒìŠ¤íŠ¸ìš© ìŠ¤í¬ë¦½íŠ¸
HOST=${HOST:-localhost}
PORT=${PORT:-8003}

echo "[*] Health ì²´í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤..."
curl -s "http://$HOST:$PORT/health" || { echo "[!] Health ì²´í¬ ì‹¤íŒ¨" >&2; exit 1; }
echo

LECTURE_ID="mp101"

echo "[*] ë©€í‹°í”„ë¡œì„¸ì‹± 30ì´ˆ ë¶„ëŸ‰ í…ìŠ¤íŠ¸ 3ê°œë¥¼ ì—…ì„œíŠ¸í•©ë‹ˆë‹¤..."
curl -sS -X POST "http://$HOST:$PORT/rag/text-upsert" \
     -H "Content-Type: application/json" \
     -d @- <<JSON
{
  "lecture_id": "${LECTURE_ID}",
  "items": [
    {
      "text": "ë©€í‹°í”„ë¡œì„¸ì‹±ì€ ì—¬ëŸ¬ ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ CPU ì½”ì–´ë¥¼ ì™„ì „íˆ í™œìš©í•˜ëŠ” ë³‘ë ¬ ì²˜ë¦¬ ë°©ì‹ì…ë‹ˆë‹¤. 30ì´ˆì§œë¦¬ ë°ëª¨ì—ì„œëŠ” ê° í”„ë¡œì„¸ìŠ¤ê°€ ë…ë¦½ëœ ë©”ëª¨ë¦¬ ê³µê°„ì„ ì‚¬ìš©í•˜ë©° ìƒí˜¸ ê°„ì„­ ì—†ì´ íƒœìŠ¤í¬ë¥¼ ë¶„ë‹´í•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
      "metadata": {"duration":"30s","topic":"ë©€í‹°í”„ë¡œì„¸ì‹± ê¸°ì´ˆ"}
    },
    {
      "text": "ë‘ ë²ˆì§¸ ë°ëª¨ì—ì„œëŠ” ì´ë¯¸ì§€ ë³€í™˜ ì‘ì—…ì„ ë„¤ ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤. ê° í”„ë¡œì„¸ìŠ¤ê°€ ë³„ë„ì˜ íì—ì„œ ì—…ë¬´ë¥¼ ê°€ì ¸ê°€ê³ , ì™„ë£Œëœ ê²°ê³¼ë¥¼ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ìˆ˜ì§‘í•´ í•©ì¹˜ëŠ” ê³¼ì •ê¹Œì§€ 30ì´ˆ ì•ˆì— ì‹œì—°ë©ë‹ˆë‹¤.",
      "metadata": {"duration":"30s","topic":"ë³‘ë ¬ ì´ë¯¸ì§€ ì²˜ë¦¬"}
    },
    {
      "text": "ì„¸ ë²ˆì§¸ ì˜ˆì‹œì—ì„œëŠ” ê³¼í•™ ê³„ì‚°ì„ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ë¡œ ë¶„í• í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ ê°„ í†µì‹ ì€ íŒŒì´í”„ë¥¼ í†µí•´ ì´ë¤„ì§€ê³ , ê³„ì‚° ì™„ë£Œ í›„ ê²°ê³¼ë¥¼ ë³‘í•©í•˜ëŠ” íë¦„ì„ 30ì´ˆ ë™ì•ˆ ìˆœì°¨ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.",
      "metadata": {"duration":"30s","topic":"ê³¼í•™ ê³„ì‚° ë¶„í• "}
    }
  ]
}
JSON
echo

SECTION_SUMMARY="í•˜ì´í¼ ìŠ¤ë ˆë”©ì€ í•˜ë‚˜ì˜ ë¬¼ë¦¬ ì½”ì–´ê°€ ë‘ ê°œì˜ ëª…ë ¹ íë¦„ì„ ë²ˆê°ˆì•„ ì‹¤í–‰í•˜ì—¬ ìì›ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì“°ë„ë¡ ì„¤ê³„ëœ ë™ì‹œ ë©€í‹°ìŠ¤ë ˆë”© ê¸°ìˆ ì´ë‹¤."

echo "[*] QA ìŠ¤íŠ¸ë¦¼ì„ ìš”ì²­í•©ë‹ˆë‹¤ (í•˜ì´í¼ ìŠ¤ë ˆë”©, ì´ì „ ì§ˆë¬¸ í¬í•¨)..."
curl --no-buffer --max-time 30 -sS -N -X POST "http://$HOST:$PORT/qa/generate" \
     -H "Content-Type: application/json" \
     -d @- <<JSON
{
  "lecture_id": "${LECTURE_ID}",
  "section_id": 1,
  "section_summary": "${SECTION_SUMMARY}",
  "subject": "ìš´ì˜ì²´ì œ",
  "previous_qa": [
    {
      "type": "ê°œë…",
      "question": "ë©€í‹°í”„ë¡œì„¸ì‹±ê³¼ ë©€í‹°ìŠ¤ë ˆë”©ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
      "answer": "ë©€í‹°í”„ë¡œì„¸ì‹±ì€ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ê°€ ë…ë¦½ëœ ë©”ëª¨ë¦¬ ê³µê°„ì—ì„œ ì‹¤í–‰ë˜ëŠ” ë°˜ë©´, ë©€í‹°ìŠ¤ë ˆë”©ì€ í•˜ë‚˜ì˜ í”„ë¡œì„¸ìŠ¤ ë‚´ì—ì„œ ì—¬ëŸ¬ ìŠ¤ë ˆë“œê°€ ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•˜ë©° ì‹¤í–‰ë©ë‹ˆë‹¤."
    }
  ]
}
JSON
echo

echo "[*] REC ìŠ¤íŠ¸ë¦¼ì„ ìš”ì²­í•©ë‹ˆë‹¤ (í•˜ì´í¼ ìŠ¤ë ˆë”©)..."
curl --no-buffer --max-time 30 -sS -N -X POST "http://$HOST:$PORT/rec/recommend" \
     -H "Content-Type: application/json" \
     -d @- <<JSON
{
  "lecture_id": "${LECTURE_ID}",
  "section_id": 1,
  "section_summary": "${SECTION_SUMMARY}",
  "previous_summaries": [],
  "yt_exclude": [],
  "wiki_exclude": [],
  "paper_exclude": []
}
JSON
echo

echo "[*] í…ŒìŠ¤íŠ¸ ì™„ë£Œ"
from __future__ import annotations

import json
from pathlib import Path

import pytest


@pytest.mark.anyio
async def test_pdf_upsert_success(async_client, test_context):
    pdf_path = Path("cap1_RAG_module/test_data/simple_test.pdf")
    payload = pdf_path.read_bytes()
    response = await async_client.post(
        "/rag/pdf-upsert",
        files={"file": ("simple_test.pdf", payload, "application/pdf")},
        data={"lecture_id": "lecture123", "base_metadata": json.dumps({"subject": "cs"})},
    )
    assert response.status_code == 200
    assert test_context.rag.pdf_calls
    call = test_context.rag.pdf_calls[-1]
    assert call["collection_id"] == "test_lecture123"
    assert call["base_metadata"] == {"subject": "cs"}
    assert Path(call["pdf_path"]).exists()


@pytest.mark.anyio
async def test_pdf_upsert_rejects_non_pdf(async_client):
    response = await async_client.post(
        "/rag/pdf-upsert",
        files={"file": ("notes.txt", b"text", "text/plain")},
        data={"lecture_id": "lecture123"},
    )
    assert response.status_code == 400
    assert "PDF" in response.json()["detail"]


@pytest.mark.anyio
async def test_pdf_upsert_blocks_empty_file(async_client):
    response = await async_client.post(
        "/rag/pdf-upsert",
        files={"file": ("empty.pdf", b"", "application/pdf")},
        data={"lecture_id": "lecture123"},
    )
    assert response.status_code == 400
    assert "ë¹ˆ íŒŒì¼" in response.json()["detail"]


@pytest.mark.anyio
async def test_pdf_upsert_invalid_metadata(async_client):
    pdf_path = Path("cap1_RAG_module/test_data/simple_test.pdf")
    response = await async_client.post(
        "/rag/pdf-upsert",
        files={"file": ("simple_test.pdf", pdf_path.read_bytes(), "application/pdf")},
        data={"lecture_id": "lecture123", "base_metadata": "{not-json}"},
    )
    assert response.status_code == 400
    assert "JSON" in response.json()["detail"]


@pytest.mark.anyio
async def test_pdf_upsert_blank_lecture_id(async_client):
    pdf_path = Path("cap1_RAG_module/test_data/simple_test.pdf")
    response = await async_client.post(
        "/rag/pdf-upsert",
        files={"file": ("simple_test.pdf", pdf_path.read_bytes(), "application/pdf")},
        data={"lecture_id": "   "},
    )
    assert response.status_code == 400
    assert "lecture_id" in response.json()["detail"]
from __future__ import annotations

import pytest


@pytest.mark.anyio
async def test_text_upsert_success(async_client, test_context):
    payload = {
        "lecture_id": "lec-1",
        "items": [
            {"text": "ìš”ì•½ë¬¸ 1", "section_id": "1"},
            {"text": "ìš”ì•½ë¬¸ 2", "metadata": {"page": 2}},
        ],
    }
    response = await async_client.post("/rag/text-upsert", json=payload)
    assert response.status_code == 200
    assert test_context.rag.text_calls
    call = test_context.rag.text_calls[-1]
    assert call["collection_id"] == "test_lec-1"
    assert len(call["items"]) == 2


@pytest.mark.anyio
async def test_text_upsert_rejects_empty_items(async_client):
    payload = {"lecture_id": "lec-1", "items": []}
    response = await async_client.post("/rag/text-upsert", json=payload)
    assert response.status_code == 422


@pytest.mark.anyio
async def test_text_upsert_requires_lecture(async_client):
    payload = {"lecture_id": "   ", "items": [{"text": "data"}]}
    response = await async_client.post("/rag/text-upsert", json=payload)
    assert response.status_code == 422


@pytest.mark.anyio
async def test_text_upsert_preserves_metadata(async_client, test_context):
    payload = {
        "lecture_id": "lec-meta",
        "items": [
            {"text": "ë‚´ìš©", "metadata": {"tag": "core"}},
        ],
    }
    await async_client.post("/rag/text-upsert", json=payload)
    call = test_context.rag.text_calls[-1]
    item = call["items"][0]
    assert item.metadata == {"tag": "core"}


@pytest.mark.anyio
async def test_text_upsert_returns_stub_result(async_client, test_context):
    payload = {"lecture_id": "lec-result", "items": [{"text": "ë‚´ìš©"}]}
    response = await async_client.post("/rag/text-upsert", json=payload)
    assert response.status_code == 200
    assert response.json()["result"] == test_context.rag.upsert_text_result
from __future__ import annotations

import json
from typing import List, Tuple

import pytest

from tests.conftest import (
    OpenAlexResponse,
    PaperInfo,
    StubRetrievedChunk,
    WikiPageInfo,
    WikiResponse,
    YouTubeResponse,
    YouTubeVideoInfo,
)


async def collect_sse(response) -> List[Tuple[str, dict]]:
    events: List[Tuple[str, dict]] = []
    current = None
    async for line in response.aiter_lines():
        if not line:
            continue
        if line.startswith("event:"):
            current = line.split(":", 1)[1].strip()
        elif line.startswith("data:"):
            data = json.loads(line.split(":", 1)[1].strip())
            events.append((current, data))
    return events


def prepare_chunks():
    return [
        StubRetrievedChunk(id="c1", text="ì•Œê³ ë¦¬ì¦˜ ë¶„ì„ ë‚´ìš©", score=0.9, metadata={"section_id": "1"}),
        StubRetrievedChunk(id="c2", text="ìë£Œ êµ¬ì¡° ê´€ë ¨ ì„¤ëª…", score=0.85, metadata={"section_id": "2"}),
        StubRetrievedChunk(id="c3", text="ì„±ëŠ¥ ìµœì í™” ì‚¬ë¡€", score=0.8, metadata={"section_id": "3"}),
    ]


def build_default_responses():
    openalex_items = [
        OpenAlexResponse(
            lecture_id="lec",
            section_id=1,
            paper_info=PaperInfo(
                url="http://example.com/paper1",
                title="Paper 1",
                abstract="Abstract 1",
                year=2024,
                cited_by_count=100,
                authors=["Alice"],
            ),
            reason="LLM ê²€ì¦",
            score=7.5,
        )
    ]
    wiki_items = [
        WikiResponse(
            lecture_id="lec",
            section_id=1,
            page_info=WikiPageInfo(
                url="http://example.com/wiki1",
                title="Wiki 1",
                extract="Extract 1",
                lang="en",
                page_id=1,
            ),
            reason="ê´€ë ¨ í‚¤ì›Œë“œ",
            score=6.8,
        )
    ]
    yt_items = [
        YouTubeResponse(
            lecture_id="lec",
            section_id=1,
            video_info=YouTubeVideoInfo(
                url="http://youtu.be/1",
                title="Video 1",
                extract="Extract 1",
                lang="en",
            ),
            reason="LLM verification",
            score=7.2,
        )
    ]
    return openalex_items, wiki_items, yt_items


@pytest.mark.anyio
async def test_rec_stream_orders_partial_events(async_client, test_context):
    test_context.rag.retrieve_result = prepare_chunks()
    oa_items, wiki_items, yt_items = build_default_responses()
    test_context.openalex.responses = oa_items
    test_context.wiki.responses = wiki_items
    test_context.youtube.responses = yt_items
    test_context.openalex.delay = 0.03
    test_context.wiki.delay = 0.0
    test_context.youtube.delay = 0.01
    
    payload = {
        "lecture_id": "lec",
        "section_id": 1,
        "section_summary": "ìë£Œêµ¬ì¡°ì™€ ì•Œê³ ë¦¬ì¦˜ì„ ì„¤ëª…í•˜ëŠ” ê°•ì˜",
        "previous_summaries": [
            {"section_id": 1, "summary": "ìŠ¤íƒ ì†Œê°œ", "timestamp": 111},
            {"section_id": 2, "summary": "í ì†Œê°œ", "timestamp": 222},
        ],
        "yt_exclude": [],
        "wiki_exclude": [],
        "paper_exclude": [],
    }
    async with async_client.stream("POST", "/rec/recommend", json=payload) as response:
        assert response.status_code == 200
        events = await collect_sse(response)
    
    assert events[0][0] == "rec_context"
    partial_events = [evt for evt in events if evt[0] == "rec_partial"]
    sources = [evt[1]["source"] for evt in partial_events]
    assert sources == ["wiki", "youtube", "openalex"]
    assert test_context.rag.retrieve_calls[-1]["top_k"] == test_context.settings.rag.rec_retrieve_top_k


@pytest.mark.anyio
async def test_rec_stream_returns_complete(async_client, test_context):
    test_context.rag.retrieve_result = prepare_chunks()
    oa_items, wiki_items, yt_items = build_default_responses()
    test_context.openalex.responses = oa_items
    test_context.wiki.responses = wiki_items
    test_context.youtube.responses = yt_items
    
    payload = {
        "lecture_id": "lec",
        "section_id": 3,
        "section_summary": "ìµœì í™” ì „ëµì„ ì œì‹œí•˜ëŠ” ê°•ì˜",
        "previous_summaries": [],
        "yt_exclude": [],
        "wiki_exclude": [],
        "paper_exclude": [],
    }
    async with async_client.stream("POST", "/rec/recommend", json=payload) as response:
        events = await collect_sse(response)
    
    complete = [evt for evt in events if evt[0] == "rec_complete"]
    assert complete
    assert complete[0][1]["completed_sources"] == 3


@pytest.mark.anyio
async def test_rec_stream_handles_errors(async_client, test_context, monkeypatch):
    test_context.rag.retrieve_result = prepare_chunks()
    
    async def failing_openalex(request):
        raise RuntimeError("OpenAlex down")
    
    monkeypatch.setattr(test_context.openalex, "recommend_papers", failing_openalex)
    oa_items, wiki_items, yt_items = build_default_responses()
    test_context.wiki.responses = wiki_items
    test_context.youtube.responses = yt_items
    
    payload = {
        "lecture_id": "lec",
        "section_id": 4,
        "section_summary": "ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ ë¶„ì„",
        "previous_summaries": [],
        "yt_exclude": [],
        "wiki_exclude": [],
        "paper_exclude": [],
    }
    async with async_client.stream("POST", "/rec/recommend", json=payload) as response:
        events = await collect_sse(response)
    errors = [evt for evt in events if evt[0] == "rec_error"]
    assert errors
    assert errors[0][1]["source"] == "openalex"


@pytest.mark.anyio
async def test_rec_stream_applies_config(async_client, test_context):
    test_context.rag.retrieve_result = prepare_chunks()
    oa_items, wiki_items, yt_items = build_default_responses()
    test_context.openalex.responses = oa_items
    test_context.wiki.responses = wiki_items
    test_context.youtube.responses = yt_items
    
    payload = {
        "lecture_id": "lec",
        "section_id": 5,
        "section_summary": "ë°ì´í„° ì‹œê°í™”ì™€ ë¶„ì„ì„ ë‹¤ë£¬ë‹¤.",
        "previous_summaries": [
            {"section_id": 3, "summary": "ì´ì „ ìš”ì•½"},
        ],
        "yt_exclude": ["Old video"],
        "wiki_exclude": ["Old wiki"],
        "paper_exclude": ["old-id"],
    }
    await async_client.post("/rec/recommend", json=payload)
    oa_request = test_context.openalex.requests[-1]
    wiki_request = test_context.wiki.requests[-1]
    yt_request = test_context.youtube.requests[-1]
    
    assert oa_request.top_k == test_context.settings.rec.openalex.top_k
    assert oa_request.sort_by == "hybrid"
    assert wiki_request.verify_wiki == test_context.settings.rec.wiki.verify
    assert wiki_request.wiki_lang == "en"
    assert yt_request.verify_yt == test_context.settings.rec.youtube.verify
    assert yt_request.yt_lang == "en"
    assert oa_request.exclude_ids == ["old-id"]
    assert wiki_request.exclude_titles == ["Old wiki"]
    assert yt_request.exclude_titles == ["Old video"]


@pytest.mark.anyio
async def test_rec_stream_requires_rag(async_client, test_context, monkeypatch):
    def raise_error(*args, **kwargs):
        raise RuntimeError("rag failure")
    
    monkeypatch.setattr(test_context.rag, "retrieve", raise_error)
    payload = {
        "lecture_id": "lec",
        "section_id": 6,
        "section_summary": "ë„¤íŠ¸ì›Œí¬ ìµœì í™” ê¸°ë²•ì„ ë‹¤ë£¬ë‹¤.",
        "previous_summaries": [],
        "yt_exclude": [],
        "wiki_exclude": [],
        "paper_exclude": [],
    }
    response = await async_client.post("/rec/recommend", json=payload)
    assert response.status_code == 400
    assert "RAG" in response.json()["detail"]
#!/bin/bash

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# QA Generate RAG ê²€ì¦ í…ŒìŠ¤íŠ¸
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

BASE_URL="http://localhost:8000"
API_ENDPOINT="/qa/generate"

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ§ª QA Generate RAG ê²€ì¦ í…ŒìŠ¤íŠ¸"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ 1: CS101 - ì•Œê³ ë¦¬ì¦˜ ì‹œê°„ ë³µì¡ë„ ê´€ë ¨ QA ìƒì„±
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo "ğŸ“ í…ŒìŠ¤íŠ¸ 1: CS101 - ì•Œê³ ë¦¬ì¦˜ ì‹œê°„ ë³µì¡ë„"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ’¡ Summary: ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ìœ¨ì„±ì„ ë¶„ì„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ í•™ìŠµí•©ë‹ˆë‹¤. Big-O í‘œê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ ë³µì¡ë„ë¥¼ í‘œí˜„í•˜ë©°, O(1), O(log n), O(n), O(n log n), O(n^2) ë“± ë‹¤ì–‘í•œ ë³µì¡ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤."
echo "ğŸ¯ ì˜ˆìƒ RAG: algo_complexity ì²­í¬ (ì‹œê°„ ë³µì¡ë„ ê´€ë ¨)"
echo ""

curl -N -X POST "${BASE_URL}${API_ENDPOINT}" \
  -H "Content-Type: application/json" \
  -d '{
    "lecture_id": "cs101",
    "section_id": 1,
    "section_summary": "ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ìœ¨ì„±ì„ ë¶„ì„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ í•™ìŠµí•©ë‹ˆë‹¤. Big-O í‘œê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ ë³µì¡ë„ë¥¼ í‘œí˜„í•˜ë©°, O(1), O(log n), O(n), O(n log n), O(n^2) ë“± ë‹¤ì–‘í•œ ë³µì¡ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.",
    "subject": "ì»´í“¨í„° ê³¼í•™"
  }'

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
sleep 2

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ 2: CS101 - ìë£Œêµ¬ì¡° ê·¸ë˜í”„
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo "ğŸ“ í…ŒìŠ¤íŠ¸ 2: CS101 - ìë£Œêµ¬ì¡° ê·¸ë˜í”„"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ’¡ Summary: ê·¸ë˜í”„ëŠ” ë…¸ë“œì™€ ì—£ì§€ë¡œ êµ¬ì„±ëœ ë¹„ì„ í˜• ìë£Œêµ¬ì¡°ì…ë‹ˆë‹¤. ì •ì (Vertex)ê³¼ ê°„ì„ (Edge)ì˜ ê°œë…ì„ ì´í•´í•˜ê³ , ìµœë‹¨ ê²½ë¡œ, ë„¤íŠ¸ì›Œí¬ ë¶„ì„, ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë“± ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤."
echo "ğŸ¯ ì˜ˆìƒ RAG: graph_def ì²­í¬ (ê·¸ë˜í”„ ì •ì˜ ë° ì‘ìš©)"
echo ""

curl -N -X POST "${BASE_URL}${API_ENDPOINT}" \
  -H "Content-Type: application/json" \
  -d '{
    "lecture_id": "cs101",
    "section_id": 5,
    "section_summary": "ê·¸ë˜í”„ëŠ” ë…¸ë“œì™€ ì—£ì§€ë¡œ êµ¬ì„±ëœ ë¹„ì„ í˜• ìë£Œêµ¬ì¡°ì…ë‹ˆë‹¤. ì •ì (Vertex)ê³¼ ê°„ì„ (Edge)ì˜ ê°œë…ì„ ì´í•´í•˜ê³ , ìµœë‹¨ ê²½ë¡œ, ë„¤íŠ¸ì›Œí¬ ë¶„ì„, ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë“± ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.",
    "subject": "ì»´í“¨í„° ê³¼í•™"
  }'

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
sleep 2

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ 3: MATH201 - ë¯¸ì ë¶„ ë„í•¨ìˆ˜
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo "ğŸ“ í…ŒìŠ¤íŠ¸ 3: MATH201 - ë¯¸ì ë¶„ ë„í•¨ìˆ˜"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ’¡ Summary: ë„í•¨ìˆ˜ëŠ” í•¨ìˆ˜ì˜ ìˆœê°„ ë³€í™”ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê·¹í•œì˜ ê°œë…ì„ ì‚¬ìš©í•˜ì—¬ ì •ì˜í•˜ë©°, ë¯¸ë¶„ ê³„ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë‹¤ì–‘í•œ ê³µì‹ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì‹¤ìƒí™œì—ì„œ ì†ë„, ê°€ì†ë„ ë“±ì„ ê³„ì‚°í•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤."
echo "ğŸ¯ ì˜ˆìƒ RAG: calc_derivative ì²­í¬ (ë„í•¨ìˆ˜ ì •ì˜ ë° ê³µì‹)"
echo ""

curl -N -X POST "${BASE_URL}${API_ENDPOINT}" \
  -H "Content-Type: application/json" \
  -d '{
    "lecture_id": "math201",
    "section_id": 1,
    "section_summary": "ë„í•¨ìˆ˜ëŠ” í•¨ìˆ˜ì˜ ìˆœê°„ ë³€í™”ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê·¹í•œì˜ ê°œë…ì„ ì‚¬ìš©í•˜ì—¬ ì •ì˜í•˜ë©°, ë¯¸ë¶„ ê³„ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë‹¤ì–‘í•œ ê³µì‹ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì‹¤ìƒí™œì—ì„œ ì†ë„, ê°€ì†ë„ ë“±ì„ ê³„ì‚°í•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤.",
    "subject": "ìˆ˜í•™"
  }'

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
sleep 2

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ 4: MATH201 - ì ë¶„ ê³µì‹
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo "ğŸ“ í…ŒìŠ¤íŠ¸ 4: MATH201 - ì ë¶„ ê³µì‹"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ’¡ Summary: ì‚¼ê°í•¨ìˆ˜ì˜ ì ë¶„ì€ íŠ¹ìˆ˜í•œ ê³µì‹ì´ í•„ìš”í•©ë‹ˆë‹¤. sin, cos, tan ë“±ì˜ ì ë¶„ ê³µì‹ì„ í•™ìŠµí•˜ê³ , ì‚¼ê° ì¹˜í™˜ì„ í™œìš©í•œ ì ë¶„ ê¸°ë²•ì„ ìµí™ë‹ˆë‹¤. ì£¼ê¸° í•¨ìˆ˜ì˜ ì ë¶„ ê³„ì‚°ì— í™œìš©ë©ë‹ˆë‹¤."
echo "ğŸ¯ ì˜ˆìƒ RAG: ì‚¼ê°í•¨ìˆ˜ ì ë¶„ ê´€ë ¨ ì²­í¬"
echo ""

curl -N -X POST "${BASE_URL}${API_ENDPOINT}" \
  -H "Content-Type: application/json" \
  -d '{
    "lecture_id": "math201",
    "section_id": 3,
    "section_summary": "ì‚¼ê°í•¨ìˆ˜ì˜ ì ë¶„ì€ íŠ¹ìˆ˜í•œ ê³µì‹ì´ í•„ìš”í•©ë‹ˆë‹¤. sin, cos, tan ë“±ì˜ ì ë¶„ ê³µì‹ì„ í•™ìŠµí•˜ê³ , ì‚¼ê° ì¹˜í™˜ì„ í™œìš©í•œ ì ë¶„ ê¸°ë²•ì„ ìµí™ë‹ˆë‹¤. ì£¼ê¸° í•¨ìˆ˜ì˜ ì ë¶„ ê³„ì‚°ì— í™œìš©ë©ë‹ˆë‹¤.",
    "subject": "ìˆ˜í•™"
  }'

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ìš”ì•½
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½:"
echo "   - ì´ 4ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"
echo "   - 2ê°œ ê°•ì˜ (cs101, math201)"
echo "   - ê° ê°•ì˜ë³„ 2ê°œ ì„¹ì…˜"
echo ""
echo "ğŸ” RAG ê²€ìƒ‰ í™•ì¸ ë°©ë²•:"
echo "   1. ì„œë²„ í„°ë¯¸ë„ì—ì„œ ğŸ” RAG ê²€ìƒ‰ ê²°ê³¼ í™•ì¸"
echo "   2. Retrieved chunksì™€ score í™•ì¸"
echo "   3. Metadataì—ì„œ ì–´ë–¤ ì²­í¬ê°€ ê²€ìƒ‰ë˜ì—ˆëŠ”ì§€ í™•ì¸"
echo ""
echo "ğŸ’¡ ì„¤ì •:"
echo "   - qa_retrieve_top_k: 2 (ê¸°ë³¸ê°’)"
echo "   - ê° ìš”ì²­ë§ˆë‹¤ 2ê°œ ì²­í¬ ê²€ìƒ‰"
echo "   - Score: ë²¡í„° ìœ ì‚¬ë„ (ë†’ì„ìˆ˜ë¡ ê´€ë ¨ì„± ë†’ìŒ)"
echo ""
#!/bin/bash

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# RAG Text Upsert ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

BASE_URL="http://localhost:8000"
API_ENDPOINT="/rag/text-upsert"

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ§ª RAG Text Upsert í…ŒìŠ¤íŠ¸ ì‹œì‘"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ 1: ê¸°ë³¸ í…ìŠ¤íŠ¸ ì—…ì„œíŠ¸ (ìµœì†Œ í•„ë“œ)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo "ğŸ“ í…ŒìŠ¤íŠ¸ 1: ê¸°ë³¸ í…ìŠ¤íŠ¸ ì—…ì„œíŠ¸ (textë§Œ ìˆëŠ” ê²½ìš°)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
curl -X POST "${BASE_URL}${API_ENDPOINT}" \
  -H "Content-Type: application/json" \
  -d '{
    "lecture_id": "cs101",
    "items": [
      {
        "text": "ë°ì´í„°ë² ì´ìŠ¤ëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„°ì˜ ì§‘í•©ì…ë‹ˆë‹¤. ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ëŠ” í…Œì´ë¸” í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."
      },
      {
        "text": "SQLì€ Structured Query Languageì˜ ì•½ìë¡œ, ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¡°ì‘í•˜ëŠ” í‘œì¤€ ì–¸ì–´ì…ë‹ˆë‹¤."
      }
    ]
  }' | python3 -m json.tool
echo ""
echo "âœ… ë™ì‘ ê³¼ì •:"
echo "   1. lecture_id='cs101' â†’ collection_id='lecture_cs101' ìƒì„±"
echo "   2. ê° itemì˜ text ìë™ ID ìƒì„± (í•´ì‹œ ê¸°ë°˜)"
echo "   3. metadataì— 'source':'text' ìë™ ì¶”ê°€"
echo "   4. OpenAI APIë¡œ ì„ë² ë”© ìƒì„± (text-embedding-3-large)"
echo "   5. ChromaDBì— ë²¡í„° ì €ì¥"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
sleep 2

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ 2: ID ëª…ì‹œ + section_id í¬í•¨
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo "ğŸ“ í…ŒìŠ¤íŠ¸ 2: IDì™€ section_id ëª…ì‹œ (ìˆ˜ë™ ID + ì„¹ì…˜ êµ¬ë¶„)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
curl -X POST "${BASE_URL}${API_ENDPOINT}" \
  -H "Content-Type: application/json" \
  -d '{
    "lecture_id": "cs101",
    "items": [
      {
        "text": "ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë°(OOP)ì€ í”„ë¡œê·¸ë¨ì„ ê°ì²´ë“¤ì˜ ëª¨ìŒìœ¼ë¡œ ë³´ëŠ” í”„ë¡œê·¸ë˜ë° íŒ¨ëŸ¬ë‹¤ì„ì…ë‹ˆë‹¤.",
        "id": "cs101_oop_intro",
        "section_id": "1"
      },
      {
        "text": "ìº¡ìŠí™”, ìƒì†, ë‹¤í˜•ì„±ì€ OOPì˜ 3ëŒ€ íŠ¹ì§•ì…ë‹ˆë‹¤.",
        "id": "cs101_oop_features",
        "section_id": "1"
      },
      {
        "text": "í´ë˜ìŠ¤ëŠ” ê°ì²´ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ í…œí”Œë¦¿ì…ë‹ˆë‹¤.",
        "id": "cs101_class_def",
        "section_id": "2"
      }
    ]
  }' | python3 -m json.tool
echo ""
echo "âœ… ë™ì‘ ê³¼ì •:"
echo "   1. 'id' í•„ë“œê°€ ìˆìœ¼ë©´ â†’ í•´ë‹¹ ID ì‚¬ìš© (ìë™ ìƒì„± ì•ˆ í•¨)"
echo "   2. 'section_id' â†’ metadata['section_id']ë¡œ ì €ì¥"
echo "   3. ê°™ì€ IDë¡œ ë‹¤ì‹œ upsertí•˜ë©´ â†’ ë®ì–´ì“°ê¸° (ì—…ë°ì´íŠ¸)"
echo "   4. section_idë³„ë¡œ query ì‹œ í•„í„°ë§ ê°€ëŠ¥"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
sleep 2

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ 3: í’ë¶€í•œ ë©”íƒ€ë°ì´í„° (ê³¼ëª©, ë¶„ë¥˜, ë‚œì´ë„ ë“±)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo "ğŸ“ í…ŒìŠ¤íŠ¸ 3: í’ë¶€í•œ ë©”íƒ€ë°ì´í„° (ê³¼ëª©, ë¶„ë¥˜, ë‚œì´ë„, í‚¤ì›Œë“œ)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
curl -X POST "${BASE_URL}${API_ENDPOINT}" \
  -H "Content-Type: application/json" \
  -d '{
    "lecture_id": "cs101",
    "items": [
      {
        "text": "ì•Œê³ ë¦¬ì¦˜ì˜ ì‹œê°„ ë³µì¡ë„ëŠ” Big-O í‘œê¸°ë²•ìœ¼ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. O(n), O(log n), O(n^2) ë“±ì´ ìˆìŠµë‹ˆë‹¤.",
        "id": "algo_complexity",
        "section_id": "3",
        "metadata": {
          "subject": "ì•Œê³ ë¦¬ì¦˜",
          "category": "ì‹œê°„ë³µì¡ë„",
          "difficulty": "ì¤‘ê¸‰",
          "keywords": ["Big-O", "ë³µì¡ë„", "ì„±ëŠ¥"],
          "professor": "ê¹€êµìˆ˜",
          "chapter": 3,
          "is_important": true
        }
      },
      {
        "text": "ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ì—ëŠ” ë²„ë¸”ì •ë ¬, í€µì •ë ¬, ë³‘í•©ì •ë ¬ ë“±ì´ ìˆìŠµë‹ˆë‹¤.",
        "id": "sorting_intro",
        "section_id": "3",
        "metadata": {
          "subject": "ì•Œê³ ë¦¬ì¦˜",
          "category": "ì •ë ¬",
          "difficulty": "ì´ˆê¸‰",
          "keywords": ["ì •ë ¬", "ë²„ë¸”ì •ë ¬", "í€µì •ë ¬"],
          "professor": "ê¹€êµìˆ˜",
          "chapter": 3
        }
      }
    ]
  }' | python3 -m json.tool
echo ""
echo "âœ… ë™ì‘ ê³¼ì •:"
echo "   1. metadata í•„ë“œì— ì„ì˜ì˜ JSON ë°ì´í„° ì €ì¥ ê°€ëŠ¥"
echo "   2. ê²€ìƒ‰ ì‹œ metadata í•„í„°ë§ ê°€ëŠ¥ (ì˜ˆ: difficulty='ì¤‘ê¸‰')"
echo "   3. ChromaDBì— ë©”íƒ€ë°ì´í„°ë¡œ ì €ì¥ë˜ì–´ ê²€ìƒ‰ ì¡°ê±´ í™œìš©"
echo "   4. ë°°ì—´(keywords), ë¶ˆë¦°(is_important), ìˆ«ì(chapter) ëª¨ë‘ ê°€ëŠ¥"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
sleep 2

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ 4: ë‘ ë²ˆì§¸ lecture (ë‹¤ë¥¸ ê°•ì˜)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo "ğŸ“ í…ŒìŠ¤íŠ¸ 4: ë‹¤ë¥¸ ê°•ì˜ (lecture_id='math201')"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
curl -X POST "${BASE_URL}${API_ENDPOINT}" \
  -H "Content-Type: application/json" \
  -d '{
    "lecture_id": "math201",
    "items": [
      {
        "text": "ë¯¸ì ë¶„í•™ì€ ë³€í™”ìœ¨ê³¼ ëˆ„ì ì„ ë‹¤ë£¨ëŠ” ìˆ˜í•™ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
        "section_id": "1",
        "metadata": {
          "subject": "ìˆ˜í•™",
          "category": "ë¯¸ì ë¶„",
          "semester": "2025-1",
          "university": "ì„œìš¸ëŒ€í•™êµ"
        }
      },
      {
        "text": "ë„í•¨ìˆ˜ëŠ” í•¨ìˆ˜ì˜ ìˆœê°„ ë³€í™”ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. f(x)ì˜ ë„í•¨ìˆ˜ëŠ” lim(hâ†’0) [f(x+h)-f(x)]/hë¡œ ì •ì˜ë©ë‹ˆë‹¤.",
        "id": "calc_derivative",
        "section_id": "1",
        "metadata": {
          "subject": "ìˆ˜í•™",
          "category": "ë¯¸ì ë¶„",
          "subcategory": "ë„í•¨ìˆ˜",
          "formula": true,
          "semester": "2025-1"
        }
      }
    ]
  }' | python3 -m json.tool
echo ""
echo "âœ… ë™ì‘ ê³¼ì •:"
echo "   1. lecture_id='math201' â†’ collection_id='lecture_math201' ìƒì„±"
echo "   2. cs101ê³¼ ì™„ì „íˆ ë³„ê°œì˜ ì»¬ë ‰ì…˜ (ë…ë¦½ì  ë²¡í„° DB)"
echo "   3. ê°•ì˜ë³„ë¡œ ê²©ë¦¬ë˜ì–´ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
sleep 2

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ 5: í˜¼í•© ì¼€ì´ìŠ¤ (ID ìˆëŠ” ê²ƒ, ì—†ëŠ” ê²ƒ, metadata ìˆëŠ” ê²ƒ, ì—†ëŠ” ê²ƒ)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo "ğŸ“ í…ŒìŠ¤íŠ¸ 5: í˜¼í•© ì¼€ì´ìŠ¤ (ëª¨ë“  ì¡°í•©)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
curl -X POST "${BASE_URL}${API_ENDPOINT}" \
  -H "Content-Type: application/json" \
  -d '{
    "lecture_id": "cs101",
    "items": [
      {
        "text": "ìŠ¤íƒì€ LIFO(Last In First Out) ìë£Œêµ¬ì¡°ì…ë‹ˆë‹¤."
      },
      {
        "text": "íëŠ” FIFO(First In First Out) ìë£Œêµ¬ì¡°ì…ë‹ˆë‹¤.",
        "id": "queue_def"
      },
      {
        "text": "ì—°ê²° ë¦¬ìŠ¤íŠ¸ëŠ” ë…¸ë“œë“¤ì´ í¬ì¸í„°ë¡œ ì—°ê²°ëœ ìë£Œêµ¬ì¡°ì…ë‹ˆë‹¤.",
        "section_id": "4"
      },
      {
        "text": "ì´ì§„ íŠ¸ë¦¬ëŠ” ê° ë…¸ë“œê°€ ìµœëŒ€ 2ê°œì˜ ìì‹ì„ ê°€ì§€ëŠ” íŠ¸ë¦¬ì…ë‹ˆë‹¤.",
        "id": "binary_tree",
        "section_id": "4"
      },
      {
        "text": "í•´ì‹œ í…Œì´ë¸”ì€ í‚¤-ê°’ ìŒì„ ì €ì¥í•˜ëŠ” ìë£Œêµ¬ì¡°ë¡œ, O(1) í‰ê·  ì‹œê°„ë³µì¡ë„ë¥¼ ê°€ì§‘ë‹ˆë‹¤.",
        "metadata": {
          "subject": "ìë£Œêµ¬ì¡°",
          "difficulty": "ì¤‘ê¸‰"
        }
      },
      {
        "text": "ê·¸ë˜í”„ëŠ” ì •ì (Vertex)ê³¼ ê°„ì„ (Edge)ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.",
        "id": "graph_def",
        "section_id": "5",
        "metadata": {
          "subject": "ìë£Œêµ¬ì¡°",
          "category": "ê·¸ë˜í”„",
          "difficulty": "ê³ ê¸‰",
          "keywords": ["ê·¸ë˜í”„", "ì •ì ", "ê°„ì„ "],
          "applications": ["ìµœë‹¨ê²½ë¡œ", "ë„¤íŠ¸ì›Œí¬", "ì†Œì…œë„¤íŠ¸ì›Œí¬"]
        }
      }
    ]
  }' | python3 -m json.tool
echo ""
echo "âœ… ë™ì‘ ê³¼ì •:"
echo "   [Item 1] textë§Œ â†’ ID ìë™ìƒì„±, metadata={'source':'text'}"
echo "   [Item 2] text + id â†’ ì§€ì •ëœ ID ì‚¬ìš©, metadata={'source':'text'}"
echo "   [Item 3] text + section_id â†’ ID ìë™, metadata={'section_id':'4','source':'text'}"
echo "   [Item 4] text + id + section_id â†’ ì§€ì • ID, metadata={'section_id':'4','source':'text'}"
echo "   [Item 5] text + metadata â†’ ID ìë™, ì§€ì •ëœ metadata ì‚¬ìš©"
echo "   [Item 6] ëª¨ë‘ ìˆìŒ â†’ ì§€ì • ID, ëª¨ë“  í•„ë“œ í™œìš©"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
sleep 2

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ 6: ëŒ€ëŸ‰ ì—…ì„œíŠ¸ (10ê°œ í•­ëª©)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo "ğŸ“ í…ŒìŠ¤íŠ¸ 6: ëŒ€ëŸ‰ ì—…ì„œíŠ¸ (10ê°œ í•­ëª© - ì„±ëŠ¥ í…ŒìŠ¤íŠ¸)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
curl -X POST "${BASE_URL}${API_ENDPOINT}" \
  -H "Content-Type: application/json" \
  -d '{
    "lecture_id": "math201",
    "items": [
      {"text": "ì ë¶„ì€ í•¨ìˆ˜ì˜ ëˆ„ì ê°’ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.", "section_id": "2", "metadata": {"chapter": 2}},
      {"text": "ì •ì ë¶„ì€ ì •í•´ì§„ êµ¬ê°„ì—ì„œì˜ ì ë¶„ì…ë‹ˆë‹¤.", "section_id": "2", "metadata": {"chapter": 2}},
      {"text": "ë¶€ì •ì ë¶„ì€ ì›ì‹œí•¨ìˆ˜ë¥¼ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤.", "section_id": "2", "metadata": {"chapter": 2}},
      {"text": "ì¹˜í™˜ì ë¶„ë²•ì€ ë³µì¡í•œ ì ë¶„ì„ ê°„ë‹¨í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.", "section_id": "3", "metadata": {"chapter": 3}},
      {"text": "ë¶€ë¶„ì ë¶„ë²•ì€ ê³±ì˜ ì ë¶„ì— ì‚¬ìš©ë©ë‹ˆë‹¤.", "section_id": "3", "metadata": {"chapter": 3}},
      {"text": "ì‚¼ê°í•¨ìˆ˜ì˜ ì ë¶„ì—ëŠ” íŠ¹ìˆ˜í•œ ê³µì‹ì´ ìˆìŠµë‹ˆë‹¤.", "section_id": "3", "metadata": {"chapter": 3, "type": "ê³µì‹"}},
      {"text": "ì´ìƒì ë¶„ì€ ë¬´í•œ êµ¬ê°„ì´ë‚˜ ë¶ˆì—°ì†ì ì„ í¬í•¨í•©ë‹ˆë‹¤.", "section_id": "4", "metadata": {"chapter": 4, "difficulty": "ê³ ê¸‰"}},
      {"text": "ì¤‘ì ë¶„ì€ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ì ë¶„ì…ë‹ˆë‹¤.", "section_id": "5", "metadata": {"chapter": 5, "difficulty": "ê³ ê¸‰"}},
      {"text": "í‘¸ë¹„ë‹ˆ ì •ë¦¬ëŠ” ì¤‘ì ë¶„ì˜ ìˆœì„œë¥¼ ë°”ê¿€ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.", "section_id": "5", "metadata": {"chapter": 5, "theorem": true}},
      {"text": "ê·¸ë¦° ì •ë¦¬ëŠ” ì„ ì ë¶„ê³¼ ì¤‘ì ë¶„ì„ ì—°ê²°í•©ë‹ˆë‹¤.", "section_id": "6", "metadata": {"chapter": 6, "theorem": true, "difficulty": "ìµœìƒê¸‰"}}
    ]
  }' | python3 -m json.tool
echo ""
echo "âœ… ë™ì‘ ê³¼ì •:"
echo "   1. 10ê°œ í•­ëª©ì˜ textë¥¼ í•œ ë²ˆì— OpenAI APIë¡œ ì „ì†¡"
echo "   2. ë°°ì¹˜ ì„ë² ë”© ìƒì„± (íš¨ìœ¨ì )"
echo "   3. ChromaDBì— bulk upsert"
echo "   4. ì‘ë‹µ ì‹œê°„ í™•ì¸ ê°€ëŠ¥"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
sleep 2

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ 7: ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸ (ê°™ì€ IDë¡œ ë‹¤ì‹œ upsert)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo "ğŸ“ í…ŒìŠ¤íŠ¸ 7: ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸ (ê°™ì€ IDë¡œ ë®ì–´ì“°ê¸°)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "7-1. ì´ˆê¸° ë°ì´í„° ì‚½ì…"
curl -X POST "${BASE_URL}${API_ENDPOINT}" \
  -H "Content-Type: application/json" \
  -d '{
    "lecture_id": "cs101",
    "items": [
      {
        "text": "íŒŒì´ì¬ì€ ì¸í„°í”„ë¦¬í„° ì–¸ì–´ì…ë‹ˆë‹¤. (êµ¬ë²„ì „)",
        "id": "python_intro_v1",
        "metadata": {"version": 1, "updated_at": "2025-01-01"}
      }
    ]
  }' | python3 -m json.tool
echo ""
echo "7-2. ê°™ì€ IDë¡œ ì—…ë°ì´íŠ¸ (ë‚´ìš© ë³€ê²½)"
sleep 1
curl -X POST "${BASE_URL}${API_ENDPOINT}" \
  -H "Content-Type: application/json" \
  -d '{
    "lecture_id": "cs101",
    "items": [
      {
        "text": "íŒŒì´ì¬ì€ ë™ì  íƒ€ì´í•‘ì„ ì§€ì›í•˜ëŠ” ê³ ìˆ˜ì¤€ ì¸í„°í”„ë¦¬í„° ì–¸ì–´ì…ë‹ˆë‹¤. (ì‹ ë²„ì „ - ë” ìƒì„¸í•¨)",
        "id": "python_intro_v1",
        "metadata": {"version": 2, "updated_at": "2025-01-14"}
      }
    ]
  }' | python3 -m json.tool
echo ""
echo "âœ… ë™ì‘ ê³¼ì •:"
echo "   1. ì²« ë²ˆì§¸ upsert: 'python_intro_v1' IDë¡œ ì €ì¥"
echo "   2. ë‘ ë²ˆì§¸ upsert: ê°™ì€ ID â†’ ê¸°ì¡´ ë°ì´í„° ë®ì–´ì“°ê¸°"
echo "   3. ì„ë² ë”©ë„ ìƒˆë¡œ ìƒì„±ë˜ì–´ ê°±ì‹ ë¨"
echo "   4. metadataë„ ì™„ì „íˆ êµì²´ë¨"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ìš”ì•½
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½:"
echo "   - ì´ 7ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"
echo "   - 2ê°œ ê°•ì˜ (cs101, math201)"
echo "   - ë‹¤ì–‘í•œ í•„ë“œ ì¡°í•© í…ŒìŠ¤íŠ¸"
echo ""
echo "ğŸ” ë‚´ë¶€ ë™ì‘ íë¦„:"
echo "   1. ìš”ì²­ â†’ FastAPI ì—”ë“œí¬ì¸íŠ¸ (/rag/text-upsert)"
echo "   2. Pydantic ê²€ì¦ (TextUpsertRequest)"
echo "   3. collection_id ìƒì„± (lecture_{lecture_id})"
echo "   4. items ì „ì²˜ë¦¬ (metadata ë³‘í•©, section_id ì²˜ë¦¬)"
echo "   5. RAGService.upsert_text() í˜¸ì¶œ"
echo "   6. ID ìƒì„± or ì‚¬ìš© (make_id() í•¨ìˆ˜)"
echo "   7. OpenAI Embedding API í˜¸ì¶œ (text-embedding-3-large)"
echo "   8. ChromaDB upsert_many() í˜¸ì¶œ"
echo "   9. ê²°ê³¼ ë°˜í™˜ (collection_id, count, embedding_dim)"
echo ""
echo "ğŸ“ ì €ì¥ ìœ„ì¹˜:"
echo "   - ChromaDB: server_storage/chroma_data/ ë˜ëŠ” chroma_data_real/"
echo "   - ì»¬ë ‰ì…˜: lecture_cs101, lecture_math201"
echo ""
echo "ğŸ”— ë‹¤ìŒ ë‹¨ê³„:"
echo "   - /rag/queryë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"
echo "   - /qa/generateë¡œ QA ìƒì„± í…ŒìŠ¤íŠ¸"
echo "   - /rec/recommendë¡œ ì¶”ì²œ í…ŒìŠ¤íŠ¸"
echo ""
#!/usr/bin/env python3
"""
RAG Vector DB ë°ì´í„° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

test_upsert.shë¡œ ì—…ì„œíŠ¸í•œ ë°ì´í„°ê°€ ChromaDBì— ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ (OpenAI API í‚¤ ë“±)
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["CHROMA_TELEMETRY_ENABLED"] = "False"

# RAG ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent / "cap1_RAG_module"))

from ragkit.service import RAGService
from ragkit.config import RAGConfig


def print_separator(char="â”", length=80):
    """êµ¬ë¶„ì„  ì¶œë ¥"""
    print(char * length)


def print_section_header(title: str):
    """ì„¹ì…˜ í—¤ë” ì¶œë ¥"""
    print("\n")
    print_separator()
    print(f"ğŸ“Š {title}")
    print_separator()


def verify_collection(service: RAGService, collection_id: str, expected_count: int):
    """
    ì»¬ë ‰ì…˜ì˜ ë°ì´í„° í™•ì¸
    
    Args:
        service: RAGService ì¸ìŠ¤í„´ìŠ¤
        collection_id: í™•ì¸í•  ì»¬ë ‰ì…˜ ID
        expected_count: ì˜ˆìƒ ë¬¸ì„œ ìˆ˜
    """
    print_section_header(f"Collection: {collection_id}")
    
    try:
        # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
        collection = service.vector_store.client.get_collection(collection_id)
        
        # ì „ì²´ ë¬¸ì„œ ìˆ˜ í™•ì¸
        count = collection.count()
        print(f"âœ… ì´ ë¬¸ì„œ ìˆ˜: {count}ê°œ")
        print(f"ğŸ“ ì˜ˆìƒ ë¬¸ì„œ ìˆ˜: {expected_count}ê°œ")
        
        if count != expected_count:
            print(f"âš ï¸  ê²½ê³ : ì˜ˆìƒ({expected_count})ê³¼ ì‹¤ì œ({count})ê°€ ë‹¤ë¦…ë‹ˆë‹¤!")
        else:
            print(f"âœ… ë¬¸ì„œ ìˆ˜ ì¼ì¹˜!")
        
        # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        results = collection.get(
            include=["documents", "metadatas"]
        )
        
        print(f"\nğŸ“„ ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡:")
        print_separator("-", 80)
        
        for i, (doc_id, document, metadata) in enumerate(
            zip(results["ids"], results["documents"], results["metadatas"]), 
            start=1
        ):
            print(f"\n[{i}] ID: {doc_id}")
            print(f"    Text: {document[:100]}{'...' if len(document) > 100 else ''}")
            print(f"    Metadata: {metadata}")
        
        print_separator("-", 80)
        
        # ë©”íƒ€ë°ì´í„° í†µê³„
        print(f"\nğŸ“ˆ ë©”íƒ€ë°ì´í„° í†µê³„:")
        
        # section_id ë¶„í¬
        section_ids = [m.get("section_id") for m in results["metadatas"] if m.get("section_id")]
        if section_ids:
            from collections import Counter
            section_counts = Counter(section_ids)
            print(f"   Section ë¶„í¬: {dict(section_counts)}")
        
        # subject ë¶„í¬
        subjects = [m.get("subject") for m in results["metadatas"] if m.get("subject")]
        if subjects:
            from collections import Counter
            subject_counts = Counter(subjects)
            print(f"   Subject ë¶„í¬: {dict(subject_counts)}")
        
        # difficulty ë¶„í¬
        difficulties = [m.get("difficulty") for m in results["metadatas"] if m.get("difficulty")]
        if difficulties:
            from collections import Counter
            diff_counts = Counter(difficulties)
            print(f"   Difficulty ë¶„í¬: {dict(diff_counts)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print_separator("â•", 80)
    print("ğŸ” RAG Vector DB ë°ì´í„° ê²€ì¦ ì‹œì‘")
    print_separator("â•", 80)
    
    # RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    print("\nì´ˆê¸°í™” ì¤‘...")
    persist_dir = os.getenv("RAG_PERSIST_DIR", "server_storage/chroma_data")
    print(f"ğŸ“ ChromaDB ê²½ë¡œ: {persist_dir}")
    
    config = RAGConfig(persist_dir=persist_dir)
    service = RAGService(config=config)
    
    # ëª¨ë“  ì»¬ë ‰ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    all_collections = service.vector_store.client.list_collections()
    print(f"\nğŸ“š ë°œê²¬ëœ ì»¬ë ‰ì…˜: {len(all_collections)}ê°œ")
    for col in all_collections:
        print(f"   - {col.name}")
    
    # test_upsert.shì—ì„œ ìƒì„±í•œ ì»¬ë ‰ì…˜ë“¤ í™•ì¸
    test_collections = {
        "lecture_cs101": {
            "expected_count": 20,  # í…ŒìŠ¤íŠ¸ 1(2) + 2(3) + 3(2) + 5(6) + 7(1+1) + ì—…ë°ì´íŠ¸
            "description": "Computer Science 101 ê°•ì˜"
        },
        "lecture_math201": {
            "expected_count": 12,  # í…ŒìŠ¤íŠ¸ 4(2) + 6(10)
            "description": "ìˆ˜í•™ 201 ê°•ì˜"
        }
    }
    
    print("\n")
    print_separator("â•", 80)
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ê²€ì¦")
    print_separator("â•", 80)
    
    results = {}
    
    for collection_id, info in test_collections.items():
        success = verify_collection(
            service, 
            collection_id, 
            info["expected_count"]
        )
        results[collection_id] = success
    
    # ìµœì¢… ìš”ì•½
    print("\n")
    print_separator("â•", 80)
    print("ğŸ“Š ê²€ì¦ ê²°ê³¼ ìš”ì•½")
    print_separator("â•", 80)
    
    for collection_id, success in results.items():
        status = "âœ… ì„±ê³µ" if success else "âŒ ì‹¤íŒ¨"
        print(f"{status} - {collection_id}")
    
    all_success = all(results.values())
    
    print("\n")
    if all_success:
        print("ğŸ‰ ëª¨ë“  ì»¬ë ‰ì…˜ ê²€ì¦ ì™„ë£Œ!")
        print("âœ… test_upsert.shì˜ ëª¨ë“  ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸  ì¼ë¶€ ì»¬ë ‰ì…˜ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        print("âŒ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ì„¸ìš”.")
    
    print_separator("â•", 80)
    
    return 0 if all_success else 1


if __name__ == "__main__":
    sys.exit(main())
#!/usr/bin/env python3
"""
RAG Vector DB ìƒì„¸ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

test_upsert.shì—ì„œ ì—…ì„œíŠ¸í•œ ë°ì´í„°ì™€ ì‹¤ì œ ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["CHROMA_TELEMETRY_ENABLED"] = "False"

# RAG ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent / "cap1_RAG_module"))

from ragkit.service import RAGService
from ragkit.config import RAGConfig


# test_upsert.shì—ì„œ ì—…ì„œíŠ¸í•œ ì˜ˆìƒ ë°ì´í„°
EXPECTED_DATA = {
    "lecture_cs101": [
        # í…ŒìŠ¤íŠ¸ 1: ê¸°ë³¸ í…ìŠ¤íŠ¸ (2ê°œ)
        {"text_snippet": "ë°ì´í„°ë² ì´ìŠ¤ëŠ” êµ¬ì¡°í™”ëœ", "has_id": False, "metadata_keys": ["source"]},
        {"text_snippet": "SQLì€ Structured Query", "has_id": False, "metadata_keys": ["source"]},
        
        # í…ŒìŠ¤íŠ¸ 2: ID + section_id (3ê°œ)
        {"text_snippet": "ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë°", "has_id": True, "id": "cs101_oop_intro", "metadata_keys": ["section_id"]},
        {"text_snippet": "ìº¡ìŠí™”, ìƒì†, ë‹¤í˜•ì„±", "has_id": True, "id": "cs101_oop_features", "metadata_keys": ["section_id"]},
        {"text_snippet": "í´ë˜ìŠ¤ëŠ” ê°ì²´ë¥¼", "has_id": True, "id": "cs101_class_def", "metadata_keys": ["section_id"]},
        
        # í…ŒìŠ¤íŠ¸ 3: í’ë¶€í•œ ë©”íƒ€ë°ì´í„° (2ê°œ)
        {"text_snippet": "ì•Œê³ ë¦¬ì¦˜ì˜ ì‹œê°„ ë³µì¡ë„", "has_id": True, "id": "algo_complexity", "metadata_keys": ["subject", "category", "difficulty", "section_id"]},
        {"text_snippet": "ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ì—ëŠ”", "has_id": True, "id": "sorting_intro", "metadata_keys": ["subject", "category", "difficulty", "section_id"]},
        
        # í…ŒìŠ¤íŠ¸ 5: í˜¼í•© ì¼€ì´ìŠ¤ (6ê°œ)
        {"text_snippet": "ìŠ¤íƒì€ LIFO", "has_id": False, "metadata_keys": ["source"]},
        {"text_snippet": "íëŠ” FIFO", "has_id": True, "id": "queue_def", "metadata_keys": ["source"]},
        {"text_snippet": "ì—°ê²° ë¦¬ìŠ¤íŠ¸ëŠ”", "has_id": False, "metadata_keys": ["section_id"]},
        {"text_snippet": "ì´ì§„ íŠ¸ë¦¬ëŠ”", "has_id": True, "id": "binary_tree", "metadata_keys": ["section_id"]},
        {"text_snippet": "í•´ì‹œ í…Œì´ë¸”ì€", "has_id": False, "metadata_keys": ["subject", "difficulty"]},
        {"text_snippet": "ê·¸ë˜í”„ëŠ” ì •ì ", "has_id": True, "id": "graph_def", "metadata_keys": ["subject", "category", "difficulty", "section_id", "applications"]},
        
        # í…ŒìŠ¤íŠ¸ 7: ì—…ë°ì´íŠ¸ (1ê°œ, ìµœì¢… ë²„ì „ë§Œ)
        {"text_snippet": "íŒŒì´ì¬ì€ ë™ì  íƒ€ì´í•‘", "has_id": True, "id": "python_intro_v1", "metadata_keys": ["version", "updated_at"]},
    ],
    "lecture_math201": [
        # í…ŒìŠ¤íŠ¸ 4: ë‹¤ë¥¸ ê°•ì˜ (2ê°œ)
        {"text_snippet": "ë¯¸ì ë¶„í•™ì€ ë³€í™”ìœ¨", "has_id": False, "metadata_keys": ["subject", "category", "section_id", "semester", "university"]},
        {"text_snippet": "ë„í•¨ìˆ˜ëŠ” í•¨ìˆ˜ì˜", "has_id": True, "id": "calc_derivative", "metadata_keys": ["subject", "category", "subcategory", "section_id", "semester", "formula"]},
        
        # í…ŒìŠ¤íŠ¸ 6: ëŒ€ëŸ‰ ì—…ì„œíŠ¸ (10ê°œ)
        {"text_snippet": "ì ë¶„ì€ í•¨ìˆ˜ì˜", "has_id": False, "metadata_keys": ["section_id", "chapter"]},
        {"text_snippet": "ì •ì ë¶„ì€ ì •í•´ì§„", "has_id": False, "metadata_keys": ["section_id", "chapter"]},
        {"text_snippet": "ë¶€ì •ì ë¶„ì€ ì›ì‹œí•¨ìˆ˜", "has_id": False, "metadata_keys": ["section_id", "chapter"]},
        {"text_snippet": "ì¹˜í™˜ì ë¶„ë²•ì€", "has_id": False, "metadata_keys": ["section_id", "chapter"]},
        {"text_snippet": "ë¶€ë¶„ì ë¶„ë²•ì€", "has_id": False, "metadata_keys": ["section_id", "chapter"]},
        {"text_snippet": "ì‚¼ê°í•¨ìˆ˜ì˜ ì ë¶„", "has_id": False, "metadata_keys": ["section_id", "chapter", "type"]},
        {"text_snippet": "ì´ìƒì ë¶„ì€ ë¬´í•œ", "has_id": False, "metadata_keys": ["section_id", "chapter", "difficulty"]},
        {"text_snippet": "ì¤‘ì ë¶„ì€ ë‹¤ë³€ìˆ˜", "has_id": False, "metadata_keys": ["section_id", "chapter", "difficulty"]},
        {"text_snippet": "í‘¸ë¹„ë‹ˆ ì •ë¦¬", "has_id": False, "metadata_keys": ["section_id", "chapter", "theorem"]},
        {"text_snippet": "ê·¸ë¦° ì •ë¦¬", "has_id": False, "metadata_keys": ["section_id", "chapter", "theorem", "difficulty"]},
    ]
}


def compare_documents(collection_id: str, expected: list, actual_results: dict):
    """
    ì˜ˆìƒ ë¬¸ì„œì™€ ì‹¤ì œ ë¬¸ì„œ ë¹„êµ
    """
    print(f"\nğŸ“‹ ìƒì„¸ ë¹„êµ: {collection_id}")
    print("=" * 80)
    
    actual_ids = actual_results["ids"]
    actual_docs = actual_results["documents"]
    actual_metas = actual_results["metadatas"]
    
    expected_count = len(expected)
    actual_count = len(actual_ids)
    
    print(f"\nğŸ“Š ë¬¸ì„œ ìˆ˜ ë¹„êµ:")
    print(f"   ì˜ˆìƒ: {expected_count}ê°œ")
    print(f"   ì‹¤ì œ: {actual_count}ê°œ")
    
    if expected_count == actual_count:
        print(f"   âœ… ë¬¸ì„œ ìˆ˜ ì¼ì¹˜!")
    else:
        print(f"   âš ï¸  ë¬¸ì„œ ìˆ˜ ë¶ˆì¼ì¹˜ (ì°¨ì´: {actual_count - expected_count})")
    
    # ê° ì˜ˆìƒ ë¬¸ì„œê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    print(f"\nğŸ“ ë¬¸ì„œë³„ ìƒì„¸ í™•ì¸:")
    print("-" * 80)
    
    found_count = 0
    missing_items = []
    
    for i, exp_item in enumerate(expected, start=1):
        snippet = exp_item["text_snippet"]
        found = False
        matched_doc = None
        matched_id = None
        matched_meta = None
        
        # í…ìŠ¤íŠ¸ ìŠ¤ë‹ˆí«ìœ¼ë¡œ ì°¾ê¸°
        for doc_id, doc, meta in zip(actual_ids, actual_docs, actual_metas):
            if snippet in doc:
                found = True
                matched_doc = doc
                matched_id = doc_id
                matched_meta = meta
                break
        
        if found:
            print(f"\nâœ… [{i}] ë°œê²¬ë¨")
            print(f"    ìŠ¤ë‹ˆí«: {snippet}...")
            print(f"    ID: {matched_id}")
            
            # ID í™•ì¸
            if exp_item.get("has_id") and exp_item.get("id"):
                expected_id = exp_item["id"]
                if matched_id == expected_id:
                    print(f"    âœ… ID ì¼ì¹˜: {expected_id}")
                else:
                    print(f"    âš ï¸  ID ë¶ˆì¼ì¹˜: ì˜ˆìƒ({expected_id}) vs ì‹¤ì œ({matched_id})")
            
            # ë©”íƒ€ë°ì´í„° í‚¤ í™•ì¸
            expected_keys = set(exp_item.get("metadata_keys", []))
            actual_keys = set(matched_meta.keys())
            
            if expected_keys.issubset(actual_keys):
                print(f"    âœ… ë©”íƒ€ë°ì´í„° í‚¤ í¬í•¨: {expected_keys}")
            else:
                missing_keys = expected_keys - actual_keys
                print(f"    âš ï¸  ëˆ„ë½ëœ ë©”íƒ€ë°ì´í„° í‚¤: {missing_keys}")
                print(f"       ì‹¤ì œ í‚¤: {actual_keys}")
            
            found_count += 1
        else:
            print(f"\nâŒ [{i}] ëˆ„ë½ë¨")
            print(f"    ìŠ¤ë‹ˆí«: {snippet}...")
            if exp_item.get("has_id") and exp_item.get("id"):
                print(f"    ì˜ˆìƒ ID: {exp_item['id']}")
            missing_items.append(exp_item)
    
    print("\n" + "=" * 80)
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"   ë°œê²¬: {found_count}/{expected_count} ({found_count/expected_count*100:.1f}%)")
    
    if missing_items:
        print(f"\nâš ï¸  ëˆ„ë½ëœ í•­ëª© ({len(missing_items)}ê°œ):")
        for item in missing_items:
            print(f"   - {item['text_snippet']}...")
    else:
        print(f"\nğŸ‰ ëª¨ë“  ì˜ˆìƒ ë¬¸ì„œê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
    
    return found_count == expected_count


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸ” RAG Vector DB ìƒì„¸ ê²€ì¦ (ì˜ˆìƒ ë°ì´í„° vs ì‹¤ì œ ë°ì´í„°)")
    print("=" * 80)
    
    # RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    persist_dir = os.getenv("RAG_PERSIST_DIR", "server_storage/chroma_data")
    config = RAGConfig(persist_dir=persist_dir)
    service = RAGService(config=config)
    
    results = {}
    
    for collection_id, expected_docs in EXPECTED_DATA.items():
        try:
            collection = service.vector_store.client.get_collection(collection_id)
            actual_results = collection.get(include=["documents", "metadatas"])
            
            success = compare_documents(collection_id, expected_docs, actual_results)
            results[collection_id] = success
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ({collection_id}): {e}")
            results[collection_id] = False
    
    # ìµœì¢… ìš”ì•½
    print("\n")
    print("=" * 80)
    print("ğŸ“Š ì „ì²´ ê²€ì¦ ê²°ê³¼")
    print("=" * 80)
    
    for collection_id, success in results.items():
        status = "âœ… ì™„ë²½" if success else "âš ï¸  ë¬¸ì œ ìˆìŒ"
        print(f"{status} - {collection_id}")
    
    all_success = all(results.values())
    
    print("\n")
    if all_success:
        print("ğŸ‰ ì™„ë²½í•©ë‹ˆë‹¤! ëª¨ë“  ì˜ˆìƒ ë°ì´í„°ê°€ ì •í™•íˆ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸  ì¼ë¶€ ë°ì´í„°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ìœ„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    print("=" * 80)
    
    return 0 if all_success else 1


if __name__ == "__main__":
    sys.exit(main())
