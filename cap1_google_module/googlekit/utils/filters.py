"""
Google ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ ìœ í‹¸ë¦¬í‹°
"""
import logging
from typing import List, Dict, Any
from urllib.parse import urlparse

logger = logging.getLogger(__name__)


def deduplicate_results(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì¤‘ë³µ ì œê±°ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    seen_urls = set()
    unique_results = []
    
    for result in results:
        url = result.get("link", "")
        if not url:
            continue
        
        # URL ì •ê·œí™” (í”„ë¡œí† ì½œ, www ì œê±° í›„ ë¹„êµ)
        parsed = urlparse(url)
        normalized = f"{parsed.netloc.replace('www.', '')}{parsed.path}"
        
        if normalized not in seen_urls:
            seen_urls.add(normalized)
            unique_results.append(result)
    
    logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {len(results)}ê°œ â†’ {len(unique_results)}ê°œ")
    
    return unique_results


def rerank_results(
    results: List[Dict[str, Any]],
    keywords: List[str]
) -> List[Dict[str, Any]]:
    """
    í‚¤ì›Œë“œ ë§¤ì¹­ë„ ê¸°ì¤€ ì¬ì •ë ¬
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì¬ì •ë ¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    def keyword_match_score(result: Dict[str, Any]) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        title = result.get("title", "").lower()
        snippet = result.get("snippet", "").lower()
        
        score = 0.0
        for keyword in keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in title:
                score += 2.0  # ì œëª© ë§¤ì¹­ ê°€ì¤‘ì¹˜ ë†’ìŒ
            if keyword_lower in snippet:
                score += 1.0
        
        return score
    
    # ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
    scored_results = [
        (result, keyword_match_score(result))
        for result in results
    ]
    scored_results.sort(key=lambda x: x[1], reverse=True)
    
    # ê²°ê³¼ë§Œ ë°˜í™˜
    reranked = [result for result, _ in scored_results]
    
    logger.info(f"ğŸ“Š ì¬ì •ë ¬ ì™„ë£Œ: {len(reranked)}ê°œ")
    
    return reranked


def filter_excluded_urls(
    results: List[Dict[str, Any]],
    exclude_urls: List[str]
) -> List[Dict[str, Any]]:
    """
    ì œì™¸ URL í•„í„°ë§
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        exclude_urls: ì œì™¸í•  URL ë¦¬ìŠ¤íŠ¸
        
    Returns:
        í•„í„°ë§ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    if not exclude_urls:
        return results
    
    # URL ì •ê·œí™”
    exclude_normalized = set()
    for url in exclude_urls:
        parsed = urlparse(url)
        normalized = f"{parsed.netloc.replace('www.', '')}{parsed.path}"
        exclude_normalized.add(normalized)
    
    filtered = []
    for result in results:
        url = result.get("link", "")
        parsed = urlparse(url)
        normalized = f"{parsed.netloc.replace('www.', '')}{parsed.path}"
        
        if normalized not in exclude_normalized:
            filtered.append(result)
    
    logger.info(f"ğŸš« ì œì™¸ URL í•„í„°ë§: {len(results)}ê°œ â†’ {len(filtered)}ê°œ")
    
    return filtered
