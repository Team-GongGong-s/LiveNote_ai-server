from __future__ import annotations

import asyncio
import logging
from typing import List

from .api import YouTubeAPIClient
from .llm import YouTubeLLMClient
from .models import (
    YouTubeRequest,
    YouTubeResponse,
    YouTubeVideoInfo,
)
from .utils import normalize_title, deduplicate_items, heuristic_score
from .config import flags

logger = logging.getLogger(__name__)


class YouTubeService:
    """High-level service for lecture-aware YouTube recommendations."""

    def __init__(self, yt_client: YouTubeAPIClient | None = None, llm: YouTubeLLMClient | None = None):
        from .config.youtube_config import YouTubeConfig
        self.yt = yt_client or YouTubeAPIClient(api_key=YouTubeConfig.YOUTUBE_API_KEY)
        self.llm = llm or YouTubeLLMClient(api_key=YouTubeConfig.OPENAI_API_KEY)

    async def recommend_videos(self, request: YouTubeRequest) -> List[YouTubeResponse]:
        from .config.youtube_config import YouTubeConfig
        from .config import flags
        
        # 1) Build queries (LLM or stub)
        q_payload = await self.llm.generate_queries(
            {
                "lecture_summary": request.lecture_summary,
                "language": request.language,
                "yt_lang": request.yt_lang,
                "previous_summaries": [s.model_dump() for s in request.previous_summaries],
                "rag_context": [c.model_dump() for c in request.rag_context],
            }
        )
        queries = list(dict.fromkeys([q.strip() for q in q_payload.get("queries", []) if q.strip()]))
        
        # ğŸ”§ Query ê°œìˆ˜ ì œí•œ (QUERY_MAX)
        if len(queries) > flags.QUERY_MAX:
            logger.info(f"Query ê°œìˆ˜ ì œí•œ: {len(queries)}ê°œ â†’ {flags.QUERY_MAX}ê°œ")
            queries = queries[:flags.QUERY_MAX]
        
        if not queries:
            queries = [request.lecture_summary[:60]]

        # 2) Search videos (fan-out) then fetch details
        # ğŸš€ OPTIMIZATION: Search all queries in parallel
        async def search_single_query(q: str):
            """Search videos for a single query in parallel"""
            items = await self.yt.search_videos(
                q=q, 
                lang=request.yt_lang, 
                max_results=flags.MAX_SEARCH_RESULTS  # ğŸ”§ flagsì—ì„œ ê°€ì ¸ì˜´
            )
            return [(normalize_title(it.title), it) for it in items]
        
        # Execute all searches in parallel
        search_results = await asyncio.gather(*[search_single_query(q) for q in queries])
        
        # Flatten results from all queries
        search_items = []
        for items in search_results:
            search_items.extend(items)

        # Apply exclusion by title early
        excludes = set(normalize_title(t) for t in request.exclude_titles)
        search_items = [(k, v) for (k, v) in search_items if k not in excludes]

        # Deduplicate
        dedup = deduplicate_items((k, v) for (k, v) in search_items)
        if not dedup:
            return []

        ids = [it.video_id for it in dedup]
        details = await self.yt.get_videos(ids)
        detail_map = {d.video_id: d for d in details}
        best_scores: list[float] = []  # min_score íƒˆë½ í›„ë³´ ì ìˆ˜ ì¶”ì 

        # ğŸš€ NO_SCORING ëª¨ë“œ: ê²€ì¦ ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜
        if flags.NO_SCORING:
            logger.info("âš¡ NO_SCORING ëª¨ë“œ: ê²€ì¦ ìŠ¤í‚µ (description ì‚¬ìš©)")
            results = []
            for it in dedup[:request.top_k]:
                d = detail_map.get(it.video_id)
                if not d:
                    continue
                
                vi = YouTubeVideoInfo(
                    url=d.url(),
                    title=d.title,
                    extract=d.description or "No description available",
                    lang=request.yt_lang
                )
                results.append(YouTubeResponse(
                    lecture_id=request.lecture_id,
                    section_id=request.section_id,
                    video_info=vi,
                    reason="search",
                    score=10.0
                ))
            logger.info(f"âœ… NO_SCORING ê²°ê³¼: {len(results)}ê°œ ë°˜í™˜")
            return results

        # 3) Build candidate list with summary + (optional) LLM score
        # ğŸš€ OPTIMIZATION: Process videos in parallel with Semaphore (ë™ì‹œì„± ì œí•œ)
        semaphore = asyncio.Semaphore(YouTubeConfig.VERIFY_CONCURRENCY)
        
        async def process_single_video(it):
            """Process one video (summary + optional LLM verification) in parallel"""
            async with semaphore:  # ğŸ”§ ë™ì‹œì„± ì œí•œ
                def best_heuristic_score(title: str, view_count: int, publish_time: str) -> float:
                    """ëª¨ë“  ê²€ìƒ‰ì–´ ì¤‘ ìµœê³  íœ´ë¦¬ìŠ¤í‹± ì ìˆ˜ ê³„ì‚°"""
                    candidate_queries = queries or [request.lecture_summary[:60]]
                    scores = [
                        heuristic_score(
                            title=title,
                            query=q,
                            view_count=view_count,
                            publish_time=publish_time,
                        )
                        for q in candidate_queries
                    ]
                    return max(scores) if scores else 0.0

                d = detail_map.get(it.video_id)
                if not d:
                    # Fall back to basic snippet if details missing
                    title = it.title
                    content = it.description
                    lang = request.yt_lang
                    url = f"https://www.youtube.com/watch?v={it.video_id}"
                    
                    # ğŸ”§ ìë§‰ ì—†ì´ ìš”ì•½ (ì œëª©/ì„¤ëª…ë§Œ)
                    sum_payload = await self.llm.summarize_content_no_transcript(
                        title=title, 
                        description=content, 
                        channel="Unknown",
                        language=request.language
                    )
                    extract = sum_payload.get("extract", content[:300])
                    
                    # Heuristic only
                    base = best_heuristic_score(title=title, view_count=0, publish_time=it.publish_time)
                    best_scores.append(base)
                    if base < request.min_score:
                        logger.info(f"ğŸ§Š YT í•„í„°ë§(min_score): {base:.2f} < {request.min_score} (no detail, url=https://www.youtube.com/watch?v={it.video_id})")
                        return None
                    
                    vi = YouTubeVideoInfo(url=url, title=title, extract=extract, lang=lang)
                    return YouTubeResponse(
                        lecture_id=request.lecture_id,
                        section_id=request.section_id,
                        video_info=vi,
                        reason="Heuristic",
                        score=base,
                    )

                # ğŸ”§ ìë§‰ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
                if flags.USE_TRANSCRIPT:
                    transcript = await self.yt.fetch_transcript(d.video_id, preferred_langs=[request.yt_lang, "en", "ko"])  # type: ignore[arg-type]
                    content_src = transcript or (d.description or d.title)
                    # ìë§‰ì´ ìˆìœ¼ë©´ ì •ìƒ ìš”ì•½
                    sum_payload = await self.llm.summarize_content(
                        title=d.title, content=content_src, language=request.language
                    )
                else:
                    # ìë§‰ ì—†ì´ ì œëª©/ì„¤ëª…ë§Œìœ¼ë¡œ ìš”ì•½
                    sum_payload = await self.llm.summarize_content_no_transcript(
                        title=d.title,
                        description=d.description or "",
                        channel=d.channel_title,
                        language=request.language
                    )
                
                extract = sum_payload.get("extract", (d.description or d.title)[:300])

                # ğŸ”§ verify_yt ëª¨ë“œì— ë”°ë¼ ì ìˆ˜ ê²°ì •
                if request.verify_yt:
                    # âœ… verify_yt=True: LLM ì ìˆ˜ë§Œ ì‚¬ìš©
                    ver = await self.llm.score_video(
                        lecture_summary=request.lecture_summary, 
                        title=d.title, 
                        extract=extract, 
                        language=request.language
                    )
                    score = float(ver.get("score", 5.0) or 5.0)
                    reason = ver.get("reason", "LLM verification")
                else:
                    # âœ… verify_yt=False: Heuristicë§Œ ì‚¬ìš©
                    score = best_heuristic_score(
                        title=d.title,
                        view_count=d.view_count,
                        publish_time=d.publish_time,
                    )
                    reason = "Heuristic"

                best_scores.append(score)
                if score < request.min_score:
                    logger.info(f"ğŸ§Š YT í•„í„°ë§(min_score): {score:.2f} < {request.min_score} (title={d.title[:60]!r})")
                    return None

                vi = YouTubeVideoInfo(
                    url=d.url(),
                    title=d.title,
                    extract=extract,
                    lang=d.default_lang or request.yt_lang,
                )
                return YouTubeResponse(
                    lecture_id=request.lecture_id,
                    section_id=request.section_id,
                    video_info=vi,
                    reason=reason,
                    score=round(score, 2),
                )
        
        # ğŸš€ Process all videos in parallel with asyncio.gather
        candidate_results = await asyncio.gather(*[process_single_video(it) for it in dedup], return_exceptions=True)
        
        # ğŸ”§ Filter out None and exceptions (with error logging)
        candidates: List[YouTubeResponse] = []
        for idx, result in enumerate(candidate_results):
            if isinstance(result, Exception):
                logger.warning(
                    f"ì˜ìƒ ì²˜ë¦¬ ì‹¤íŒ¨ (idx={idx}, video_id={dedup[idx].video_id if idx < len(dedup) else 'unknown'}): {result}",
                    exc_info=result
                )
            elif result is not None:
                candidates.append(result)

        # 4) Sort and cap by effective top_k
        candidates.sort(key=lambda r: r.score, reverse=True)
        final = candidates[: request.effective_top_k()]

        if not final:
            best = max(best_scores) if best_scores else None
            logger.info(f"ğŸ§Š YT ê²°ê³¼ ì—†ìŒ: min_score={request.min_score}, best_score={best}")

        return final
